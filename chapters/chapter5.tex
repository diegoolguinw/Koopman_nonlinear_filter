En sus inicios, este trabajo de tesis prometía crear un algoritmo que fuese competitivo, a nivel de resultados numéricos, con los que existen en la literatura, como EKF, UKF y PF. La idea subyacente era aprovechar el poder de la linealización provista por \textit{kernel} Extended Dynamic Mode Decomposition para hacer Filtro de Kalman sobre ese nuevo sistema, esperando que eso aproximara bien la solución del problema de filtraje original.

Una vez cumplido este objetivo, una profundización de los aspectos teóricos del filtro, junto con una exploración más amplia de la literatura y el estado del arte, permitieron vislumbrar una cota teórica para el error del filtro y sus consecuencias.

Es así que esta investigación pasó de entregar una mera herramienta computacional, ya propuesta por otros autores, a una que además tiene cotas de error teóricas que, no solo no han sido demostradas a la fecha de entrega de este escrito, sino que no tantos métodos de filtraje tienen cotas de error y muchos están basados en heurísticas de las cuáles no se sabe en qué casos pueden fallar. Estas cotas de error componen la más importante innovación de este trabajo, posicionándose dentro de aquellos métodos simples de implementar con certificaciones teóricas.

Es más, de este trabajo se desprende de manera importante una metodología de la cuál, dada una linealización del sistema con una cota de error, se puede desprender una cota de error para el filtro. Es en este contexto que entonces los objetos relevantes a estudiar son las distintas maneras de hacer Extended Dynamic Mode Decomposition, de donde \textit{kernel} Extended Dynamic Mode Decomposition, expuesta en este trabajo, resulta ser aquella con mayor estudio en la literatura y que resulta tener cotas de error, aunque sean recientes para este trabajo. Es esta clase de nuevos filtros basados en eDMD la que se ha denominado en este trabajo como Extended Dynamic Mode Decomposition Kalman Filters (EDMD-KF).

Además, el trabajar con \textit{kernels} permitió explorar la potencia de los RKHS para llevar las ideas estadísticas de dimensión finita, eventualmente no lineales, a otras lineales pero en dimensión infinita. Esto además revive su potencial para problemas de filtraje, lo que ya había sido explorado por Song y Fukumizu, junto con colaboradores.

Todo esto danza entorno a la literatura del operador de Koopman que, más viva que nunca, sigue entregando nuevas técnicas para su aproximación, lo que daría nuevas vías para la creación de nuevos filtros. Lo que no deja de impresionar es todo lo que la simple idea detrás del operador, una composición de funciones haya provocado.

Más allá del aspecto teórico, se observa a lo largo de los experimentos numéricos que el filtro vence, en cuánto a error de aproximar el estado subyacente y filtrar el ruido, a los otros filtros probados en el contexto de distintas dinámicas de contagios que, aunque cambie la cantidad de estados y las interacciones, todas tienen en común una interacción no lineal común entre una población susceptible y una infectada, que resulta ser cuadrática y que además se puede caracterizar en un único parámetro.

Esta ventaja en cuánto a precisión se acentúa en escenarios de alta incertidumbre, en donde los ruidos de dinámica y observación son altos. Esto incluso frente a Filtros de Partículas, los que probablemente son los predominantes en la literatura de Filtros con cotas de error. Queda pendiente de este trabajo comparar con los Ensemble Kalman Filters \cite{Evensen1994SequentialStatistics}, también basados en métodos tipo Monte Carlo.

Incluso, este método, configurado y pensado de manera correcta, al utilizarse para estimar parámetros, logra obtener un rendimiento similar a \textit{samplers} como Differential Evolution Metropolis y NUTS, en el contexto de Markov Chain Monte Carlo, en cuanto a resultados, pero en tiempo incluso menor, lo que podría acelerar procesos de estimación de parámetros, que para muchos modelos demora del orden de horas.

Además, este trabajo logró mejorar la cota que presentaron Philipp et al. \cite{Philipp2024ErrorOperator}, lo que era uno de los mejores resultados encontrados a la fecha, provista por un equipo completo dedicado a encontrar cotas para el EDMD.

A pesar de ello, lo hecho durante este trabajo tiene ciertos aspectos por completar y que deben trabajarse en el futuro, por ejemplo, la observabilidad, que es crucial en el concepto del Filtro de Kalman lineal en dimensión finita. En dimensión infinita y en sistemas estocásticos se han explorado ideas que generalicen estas nociones, como la observabilidad en distribución \cite{Massiani2024Data-DrivenSystems} que está relacionada también con los \textit{kernels}, en específico con los \textit{kernels} característicos \cite{Sriperumbudur2010OnMeasures, Sriperumbudur2011UniversalityMeasures}, que es una clase \textit{kernels} que incluye a los \textit{kernels} universales, de los cuáles Matérn forma parte.

El aspecto de la observabilidad, en un contexto similar con Koopman, ya ha sido estudiado por Surana y colaboradores \cite{Surana2016LinearFramework}, por lo que seguir esas direcciones puede ser útil en el futuro. La observabilidad conlleva a la estabilización de sistemas, lo que es crítico para el buen funcionamiento del filtro para sistemas lineales, incluso para sistemas de dimensión infinita \cite{Afshar2024ExtendedSystems}.

En el aspecto práctico, el filtro aún tiene hiperparámetros que deben tener una metodología para elegirse y que tenga un sustento teórico. En primer lugar se encuentra el \textit{kernel}, en donde Matérn no es la única opción, sino que también existen opciones como \textit{kernels} de soporte compacto, como los propuestos por Wendland \cite{Wendland1995PiecewiseDegree}, y que también generan como RKHS un espacio de Sobolev.

Para el caso del \textit{kernel} de Matérn, falta elegir los hiperparámetros $\nu$ y $\gamma$. Sobre el primero, lo único que se sabe es que cuando $\nu \to \infty$ el \textit{kernel} de Matérn converge al \textit{kernel} Gaussiano \cite{Rasmussen2005GaussianLearning}, que se sabe no tiene tan buenas propiedades aproximantes, lo que parece apuntar a $\nu$ pequeño, coincidiendo con los buenos resultados numéricos presentados. Mientras tanto, para el otro parámetro, el ancho de banda $\gamma$, se puede seleccionar maximizando la verosimilitud con respecto a un conjunto de datos de entrenamiento \cite{Rasmussen2005GaussianLearning}.

Además de ello, el procedimiento expuesto en este trabajo depende fuertemente de multiplicación es inversión de matrices, lo que implica muchas operaciones $O(N^3)$, mientras que se gana una precisión de $O(N^{-1/2})$. Una alternativa a la aproximación por \textit{kernel} Extended Dynamic Decomposition es aquella realizada por neural Dynamic Mode Decomposition (nDMD) \cite{Xiong2024KoopmanEquations, Frion2023LeveragingData, Terao2021ExtendedEquations}, lo que puede reducir el costo de las operaciones realizadas posterior al entrenamiento de la red. Incluso, una idea similar a la expuesta en \cite{Li2022Data-DrivenNetworks, Liu2024NeuralSystems, Garmaev2024DeepModelling} puede asemejarse a lo que sería un análogo del filtro propuesto en esta tesis, pero aproximando el operador de Koopman vía redes neuronales. 

Un inconveniente es que todas las ideas expuestas antes no presentan ninguna cota de error, como sí se hizo en este trabajo, por lo que faltaría avanzar en poder entregar una cota de error para nDMD, de lo que se desprendería de manera directa, con lo hecho en el capítulo 4, una cota de error para el filtro en función de la cantidad de neuronas de la capa oculta de la red.