El hecho de que sea posible construir un sistema lineal que se asemeje significativamente a otro sistema, eventualmente no lineal, sugiere la viabilidad de emplear dicho sistema linealizado para abordar problemas de filtraje no lineal mediante el uso del Filtro de Kalman. 

En la literatura, ya existen diversas conexiones entre el Filtro de Kalman y el operador de Koopman. Por ejemplo, se ha explorado su utilización para la corrección de errores \cite{Jiang2022CorrectingFilters}, la estimación de modos de Koopman \cite{Liu2024EstimateFilter}, y la mejora de algoritmos como el Filtro de Kalman Extendido \cite{Ramadan2024ExtendedControl}. Además, algoritmos similares al que se propone en este capítulo han sido presentados y validados en diferentes aplicaciones, como se observa en \cite{Wang2022KoopmanSystem, Wang2023Innovation-saturatedOutliers, Netto2018RobustEstimation, Syed2021Koopman-basedXFEL, HuangData-DrivenFlight}. 

Por otra parte, trabajos pioneros en la línea de observabilidad e identificación de sistemas, liderados por Surana y colaboradores, han proporcionado una base sólida en este campo, como se documenta en \cite{Surana2016KoopmanSystems, Surana2016LinearFramework}.

Aunque el algoritmo propuesto en este capítulo no constituye una contribución original en términos de su estructura, sí lo es la justificación de su funcionamiento y convergencia bajo el supuesto de contar con un muestreo suficiente de puntos. En este contexto, se demuestra que el \textit{kernel Dynamic Mode Decomposition} converge al operador de Koopman en un espacio de dimensión infinita. 

El objetivo principal de este capítulo es demostrar que, bajo ciertas hipótesis, la tasa de convergencia del algoritmo es del orden de $N^{-1/2}$, lo que lo posiciona como un competidor viable frente a otros algoritmos de filtraje presentes en la literatura, tales como los filtros de partículas, discutidos previamente en la sección de preliminares.

Para ello, se propone primero descomponer el error del Filtro de Kalman aplicado a dos sistemas lineales, formulando este error en función de los datos provenientes de los sistemas. Este análisis, hasta la fecha de redacción de este trabajo, no se encuentra documentado en la literatura y, por lo tanto, constituye una contribución original de esta investigación.

Posteriormente, se construye el filtro propuesto, denominado \textit{Kernel Koopman Kalman Filter} (KKKF), siguiendo una metodología análoga a la empleada en el Filtro de Kalman para sistemas lineales con mínimos cuadrados recursivos \cite{Kalman1960AProblems, Triantafyllopoulos2021BayesianBeyond}, y particularmente a como se formula en \cite{Gebhard2019} para la construcción de la denominada \textit{Kernel Bayes Rule}.

Finalmente, utilizando los resultados obtenidos en el capítulo anterior junto con la descomposición del error, se demuestra la cota de error propuesta. El desempeño del filtro es evaluado en diferentes sistemas mediante una implementación en Python desarrollada específicamente para este trabajo \cite{Olguin2024KKKF:Filter}.

\section{Descomposición de error de Kalman}

El objetivo de esta sección es analizar el error que se genera entre dos reglas de Kalman, lo cual permitirá cuantificar la discrepancia entre una regla de Kalman aproximante y otra exacta, ambas definidas formalmente más adelante en esta misma sección. 

Para este propósito, se consideran dos sistemas dinámicos observados en un espacio de Hilbert con espacios de estados $E_x$ y de observaciones $E_y$, descritos por las siguientes ecuaciones:  
\begin{equation*}
	\begin{aligned}
		\mu_{i,k}  &= A_{i,k} \mu_{i,k-1} + \nu_{i,k}, \\
		y_{i,k} &= C_{i,k} \mu_{i,k} + \xi_{i,k},
	\end{aligned}
\end{equation*}
donde $A_{i,k} : E_x \to E_x$ y $C_{i,k}: E_x \to E_y$ son operadores lineales; $\nu_{i,k} \in E_x$ y $\xi_{i,k} \in E_y$ representan variables aleatorias con segundo momento finito y operadores de covarianza $\mathcal{Q}_{i,k}$ y $\mathcal{R}_{i,k}$, respectivamente. Todo esto se considera para $i \in \{1,2\}$ y $k \geq 1$.

Cada uno de estos sistemas tiene asociada una regla de Kalman, la cual está definida por las siguientes expresiones:  
\begin{equation*}
	\begin{aligned}
		\mathcal{P}_{i,k}^- &= A_{i,k}^* \mathcal{P}_{i,k-1}^+ A_{i,k} + \mathcal{Q}_{i,k}, \\
		\S_{i,k} &= C_{i,k} \mathcal{P}_{i,k}^- C_{i,k}^* + \mathcal{R}_{i,k}, \\
		\K_{i,k} &= \mathcal{P}_{i,k}^- C_{i,k} \S_{i,k}^{-1}, \\
		\mathcal{P}_{i,k}^+ &= (I - \K_{i,k} C_{i,k}) \mathcal{P}_{i,k}^-, \\
		\hat{\mu}_{i,k} &= A_{i,k} \hat{\mu}_{i,k-1} + \K_{i,k} (y_{i,k} - C_{i,k} \hat{\mu}_{i,k-1}),
	\end{aligned}
\end{equation*}
con $i \in \{1,2\}$ y $k \geq 1$. Aquí, $\mathcal{P}_{i,k}^-$ y $\mathcal{P}_{i,k}^+$ representan los operadores de covarianza del error a priori y a posteriori, respectivamente, mientras que $\K_{i,k}$ es el operador de ganancia de Kalman, todos ellos definidos en los espacios indicados.

Estas reglas se inicializan como sigue:
\begin{equation*}
	\hat{\mu}_{i,0} = \mathbb{E}[\mu_{i,0}], \quad \mathcal{P}_{i,0} = \text{Cov}(\mu_{i,0}).
\end{equation*}

Con estas definiciones, se presenta un resultado clave que ilustra cómo la discrepancia en norma entre las reglas de Kalman puede descomponerse en función de las discrepancias en norma de los elementos asociados, junto con la influencia de las iteraciones previas.


\begin{teo}[Descomposición de error de Kalman]
	Sea $k \geq 1$. Si los operadores $\S_{i,k}$ son invertibles, entonces existen constantes $c_{k,j}^i$ con $j \in \{1, \dots, 7\}$, $i \in \{1, 2\}$, tales que se cumplen las siguientes desigualdades:
	\begin{equation*}
		\begin{aligned}
			\| \hat{\mu}_{1,k} - \hat{\mu}_{2,k} \| \leq & \, c_{1,k}^1 \| A_{1,k} - A_{2,k} \| + c_{2,k}^1 \| C_{1,k} - C_{2,k} \| \\ 
			&+ c_{3,k}^1 \| \mathcal{Q}_{1,k} - \mathcal{Q}_{2,k} \| + c_{4,k}^1 \| \mathcal{R}_{1,k} - \mathcal{R}_{2,k} \| \\
			&+ c_{5,k}^1 \| y_{1,k} - y_{2,k} \| + c_{6,k}^1 \| \hat{\mu}_{1,k-1} - \hat{\mu}_{2,k-1} \| \\
			&+ c_{7,k}^1 \| \mathcal{P}_{1,k-1}^+ - \mathcal{P}_{2,k-1}^+ \|,
		\end{aligned}
	\end{equation*}
	y
	\begin{equation*}
		\begin{aligned}
			\| \mathcal{P}_{1,k}^+ - \mathcal{P}_{2,k}^+ \| \leq & \, c_{1,k}^2 \| A_{1,k} - A_{2,k} \| + c_{2,k}^2 \| C_{1,k} - C_{2,k} \| \\ 
			&+ c_{3,k}^2 \| \mathcal{Q}_{1,k} - \mathcal{Q}_{2,k} \| + c_{4,k}^2 \| \mathcal{R}_{1,k} - \mathcal{R}_{2,k} \| \\
			&+ c_{5,k}^2 \| y_{1,k} - y_{2,k} \| + c_{6,k}^2 \| \hat{\mu}_{1,k-1} - \hat{\mu}_{2,k-1} \| \\
			&+ c_{7,k}^2 \| \mathcal{P}_{1,k-1}^+ - \mathcal{P}_{2,k-1}^+ \|.
		\end{aligned}
	\end{equation*}

	Aquí, las constantes $c_{k,j}^i$ son positivas y dependen de $k$ únicamente a través de las normas $\| A_{i,k} \|$, $\| C_{i,k} \|$, $\| \mathcal{Q}_{i,k} \|$, $\| \mathcal{R}_{i,k} \|$, $\| \S_{i,k}^{-1} \|$, $\| y_{i,k} \|$, $\| \hat{\mu}_{i,k-1} \|$ y $\| \mathcal{P}_{i,k-1}^+ \|$.
	\label{teo:error_kalman}
\end{teo}

\begin{proof}
Se observa que  
\begin{equation*}
	\begin{aligned}
		&	\| \hat \mu_{1,k} - \hat \mu_{2,k} \|_{E_x}  \\
		\leq & \, \| A_{1,k} \mu_{1,k-1}  - A_{2,k} \mu_{2,k-1} \|  \\
		& + \|  \K_{1,k} (y_{1,k} - C_{1,k} \hat\mu_{1,k-1}) -  \K_{2,k} (y_{2,k} - C_{2,k} \hat\mu_{2,k-1})  \|.
	\end{aligned}
\end{equation*}

El primer término, denominado \textit{error de predicción}, satisface la siguiente desigualdad:
\begin{equation*}
	\begin{aligned}
		& \| A_{1,k} \mu_{1,k-1}  - A_{2,k} \mu_{2,k-1} \|  \\
		& \leq \| A_{1,k} \mu_{1,k-1}  - A_{1,k} \mu_{2,k-1} \| + \| A_{1,k} \mu_{2,k-1}  - A_{2,k} \mu_{2,k-1} \| \\
		& \leq \| A_{1,k} \| \| \mu_{1,k-1}  - \mu_{2,k-1} \| +  \| \mu_{2,k-1} \| \| A_{1,k} - A_{2,k} \|.
	\end{aligned}
\end{equation*}

Por otro lado, el segundo término, denominado \textit{error de actualización}, cumple:
\begin{equation*}
	\begin{aligned}
		& \|  \K_{1,k} (y_{1,k} - C_{1,k} \hat\mu_{1,k-1}) -  \K_{2,k} (y_{2,k} - C_{2,k} \hat\mu_{2,k-1})  \| \\
		& \leq  \| \K_{1,k} y_{1,k} -  \K_{2,k} y_{2,k}  \| + \| \K_{1,k} C_{1,k} \hat\mu_{1,k-1} - \K_{2,k} C_{2,k} \hat\mu_{2,k-1}  \| \\
		& \leq \| \K_{1,k} y_{1,k} -  \K_{1,k} y_{2,k}  \| + \| \K_{1,k} y_{2,k} -  \K_{2,k} y_{2,k}  \| \\
		& \quad + \| \K_{1,k} C_{1,k} \hat\mu_{1,k-1} - \K_{1,k} C_{2,k} \hat\mu_{2,k-1}  \| + \| \K_{1,k} C_{2,k} \hat\mu_{2,k-1} - \K_{2,k} C_{2,k} \hat\mu_{2,k-1}  \| \\
		& \leq \| \K_{1,k} \| \|  y_{1,k} - y_{2,k}  \| + \| y_{2,k} \| \| \K_{1,k}  -  \K_{2,k}  \| \\
		& \quad + \| \K_{1,k} \| \|  C_{1,k} \hat\mu_{1,k-1} - C_{2,k} \hat\mu_{2,k-1}  \| + \| C_{2,k} \hat\mu_{2,k-1} \| \| \K_{1,k}  - \K_{2,k} \| \\
		& \leq \| \K_{1,k} \| \|  y_{1,k} - y_{2,k}  \| + \| y_{2,k} \| \| \K_{1,k}  -  \K_{2,k}  \| \\
		& \quad + \| \K_{1,k} \| \left ( \|  C_{1,k} \hat\mu_{1,k-1} - C_{1,k} \hat\mu_{2,k-1}  \| + \|  C_{1,k} \hat\mu_{2,k-1} - C_{2,k} \hat\mu_{2,k-1}  \| \right ) \\
		& \quad + \| C_{2,k} \hat\mu_{2,k-1} \| \| \K_{1,k}  - \K_{2,k} \| \\
		& \leq \| \K_{1,k} \| \|  y_{1,k} - y_{2,k}  \| + \| y_{2,k} \| \| \K_{1,k}  -  \K_{2,k}  \| \\
		& \quad + \| \K_{1,k} \| \left ( \| C_{1,k}  \| \|  \hat\mu_{1,k-1} - \hat\mu_{2,k-1}  \| + \| \hat\mu_{2,k-1}  \| \| C_{1,k} - C_{2,k}  \| \right ) \\
		& \quad + \| C_{2,k} \hat\mu_{2,k-1} \| \| \K_{1,k}  - \K_{2,k} \|.
	\end{aligned}		
\end{equation*}

En virtud de lo anterior, se debe analizar la diferencia en norma de los operadores de ganancia:
\begin{equation*}
	\begin{aligned}
		& \| \K_{1,k}  - \K_{2,k} \| \\
		& \leq \| \mathcal{P}_{1,k}^- C_{1,k}\S_{1,k}^{-1} -  \mathcal{P}_{2,k}^- C_{2,k} \S_{2,k}^{-1} \| \\
		& \leq \| \mathcal{P}_{1,k}^- C_{1,k}\S_{1,k}^{-1} - \mathcal{P}_{2,k}^- C_{1,k}\S_{1,k}^{-1} \| + \| \mathcal{P}_{2,k}^- C_{1,k}\S_{1,k}^{-1} - \mathcal{P}_{2,k}^- C_{2,k}\S_{2,k}^{-1} \| \\
		& \leq \| C_{1,k}\S_{1,k}^{-1} \| \| \mathcal{P}_{1,k}^- - \mathcal{P}_{2,k}^-\| + \| \mathcal{P}_{2,k}^- \| \|  C_{1,k}\S_{1,k}^{-1} -  C_{2,k}\S_{2,k}^{-1}\| \\
		& \leq \| C_{1,k}\S_{1,k}^{-1} \| \| \mathcal{P}_{1,k}^- - \mathcal{P}_{2,k}^-\| \\
		& \quad + \| \mathcal{P}_{2,k}^- \| ( \|  C_{1,k}\S_{1,k}^{-1} -  C_{1,k}\S_{2,k}^{-1}\| + \|  C_{1,k}\S_{2,k}^{-1} -  C_{2,k}\S_{2,k}^{-1}\|) \\
		& \quad + \| \mathcal{P}_{2,k}^- \| ( \|  C_{1,k} \| \| \S_{1,k}^{-1} -  \S_{2,k}^{-1}\| + \| \S_{2,k}^{-1} \| \| C_{1,k} -  C_{2,k}\|).
	\end{aligned}
\end{equation*}
En donde
\begin{equation*}
	\| \mathcal{P}_{i,k}^- \|  \leq \| A_{i,k} \|^2 \| \mathcal{P}_{i,k-1}^+\| + \| \mathcal{Q}_{i,k}\|, \quad i \in \{ 1, 2 \}.
\end{equation*}

Primero, para las diferencias en norma de los operadores de covarianza de error a priori se tiene:
\begin{equation*}
	\begin{aligned}
		& \| \mathcal{P}_{1,k}^- - \mathcal{P}_{2,k}^-\| \\
		& = \| A_{1,k}^* \mathcal{P}_{1,k-1}^+ A_{1,k} + \mathcal{Q}_{1,k} - A_{2,k}^* \mathcal{P}_{2,k-1}^+ A_{2,k} + \mathcal{Q}_{2,k}  \| \\
		& \leq \|A_{1,k}^* \mathcal{P}_{1,k-1}^+ A_{1,k} - A_{2,k}^* \mathcal{P}_{2,k}^+ A_{2,k} \| + \| \mathcal{Q}_{1,k} - \mathcal{Q}_{2,k}  \| \\
		& \leq \|A_{1,k}^* \mathcal{P}_{1,k-1}^+ A_{1,k} - A_{1,k}^* \mathcal{P}_{2,k-1}^+ A_{2,k} \| +  \|A_{1,k}^* \mathcal{P}_{2,k-1}^+ A_{2,k} - A_{2,k}^* \mathcal{P}_{2,k-1}^+ A_{2,k} \| \\ 
		& \quad + \| \mathcal{Q}_{1,k} - \mathcal{Q}_{2,k}  \| \\
		& \leq \|A_{1,k}^* \| \| \mathcal{P}_{1,k-1}^+ A_{1,k} - \mathcal{P}_{2,k-1}^+ A_{2,k} \| + \| \mathcal{P}_{2,k-1}^+ A_{2,k}  \| \|A_{1,k}^* - A_{2,k}^* \| \\ 
		& \quad + \| \mathcal{Q}_{1,k} - \mathcal{Q}_{2,k}  \| \\
		& \leq \|A_{1,k} \| \| \mathcal{P}_{1,k-1}^+ A_{1,k} - \mathcal{P}_{2,k-1}^+ A_{2,k} \| + \| \mathcal{P}_{2,k-1}^+ A_{2,k}  \| \|A_{1,k} - A_{2,k} \| \\ 
		& \quad + \| \mathcal{Q}_{1,k} - \mathcal{Q}_{2,k}  \| \\
		& \leq \|A_{1,k} \| (\| \mathcal{P}_{1,k}^+ A_{1,k} - \mathcal{P}_{1,k-1}^+ A_{2,k} \| + \| \mathcal{P}_{1,k-1}^+ A_{2,k} - \mathcal{P}_{2,k-1}^+ A_{2,k} \| ) \\
		& \quad + \| \mathcal{P}_{2,k-1}^+ \| \| A_{2,k}  \| \|A_{1,k} - A_{2,k} \| \\ 
		& \quad + \| \mathcal{Q}_{1,k} - \mathcal{Q}_{2,k}  \| \\
		& \leq \|A_{1,k} \| ( \| \mathcal{P}_{1,k-1}^+ \| \|  A_{1,k} - A_{2,k} \| + \| A_{2,k} \| \| \mathcal{P}_{1,k-1}^+  - \mathcal{P}_{2,k-1}^+  \| ) \\
		& \quad + \| \mathcal{P}_{2,k-1}^+ \| \| A_{2,k}  \| \|A_{1,k} - A_{2,k} \| \\ 
		& \quad + \| \mathcal{Q}_{1,k} - \mathcal{Q}_{2,k}  \|.
	\end{aligned}
\end{equation*}

Se analizará finalmente el término $\| \S_{1,k}^{-1} -  \S_{2,k}^{-1}\|$. Para ello, se observa que
\begin{equation*}
	\S_{1,k}^{-1} -  \S_{2,k}^{-1} = \S_{2,k}^{-1} (\S_{2,k} - \S_{1,k}) \S_{1,k}^{-1}.
\end{equation*}
A partir de esta expresión, se tiene que
\begin{equation*}
	\begin{aligned}
		\| \S_{1,k}^{-1} -  \S_{2,k}^{-1} \| & \leq  \| \S_{2,k}^{-1} (\S_{2,k} - \S_{1,k}) \S_{1,k}^{-1} \| \\
		& \leq \| \S_{1,k}^{-1} \| \|  \S_{2,k}^{-1} \| \| \S_{2,k} - \S_{1,k}\| \\
		& \leq  \| \S_{1,k}^{-1} \| \|  \S_{2,k}^{-1} \|  \| C_{1,k} \mathcal{P}_{1,k}^- C_{1,k}^* + \mathcal{R}_{1,k} - C_{2,k} \mathcal{P}_{2,k}^- C_{2,k}^* + \mathcal{R}_{2,k} \|.
	\end{aligned}
\end{equation*}
En este contexto, y de manera análoga a lo previamente demostrado, se obtiene la siguiente estimación para la expresión anterior:
\begin{equation*}
	\begin{aligned}
		\| C_{1,k} \mathcal{P}_{1,k}^- C_{1,k}^* + \mathcal{R}_{1,k} - C_{2,k} \mathcal{P}_{2,k}^- C_{2,k}^* + \mathcal{R}_{2,k} \| \\
		& \leq \|C_{1,k-1} \|  \| \mathcal{P}_{1,k-1}^+ \| \|  C_{1,k} - C_{2,k} \|  \\
            & \quad + \|C_{1,k-1} \|  \| C_{2,k} \| \| \mathcal{P}_{1,k-1}^+  - \mathcal{P}_{2,k-1}^+  \| \\
		& \quad + \| \mathcal{P}_{2,k-1}^+ \| \| C_{2,k}  \| \|C_{1,k} - C_{2,k} \| \\
		& \quad+ \| \mathcal{R}_{1,k} - \mathcal{R}_{2,k}  \|.
	\end{aligned}
\end{equation*}

\end{proof}

Con lo anterior, se concluye que, para una iteración $k$, el error depende tanto del error en la condición para la estimación del estado como del operador de covarianza del error a posteriori.

\section{KKKF: Kernel Koopman Kalman Filter}

En esta sección se presenta la deducción del algoritmo de filtraje solo utilizando la teoría de RKHS junto con el operador de Koopman de manera meticulosa. Primero se deduce una dinámica del \textit{embedding}, para ello notar que  
\begin{equation*}
	\begin{aligned}
		\Phi_\X (\mathbf{x}_{k+1}) &= \Phi_\X (\mathbf{f}(\mathbf{x}_k, \mathbf{w}_k)) \\
        &= \E[\Phi_\X (X^+) | X = \mathbf{x}_k] + \Phi_\X (\mathbf{f}(\mathbf{x}_k, \mathbf{w}_k)) - \E[\Phi_\X (X^+) | X = \mathbf{x}_k]\\
        &= C_{X^+|X} \Phi_\X (\mathbf{x}_k) + \zeta_k
	\end{aligned}
\end{equation*}
donde 
\begin{equation*}
    \zeta_k = \Phi_\X (\mathbf{f}(\mathbf{x}_k, \mathbf{w}_k)) - \E[\Phi_\X (X^+) | X = \mathbf{x}_k]
\end{equation*}
es una variable aleatoria infinita dimensional cuyo operador de covarianza está acotado y se denota por $\mathcal{Q}_k$. Además, esta variable aleatoria tiene media nula, esto ya que
\begin{equation*}
    \mathbf{f}(\mathbf{x}_k, \mathbf{w}_k) \sim X^+ | X = \mathbf{x}_k,
\end{equation*}
con lo que
\begin{equation*}
    \begin{aligned}
         \E[\zeta_k] &= \E[\Phi_\X (\mathbf{f}(\mathbf{x}_k, \mathbf{w}_k))] - \E[\E[\Phi_\X (X^+) | X = \mathbf{x}_k]] \\
         &= \E[\E[\Phi_\X (X^+) | X = \mathbf{x}_k]] - \E[\E[\Phi_\X (X^+) | X = \mathbf{x}_k]] \\
         &= 0.
    \end{aligned}
\end{equation*}
De manera análoga, para el \textit{embedding} de la observación se tiene
\begin{equation*}
	\begin{aligned}
		\Phi_\Y (\mathbf{y}_{k}) &= \Phi_\Y (\mathbf{g}(\mathbf{x}_k, \mathbf{v}_k)) \\
        &= \E[\Phi_\Y (Y) | X = \mathbf{x}_k] + \Phi_\Y (\mathbf{g}(\mathbf{x}_k, \mathbf{v}_k)) - \E[\Phi_\Y (Y) | X = \mathbf{x}_k]\\
        &= C_{Y|X} \Phi_\X (\mathbf{x}_k) + \nu_k
	\end{aligned}
\end{equation*}
donde $\nu_k$ es una variable aleatoria infinita dimensional cuyo operador de covarianza está acotado, denotado por $\mathcal{R}_k$, y está centrada ya que
\begin{equation*}
    \mathbf{g}(\mathbf{x}_k, \mathbf{v}_k) \sim Y | X = \mathbf{x}_k
\end{equation*}
con lo que
\begin{equation*}
    \begin{aligned}
         \E[\nu_k] &= \E[\Phi_\Y (\mathbf{g}(\mathbf{x}_k, \mathbf{v}_k))] - \E[\E[\Phi_\Y (Y) | X = \mathbf{x}_k]] \\
         &= \E[\E[\Phi_\Y (Y) | X = \mathbf{x}_k]] - \E[\E[\Phi_\Y (Y) | X = \mathbf{x}_k]] \\
         &= 0.
    \end{aligned}
\end{equation*}
Siguiendo un procedimiento similar al utilizado en \cite{Gebhard2019}, se define
\begin{equation*}
	\hat{\mu}_k = \mathbb{E} [\Phi_\X (\mathbf{x}_k) | \mathbf{y}_{1:k}], \quad \mathcal{P}_{k} = \text{Cov}(\Phi_\X(\mathbf{x}_k) - \hat{\mu}_k)
\end{equation*}
y, en particular,
\begin{equation*}
	\hat{\mu}_0 = \hat{\mu}_0^- = \mathbb{E} [\Phi_\X (\mathbf{x}_0)], \quad \mathcal{P}_{0} = \text{Cov}(\Phi_\X (\mathbf{x}_0) - \hat{\mu}_0).
\end{equation*}
Se define
\begin{equation*}
	\hat{\mu}_{k+1}^- = \mathbb{E} [\Phi_\X (\mathbf{x}_{k+1}) | \mathbf{y}_{1:k}], \quad \mathcal{P}_{k+1}^- = \text{Cov}(\Phi_\X (\mathbf{x}_{k+1}) - \hat{\mu}_{k+1}^-)
\end{equation*}
El cual, por la regla de Bayes para kernels \cite{Fukumizu2013KernelKernels}, satisface
\begin{equation*}
	\hat{\mu}_{k+1}^- = \mathbb{E} [\Phi_\X (\mathbf{x}_{k+1}) | \mathbf{y}_{1:k}] = C_{X^+|X} \mathbb{E} [\Phi_\X (\mathbf{x}_{k}) | \mathbf{y}_{1:k}] = C_{X^+|X} \hat{\mu}_k.
\end{equation*}
A partir de esto, y utilizando la independencia de $\zeta_k$ con respecto a $\Phi_\X (\mathbf{x}_k)$, se obtiene
\begin{equation*}
	\begin{aligned}
		\mathcal{P}_{k+1}^- &= \text{Cov}(\Phi_\X (\mathbf{x}_{k+1}) - \hat{\mu}_{k+1}^-)  \\
		&= \text{Cov}(C_{X^+|X}\Phi_\X (\mathbf{x}_{k}) + \zeta_{k+1} - C_{X^+|X}\hat{\mu}_{k}) \\
		&= C_{X^+|X} \text{Cov} (\Phi_\X (\mathbf{x}_{k}) - \hat{\mu}_{k})C_{X^+|X}^* + \text{Cov}(\zeta_{k+1}) \\
		&= C_{X^+|X} \mathcal{P}_k (C_{X^+|X})^* + \mathcal{Q}_{k+1}
	\end{aligned}
\end{equation*}
Posteriormente, se debe proyectar sobre las observaciones para obtener la estimación a posteriori, es decir,
\begin{equation*}
	\hat{\mu}_{k+1}^- = \mathbb{E} [\Phi_\X (\mathbf{x}_{k+1}) | \mathbf{y}_{1:k}] \quad \text{se actualiza a} \quad \hat{\mu}_{k+1} = \mathbb{E} [\Phi_\X (\mathbf{x}_{k+1}) | \mathbf{y}_{1:k+1}].
\end{equation*}

Se propone que
\begin{equation*}
	\hat{\mu}_{k+1} = \hat{\mu}_{k+1}^- + \K_{k+1} (\mathbf{y}_{k+1} - C_{Y|X} \hat{\mu}_{k+1}^-),
\end{equation*}
lo que implica una actualización similar a la del filtro de Kalman lineal, donde ahora $\K_k : \mathbb{R}^p \to \mathcal{H}_\X$ es el operador de ganancia de Kalman. 

En Gebhardt et al. \cite{Gebhard2019} se deduce que $\hat{\mu}_k$ es un estimador insesgado de $\mu_k$, para todo $k$ y además una expresión para el operador de ganancia, que se deja a continuación por completitud de la deducción del filtro, en donde se ha ajustado la notación.

\begin{prop}[Adaptación de Gebhardt et al. \cite{Gebhard2019}]
    El estimador $\hat{\mu}_k$ es insesgado para $\mu_k$, para todo $k$ y el operador de ganancia de Kalman $\K_k: \R^p \to \H_\X$ viene dado por
    \begin{equation*}
        \mathcal{K}_k = \mathcal{P}^-_{k}C_{Y|X}^*(C_{Y|X}\mathcal{P}^-_{k}C_{Y|X}^* + \mathcal{R}_k)^{-1}.
    \end{equation*}
    \label{prop:unbias_kalman_operator}
\end{prop}

\begin{proof}
    Denotando $\varepsilon_k^- = \hat{\mu}_k^- - \mu_k \in \H_\X$ el error a priori y $\varepsilon_k^+ = \hat{\mu}_k - \mu_k \in \H_\X$ el error a posteriori, se tiene lo siguiente
    \begin{align*}
    \varepsilon_k^+ &= \hat{\mu}_k - \mu_k \\
                &=\hat{\mu}_{k}^- + \K_{k} (\mathbf{y}_{k} - C_{Y|X} \hat{\mu}_{k}^-) - \mu_k \\
                &= \varepsilon_k^- +  \K_{k} \mathbf{y}_{k} - \K_k C_{Y|X} \hat{\mu}_{k}^- \\
                &= \varepsilon_k^- +  \K_{k} \mathbf{y}_{k} - \K_k C_{Y|X} \hat{\mu}_{k}^- + \K_k C_{Y|X} \mu_k - \K_k C_{Y|X} \mu_k\\
                &= \varepsilon_k^- + \K_k C_{Y|X} (-\hat{\mu}_{k}^- + \mu_k) + \K_{k} \mathbf{y}_{k} - \K_k C_{Y|X} \mu_k\\
                &= \varepsilon_k^- - \K_k C_{Y|X}  \varepsilon_k^- + \K_{k} \mathbf{y}_{k} - \K_k C_{Y|X} \mu_k\\
                &= ( I - \K_k C_{Y|X} ) \varepsilon_k^- + \K_{k} (\mathbf{y}_{k} - C_{Y|X} \mu_k)\\
                &= ( I - \K_k C_{Y|X} ) \varepsilon_k^- + \K_{k} \nu_k
\end{align*}

Primero se probará que $\E[\varepsilon_k^-] = 0$, esto por inducción. En primer lugar, para $k=0$ se tiene por construcción, con lo que el caso el caso base queda probado. Suponiendo que se cumple para $k \in \N$, se propone probarlo para $k+1$.

Notando que 
\begin{equation*}
    \varepsilon_{k+1}^- = \hat{\mu}_{k+1}^- - \mu_{k+1} = C_{X^+|X} \hat{\mu}_k - C_{X^+|X} \mu_k - \zeta_k
\end{equation*}
se tiene que
\begin{equation*}
    \E[\varepsilon_{k+1}^-] = C_{X^+|X} \E [\hat{\mu}_k] - \E[\zeta_k] = C_{X^+|X} \E[\varepsilon^+_k]
\end{equation*}
ya que las variables aleatorias $\zeta_k$ son centradas. Con ello
\begin{equation*}
    \E[\varepsilon^+_k] = ( I - \K_k C_{Y|X} ) \E[\varepsilon_k^-] + \K_{k} \E[\nu_k] = 0
\end{equation*}
ya los $\nu_k$ son centrados y por la hipótesis de inducción $\E[\varepsilon_k^-] = 0$. Luego por principio de inducción queda probado que $\E[\varepsilon_k^-] = 0$, para todo $k$. 

Haciendo la misma manipulación de antes, se prueba que $\E[\varepsilon^+_k] = 0$, para todo $k$, con lo que el estimado $\hat{\mu}_k$ es insesgado para $\mu_k$.

Ahora para el operador de ganancia se debe recordar que en el problema de filtraje se busca minimizar la pérdida cuadrática esperada asociada a la estimación de la trayectoria, la que viene dada por
\begin{equation*}
    \E[(\hat{\mu}_{k} - \mu_{k})^* (\hat{\mu}_{k} - \mu_{k}) ] = \E [ (\varepsilon_k^+)^* \varepsilon_k^+ ]
\end{equation*}

Dado que el estimador es insesgado, esto se puede reformular como la minimización de la traza de la covarianza de error a posteriori $\mathcal{P}_k$, es decir
\begin{align*}
\min_{\K_k} \mathbb{E}[(\varepsilon_k^+)^* \varepsilon_k^+] &= \min_{\K_k} \text{Tr} \mathbb{E}[\varepsilon_k^+(\varepsilon_k^+)^*] \\
&= \min_{\K_k} \text{Tr} \mathcal{P}_{k}.
\end{align*}

Sustituyendo la expresión para el error a posteriori se tiene que

\begin{align*}
\mathcal{P}_k &= \mathbb{E}[\varepsilon^+_k(\varepsilon^+_k)^*] \\
&= \mathbb{E}[((I - \K_k C_{Y|X})\varepsilon^-_k - \K_k \nu_k)((I - \K_k C_{Y|X})\varepsilon^-_k - \K_k \nu_k)^*] \\
&= (I - \K_k C_{Y|X})\mathbb{E}[\varepsilon^-_k(\varepsilon^-_k)^*](I - \K_k C_{Y|X})^* \\
&\quad - \K_k\mathbb{E}[\nu_k(\varepsilon^-_k)^*](I - \K_k C_{Y|X})^* \\
&\quad - (I - \K_k C_{Y|X})\mathbb{E}[\varepsilon^-_k\nu_k^*]\K_k^* \\
&\quad + \K_k\mathbb{E}[\nu_k\nu_k^*]\K_k^*
\end{align*}

Dado que se asume que el ruido del operador de observación es independiente del error de estimación y dado que se asumió que la estimación a priori tiene media cero, se obtiene

\begin{equation*}
\mathbb{E}[\nu_k(\varepsilon^-_k)^*] = \mathbb{E}[\nu_k]\mathbb{E}[(\varepsilon^-_k)^*] = \mathbb{E}[(\varepsilon^-_k)^*]\mathbb{E}[\nu_k] = \mathbb{E}[\varepsilon^-_k\nu_k^*] = 0.
\end{equation*}

Con esta perspectiva, el operador de covarianza posterior puede reformularse como

\begin{equation*}
\mathcal{P}_k = (I - \K_k C_{Y|X})\mathcal{P}^-_{k}(I - \K_k C_{Y|X})^* + \K_k \mathcal{R}_k \K_k^*,
\end{equation*}

donde $\mathcal{R}_k = \mathbb{E}[\nu_k\nu_k^*]$ es la covarianza del ruido asociado a la observación. Tomando la derivada de la traza del operador de covarianza e igualándola a cero se obtiene
\begin{align*}
2(I - \K_k C_{Y|X})\mathcal{P}^-_{k}(-C_{Y|X}^*) + 2\K_k \mathcal{R}_k &= 0,
\end{align*}
que es equivalente a 
\begin{equation}
    -(I - \K_k C_{Y|X})\mathcal{P}^-_{k}C_{Y|X}^* + \K_k \mathcal{R}_k = 0.
    \label{eq:kalman_gain_rel}
\end{equation}
Con esto
\begin{align*}
\K_k C_{Y|X}\mathcal{P}^-_{k}C_{Y|X}^* + \K_k \mathcal{R}_k &= \K^-_{k}C_{Y|X}^* \\
\K_k(C_{Y|X}\mathcal{P}^-_{k}C_{Y|X}^* + \mathcal{R}_k) &= \mathcal{P}^-_{k}C_{Y|X}^* .
\end{align*}

A priori se debe tener cuidado con invertir el operador que acompaña a $\K_k$, pero por un argumento de inyectividad se puede afirmar que

\begin{equation*} 
\K_k = \mathcal{P}^-_{k}C_{Y|X}^*(C_{Y|X}\mathcal{P}^-_{k}C_{Y|X}^* + \mathcal{R}_k)^{-1}.
\end{equation*}

Con esto se obtiene la expresión requerida para el operador de ganancia de Kalman.
\end{proof}

Recobrando la expresión para el operador de covarianza de error a posteriori

\begin{equation*}
\mathcal{P}_k = (I - \K_k C_{Y|X})\mathcal{P}^-_{k}(I - \K_k C_{Y|X})^* + \K_k \mathcal{R}_k \K_k^*,
\end{equation*}
se obtiene que
\begin{equation*}
\mathcal{P}_k = (I - \K_k C_{Y|X})\mathcal{P}^-_{k} - (I - \K_k C_{Y|X})\mathcal{P}^-_{k} C_{Y|X}^* \K_k^* + \K_k \mathcal{R}_k \K_k^*,
\end{equation*}
y con ello
\begin{equation*}
\mathcal{P}_k = (I - \K_k C_{Y|X})\mathcal{P}^-_{k} + (-(I - \K_k C_{Y|X})\mathcal{P}^-_{k} C_{Y|X}^*  + \K_k \mathcal{R}_k) \K_k^*,
\end{equation*}
que, recordando de \eqref{eq:kalman_gain_rel} que se tiene
\begin{equation*}
    -(I - \K_k C_{Y|X})\mathcal{P}^-_{k} C_{Y|X}^*  + \K_k \mathcal{R}_k
\end{equation*}
se llega a que
\begin{equation*}
    \mathcal{P}_k = (I - \K_k C_{Y|X})\mathcal{P}^-_{k}.
\end{equation*}

Entonces, con expresiones cerradas para el operador de ganancia de Kalman y los operadores de covarianza de error, las ecuaciones para cada iteración se expresan de la siguiente manera:
\begin{equation*}
	\begin{aligned}
		\hat{\mu}_{k+1}^- & = C_{X^+|X} \hat{\mu}_{k} \\
		\mathcal{P}_{k+1}^- & = C_{X^+|X} \mathcal{P}_k (C_{X^+|X})^* + \mathcal{Q}_{k+1} \\
		\S_{k+1} & = C_{Y|X} \mathcal{P}_{k+1}^- (C_{Y|X})^* + \mathcal{R}_{k+1} \\
		\K_{k+1} & = \mathcal{P}_{k+1}^- (C_{Y|X})^* \S_{k+1}^{-1} \\
		\mathcal{P}_{k+1} & = (I - \K_{k+1} C_{Y|X}) \mathcal{P}_{k+1}^- \\
		\hat{\mu}_{k+1} &= C_{X^+|X} \hat{\mu}_k + \K_{k+1} (\mathbf{y}_{k+1} - C_{Y|X} \hat{\mu}_{k+1}^-)
	\end{aligned}
\end{equation*}
con las condiciones iniciales:
\begin{equation*}
	\hat{\mu}_0 = \E [\Phi_\X (\mathbf{x}_0)], \quad \mathcal{P}_{0} = \text{Cov}(\Phi_\X (\mathbf{x}_0) - \hat{\mu}_0).
\end{equation*}

Ahora, expresando todo en términos del operador de Koopman, gracias a que
\begin{equation*}
	C_{X^+|X} = \U^*, \quad C_{Y|X} = \G^*
\end{equation*}
se obtiene:
\begin{equation*}
	\begin{aligned}
		\hat{\mu}_{k+1}^- & = \U^* \hat{\mu}_{k} \\
		\mathcal{P}_{k+1}^- & = \U^* \mathcal{P}_k \U + \mathcal{Q}_{k+1} \\
		\S_{k+1} & = \G^* \mathcal{P}_{k+1}^- \G + \mathcal{R}_{k+1} \\
		\K_{k+1} & = \mathcal{P}_{k+1}^- \G \S_{k+1}^{-1} \\
		\mathcal{P}_{k+1} & = (I + \K_{k+1} \G^*) \mathcal{P}_{k+1}^- \\
		\hat{\mu}_{k+1} &= \U^* \hat{\mu}_k + \K_{k+1} (\mathbf{y}_{k+1} - \G^* \hat{\mu}_{k+1}^-)
	\end{aligned}
\end{equation*}

Si ahora se realizan las aproximaciones finito dimensionales, se obtiene:
\begin{equation*}
	\begin{aligned}
		\hat{\mu}_{N, k+1}^- & = \U^*_N \hat{\mu}_{N, k} \\
		\mathcal{P}_{N, k+1}^- & = \U^*_N \mathcal{P}_{N,k} \U_N + \mathcal{Q}_{N, k+1} \\
		\K_{N,k+1} & = \mathcal{P}_{N, k+1}^- \G_N (\G^*_N \mathcal{P}_{N, k+1}^- \G_N + \mathcal{R}_{N, k+1})^{-1} \\
		\mathcal{P}_{N, k+1} & = (I + \K_{N,k+1} \G_N^*) \mathcal{P}_{N,k+1}^- \\
		\hat{\mu}_{N,k+1} &= \U^* \hat{\mu}_{N,k} + \K_{N,k+1} (\mathbf{y}_{k+1} - \G^*_N \hat{\mu}_{N,k+1}^-)
	\end{aligned}
\end{equation*}

En donde $\mathcal{Q}_{N,k+1}$, $\mathcal{R}_{N,k+1}$ son los estimadores insesgados de $\mathcal{Q}_{k+1}$ y $\mathcal{R}_{k+1}$, respectivamente, es decir:
\begin{equation}
	\mathcal{Q}_{N,k+1} = \frac{1}{N-1}\sum_{j=1}^N (z_{1,j} - \bar{z}_1)^2, \quad \mathcal{R}_{N,k+1} = \frac{1}{N-1}\sum_{j=1}^N (z_{2,j} - \bar{z}_2)^2
	\label{eq: emp_covars}
\end{equation}
donde $\{ z_{1,j} \}_{j=1}^N \sim \zeta_k^N$, $\{ z_{2,j} \}_{j=1}^N \sim \nu_k^N$ y 
\begin{equation*}
	\bar{z}_i = \frac{1}{N} \sum_{j=1}^N z_{i,j}
\end{equation*}
Si $X_0$ es la distribución dada para la condición inicial y $\{ x_j \}_{j=1}^N \sim X_0$, entonces la inicialización viene dada por:
\begin{equation}
	\hat{\mu}_{N,0} = \frac{1}{N} \sum_{j=1}^N \Phi_\X(x_{k}), \quad \mathcal{P}_{N,0} = \frac{1}{N-1} \sum_{j=1}^N (\Phi_\X(x_{k}) - \hat{\mu}_{N,0})^2
	\label{eq: mean_element_covar}
\end{equation}

Es con todo esto que se concluye la deducción del filtro, tanto en dimensión infinita como su representante en dimensión infinita, para este último se puede encontrar su pseudo-código en el algoritmo \ref{alg:KKKF}.

\begin{algorithm}[h]
\caption{Cov($W, N, \nu$)}\label{alg:AppCov}
\begin{algorithmic}[1]
\State \textbf{Entrada:} $W$ ley de una variable \textit{sampleable} con soporte en un conjunto $\Omega$, $N \geq 2$ cantidad de muestras a tomar, $\nu:\Omega \to \R^d$ función.
\Ensure $\hat{\Sigma} \in \R^{d \times d}$ estimación de matriz de covarianzas de $\nu(W)$.
\State $\{ \mathbf{w}_i \}_{i=1}^N \sim W^N$ \Comment{$N$ muestras independientes de $W$} 
\State $\hat{\nu} = \frac{1}{N} \sum_{i=1}^N \nu (\mathbf{w}_i)$ \Comment{Promedio empírico}
\State $\hat{\Sigma} = \frac{1}{N-1} \sum_{i=1}^N (\nu (\mathbf{w}_i) - \hat{\nu})^T(\nu (\mathbf{w}_i) - \hat{\nu})$  \Comment{Covarianza muestral insesgada}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Kernel Kalman Koopman Filter (KKKF)}\label{alg:KKKF}
\begin{algorithmic}[1]
\State \textbf{Entrada:} Dinámica discreta como en (\ref{eq:no_lin_dis_chap3}), $\mathbf{x}_0$ \textit{prior} sobre la condición inicial, $\mathbf{y}_{1:N}$ observaciones, $\mathbf{k}:\X \times \X \to \R$ \textit{kernel} semidefinido positivo, $N$ dimensión de aproximación del operador de Koopman y $n_{\text{samples}}$ cantidad de muestras para aproximar las matrices de covarianza.
\State \textbf{Salida:} $(\hat{\mathbf{x}}_{k|k})_{k=0}^{N}$ estimador de la trayectoria y $(\hat{\mathbf{P}}^{\mathbf{x}}_{k|k})_{k=0}^{N}$ matrices de covarianza de error.
\State $\mathbf{U}_N, \, \Phi_N (\cdot), \, \mathbf{G}_N, \mathbf{B}_N \gets $ kEDMD($\mu_\X$, $\rho_f$, $\rho_g$, $k$, $N$)
\State $\hat{\mathbf{x}}_{0|0}   \gets \E [\mathbf{x}_0]$ \Comment{Estimación de la condición inicial para $\mathbf{x}$}
\State $\hat{\mathbf{z}}_{0|0}   \gets \mathbf{\Phi}_N(\hat{\mathbf{x}}_{0|0})$ \Comment{Estimación de la condición inicial para $\mathbf{z}$}
\State $\hat{\mathbf{P}}^\mathbf{x}_{0|0} \gets \E [(\mathbf{x}_0 - \hat{\mathbf{x}}_{0})(\mathbf{x}_0 - \hat{\mathbf{x}}_{0})^T]$ \Comment{Covarianza de error inicial para $\mathbf{x}$}
\State $\hat{\mathbf{P}}^\mathbf{z}_{0|0} \gets$ Cov($\mathbf{x}_0$, $n_{\text{samples}}, \mathbf{\Phi}_N$) \Comment{Covarianza de error inicial para $\mathbf{z}$}
\For{$k = 0, \dots, N-1$}
    \State $\hat{\mathbf{x}}_{k+1|k} \gets \Tilde{\mathbf{f}}(\mathbf{x}_{k|k})$
    \Comment{Estimación a priori para $\mathbf{x}$}
    \State $\hat{\mathbf{z}}_{k+1|k} \gets \mathbf{\Phi}_N(\hat{\mathbf{x}}_{k+1|k})$
    \Comment{Estimación a priori para $\mathbf{z}$}
    \State $\mathbf{Q}_k \gets $ Cov($\mathbf{w}_k$, $n_{\text{samples}}, \mathbf{\Phi}_N(\mathbf{f}(\mathbf{x}_{k|k}, \cdot))$) 
    \Comment{Covarianza de la dinámica para $\mathbf{z}$}
    \State $\mathbf{P}^{\mathbf{z}}_{k+1|k} \gets \mathbf{U} \mathbf{P}^{\mathbf{z}}_{k|k} \mathbf{U}^T + \mathbf{Q}_k$
    \Comment{Error de covarianza a priori}
    \State $\hat{\mathbf{y}}_{k+1|k} \gets \Tilde{\mathbf{g}}(\hat{\mathbf{x}}_{k+1|k})$ 
    \Comment{Estimación de observación a priori}
    \State $\mathbf{e}_{\mathbf{y}_{k+1|k}} \gets \mathbf{y}_{k+1} - \hat{\mathbf{y}}_{k+1|k}$
    \Comment{Error a priori (innovación)}
    \State $\mathbf{R}_{k+1} \gets $ Cov($\mathbf{v}_k$, $n_{\text{samples}}, \mathbf{g}(\mathbf{x}_{k+1|k}, \cdot)$) 
    \Comment{Covarianza de la observación para $\mathbf{z}$}
    \State $ \mathbf{S}_{k+1} \gets \mathbf{C} \mathbf{P}^{\mathbf{z}}_{k|k} \mathbf{C}^T + \mathbf{R}_{k+1}$
    \Comment{Covarianza residual para $\mathbf{z}$}
    \State $\mathbf{K}_{k+1} \gets \mathbf{P}^{\mathbf{z}}_{k+1|k} \mathbf{C}^T$Cholesky$(\mathbf{S}_{k+1})^{-1}$
    \Comment{Ganancia de Kalman}
    \State $\hat{\mathbf{z}}_{k+1|k+1} \gets \hat{\mathbf{z}}_{k+1|k} + \mathbf{K}_{k+1} \mathbf{e}_{\mathbf{y}_{k+1|k}}$
    \Comment{Estimación a posteriori para $\mathbf{z}$}
    \State $\hat{\mathbf{x}}_{k+1|k+1} \gets \mathbf{B}\hat{\mathbf{z}}_{k+1|k+1}$
    \Comment{\textit{Lift back} para el estado}
    \State $\mathbf{P}^\mathbf{z}_{k+1|k+1} \gets (\mathbf{I} - \mathbf{K}_{k+1} 
    \mathbf{C}) \mathbf{P}^{\mathbf{z}}_{k+1|k}$
    \Comment{Error de covarianza a posteriori para $\mathbf{z}$}
    \State $\mathbf{P}^\mathbf{x}_{k+1|k+1} \gets \mathbf{B}\mathbf{P}^\mathbf{z}_{k+1|k+1} \mathbf{B}^T$
    \Comment{\textit{Lift back} para la covarianza}
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Cota de error de KKKF}

El primer paso para deducir la cota de error es dejar la discrepancia en norma de los elementos a comparar, en un cierto instante $k$, en función del instante anterior $k-1$ y en función de los operadores involucrados.

\begin{prop}
	Para $k \geq 1$, existen constantes $c_{k,j}^i$ con $j \in \{ 1, \dots, 6\}$, $i \in \{ 1, 2\}$ tales que
	\begin{equation*}
		\begin{aligned}
			\| \hat \mu_{k} - \hat \mu_{N,k}  \| \leq & \, c_{1,k}^1 \| \U - \U_N \| +  c_{2,k}^1 \| \G - \G_N \| \\ 
			&+ c_{3,k}^1 \| \mathcal{Q}_{k} - \mathcal{Q}_{N, k} \| +c_{4,k}^1 \| \mathcal{R}_{k} - \mathcal{R}_{N, k} \| \\
			& + c_{5,k}^1 \| \hat \mu_{k-1} - \hat \mu_{N, k-1} \| + c_{6,k}^1 \| \mathcal{P}_{k-1} - \mathcal{P}_{N, k-1} \|
		\end{aligned}
		\label{}
	\end{equation*}
	\begin{equation*}
		\begin{aligned}
			\| \mathcal{P}_{k} - \mathcal{P}_{N,k} \| \leq & \, c_{1,k}^2 \| \U - \U_N \| +  c_{2,k}^2 \| \G - \G_N \| \\ 
			&+ c_{3,k}^2 \| \mathcal{Q}_{k} - \mathcal{Q}_{N, k} \| +c_{4,k}^2 \| \mathcal{R}_{k} - \mathcal{R}_{N, k} \| \\
			& + c_{5,k}^2 \| \hat \mu_{k-1} - \hat \mu_{N, k-1} \| + c_{6,k}^2 \| \mathcal{P}_{k-1} - \mathcal{P}_{N, k-1} \|
		\end{aligned}
	\end{equation*}
	En donde las constantes $c_{k,j}^i$ son positivas y dependen de $k$ solo a través de $\| \U \| $, $\| \G \| $, $\| \mathcal{Q}_{k} \| $, $\| \mathcal{R}_{k} \| $, $\| \S_{k}^{-1} \| $, $\| \hat{\mu}_{k-1} \| $ y $\| \mathcal{P}_{k-1} \| $.
	\label{prop:err_kkkf_1}
\end{prop}

\begin{proof}
    Esto es directo del teorema \ref{teo:error_kalman}, ocupando
    \begin{equation*}
        A_{1,k} = \U, \quad A_{2,k} = \U_N 
    \end{equation*}
    \begin{equation*}
        C_{1,k} = \G, \quad C_{2,k} = \G_N 
    \end{equation*}
    \begin{equation*}
        \mathcal{Q}_{1,k} = \mathcal{Q}_k, \quad \mathcal{Q}_{2,k} = \mathcal{Q}_{N, k}
    \end{equation*}
    \begin{equation*}
        \mathcal{R}_{1,k} = \mathcal{R}_k, \quad \mathcal{R}_{2,k} = \mathcal{R}_{N, k}
    \end{equation*}
    \begin{equation*}
        y_{1, k} = y_{2, k} = \mathbf{y}_k.
    \end{equation*}
    Es decir, no se tiene el error por diferencia en las observaciones, ya que se considera que ambos sistemas tienen las mismas observaciones.
\end{proof}

\begin{teo}
    Bajo las hipótesis del teorema \ref{teo:error_koop}, sea $\delta \in (0, 1)$, si $\H_\X$ es equivalente en norma $H^{\nu + n/2}$, entonces con probabilidad $1-\delta$ se tiene que existen constantes $C_i^j$ para $i \in \{1, 2, 3\}$, $j \in \{1, 2\}$ tales que
	
	\begin{equation*}
		\begin{aligned}
			\| \hat \mu_{k} - \hat \mu_{N,k}  \| \leq & \, C_{k}^1 N^{-1/2}
		\end{aligned}
	\end{equation*}
	\begin{equation*}
		\begin{aligned}
			\| \mathcal{P}_{k} - \mathcal{P}_{N,k}  \| \leq & \, C_{k}^2 N^{-1/2} 
		\end{aligned}
	\end{equation*}
	En donde las constantes $C_i^j$ son positivas y dependen de $k$ solo a través de $\| \U \| $, $\| \G \| $, $\| \mathcal{Q}_{j} \| $, $\| \mathcal{R}_{j} \| $, $\| \S_{k}^{-1} \| $, $\| \hat{\mu}_{j} \| $ y $\| \mathcal{P}_{j} \| $, con $j \in \{ 0, \dots, k-1\}$.
    \label{teo:teo_kkkf_2}
\end{teo}

Antes de probar esta proposición, se enuncia un lema que permite dar cotas para los elementos y operadores cuya norma se puede acotar por algo de orden $N^{-1/2}$, que son resultados conocidos en la literatura.
\begin{lema}[Zhou et al. \cite{Zhou2019ASpaces}] Sea $N \in \N$ y $\mathcal{Q}_{N,0}$, $\mathcal{R}_{N,0}$, $\mu_{N,0}$, $\mathcal{P}_{N,0}$, definidos en \ref{eq: emp_covars} y \ref{eq: mean_element_covar}, respectivamente, luego existe una constante $C$ tal que
	\begin{equation*}
		\| \hat{\mu}_0 - \hat{\mu}_{N,0} \|_{\H_\X}, \, \|  \mathcal{P}_{0} -  \mathcal{P}_{N,0}\|_{HS}, \, \, \| \mathcal{Q}_{k} - \mathcal{Q}_{N, k} \|_{HS}, \| \mathcal{R}_{k} - \mathcal{R}_{N, k} \|_{HS} \leq C\cdot N^{-1/2}
	\end{equation*}
	que, sin pérdida de generalidad, se puede tomar común para todas las cotas.
	\label{lema:oper_sqrt_N}
\end{lema}
Ahora se procede con la demostración de la Proposición \ref{teo:teo_kkkf_2}.
\begin{proof}
	Gracias a la proposición \ref{prop:err_kkkf_1} y el lema \ref{lema:oper_sqrt_N} se obtiene que existen constantes $c_{k,j}^i$ con $j \in \{ 1, \dots, 6\}$, $i \in \{ 1, 2\}$ tales que
	\begin{equation*}
		\begin{aligned}
			\| \hat \mu_{k} - \hat \mu_{N,k}  \| \leq & \, c_{1,k}^1 \| \U - \U_N \| +  c_{2,k}^1 \| \G - \G_N \| \\ 
			&+ c_{3,k}^1 C N^{-1/2}+c_{4,k}^1 C N^{-1/2} \\
			& + c_{5,k}^1 \| \hat \mu_{k-1} - \hat \mu_{N, k-1} \| + c_{6,k}^1 \| \mathcal{P}_{k-1} - \mathcal{P}_{N, k-1} \|
		\end{aligned}
		\label{}
	\end{equation*}
	\begin{equation*}
		\begin{aligned}
			\| \mathcal{P}_{k} - \mathcal{P}_{N,k} \| \leq & \, c_{1,k}^2 \| \U - \U_N \| +  c_{2,k}^2 \| \G - \G_N \| \\ 
			&+ c_{3,k}^2 C N^{-1/2}+c_{4,k}^2 C N^{-1/2} \\
			& + c_{5,k}^2 \| \hat \mu_{k-1} - \hat \mu_{N, k-1} \| + c_{6,k}^2 \| \mathcal{P}_{k-1} - \mathcal{P}_{N, k-1} \|
		\end{aligned}
	\end{equation*}
	Por teorema \ref{teo:error_koop_sqrt_N} entonces se concluye que con probabilidad $1-\delta$ existen constantes $C^1_{1, k}$ y $C^2_{1, k}$ tales que
	\begin{equation*}
		\begin{aligned}
			\| \hat \mu_{k} - \hat \mu_{N,k}  \| \leq & \, C_{1,k}^1 N^{-1/2} + C_{2,k}^1 \| \hat \mu_{k-1} - \hat \mu_{N, k-1} \| + C_{3,k}^1 \| \mathcal{P}_{k-1}  - \mathcal{P}_{N, k-1}  \|
		\end{aligned}
	\end{equation*}
	\begin{equation*}
		\begin{aligned}
			\| \mathcal{P}_{k} - \mathcal{P}_{N,k}  \| \leq & \, C_{1,k}^2 N^{-1/2} + C_{2,k}^2 \| \hat \mu_{k-1} - \hat \mu_{N, k-1} \| + C_{3,k}^2 \| \mathcal{P}_{k-1} - \mathcal{P}_{N,k-1} \|
		\end{aligned}
	\end{equation*}
	Para propagar el error hasta la condición inicial se procede por inducción. Para ello primero el caso base $k=1$ que se tiene directo por el teorema \ref{teo:error_kalman} aplicado a $k=1$.\\
	Se supone entonces que para $k \in \N$ se cumple 
	\begin{equation*}
		\begin{aligned}
			\| \hat \mu_{k} - \hat \mu_{N,k}  \| \leq & \, C_{1,k}^1 N^{-1/2} + C_{2,k}^1 \| \hat \mu_{0} - \hat \mu_{N, 0} \| + C_{3,k}^1 \| \mathcal{P}_{0}  - \mathcal{P}_{N, 0}  \|
		\end{aligned}
	\end{equation*}
	\begin{equation*}
		\begin{aligned}
			\| \mathcal{P}_{k} - \mathcal{P}_{N,k}  \| \leq & \, C_{1,k}^2 N^{-1/2} + C_{2,k}^2 \| \hat \mu_{0} - \hat \mu_{N, 0} \| + C_{3,k}^2 \| \mathcal{P}_{0} - \mathcal{P}_{N, 0} \|
		\end{aligned}
	\end{equation*}
	Ahora se prueba para $k+1$, que basta hacerlo para $\| \hat \mu_{k+1} - \hat \mu_{N,k+1}  \|$, para la otra cota análoga.
	\begin{equation*}
	\begin{aligned}
		\| \hat \mu_{k+1} - \hat \mu_{N,k+1}  \| \leq & \, C_{1,k+1}^1 N^{-1/2} + c_{5,k+1}^1 \| \hat \mu_{1, k} - \hat \mu_{2, k} \| + c_{6,k+1}^1 \| \mathcal{P}_{k}  - \mathcal{P}_{N, k}  \| 
	\end{aligned}
	\end{equation*}
	Ocupando la hipótesis inductiva
	\begin{equation*}
	\begin{aligned}
		\leq & \, C_{1,k}^1 N^{-1/2} \\
		& + c_{5,k+1}^1 (C_{1,k}^1 N^{-1/2} + C_{2,k}^1 \| \hat \mu_{0} - \hat \mu_{N, 0} \| + C_{3,k}^1 \| \mathcal{P}_{0}  - \mathcal{P}_{N, 0}  \|) \\
		& + c_{6,k+1}^1 (C_{1,k}^2 N^{-1/2} + C_{2,k}^2 \| \hat \mu_{0} - \hat \mu_{N, 0} \| + C_{3,k}^2 \| \mathcal{P}_{0} - \mathcal{P}_{N, 0} \|) \\
		= & \, C_{1,k+1}^1 N^{-1/2} + C_{2,k+1}^1 \| \hat \mu_{0} - \hat \mu_{N, 0} \| + C_{3,k+1}^1 \| \mathcal{P}_{0} - \mathcal{P}_{N, 0} \|
	\end{aligned}
	\end{equation*}
	Usando el lema \ref{lema:oper_sqrt_N} se obtiene que existe una constante $C_{k+1}^1$ tal que
	\begin{equation*}
		\begin{aligned}
			\| \hat \mu_{k} - \hat \mu_{N,k}  \| \leq & \, C_{k+1}^1 N^{-1/2}
		\end{aligned}
	\end{equation*}
\end{proof}

Ahora se debe estudiar el error inducido por volver al espacio de dimensión original mediante el operador de \textit{lifting back} $\B : \R^n \to \X$ definido en la sección anterior, que cumple
\begin{equation*}
    \B^* \Phi_\X (\mathbf{x}) = \mathbf{x}.
\end{equation*}

Se propone entonces el estimador exacto del problema como
\begin{equation*}
    \hat{\mathbf{x}}_{k} = \B^* \hat{\mu}_{k}
\end{equation*}
que en la práctica es inaccesible, pero que se aproxima por el estimador
\begin{equation*}
    \hat{\mathbf{x}}_{N, k} = \B^*_{N} \hat{\mu}_{N,k}.
\end{equation*}
Este será el estimador que entregará el algoritmo de filtraje KKKF. Además, recordando que la matriz de covarianza de error a posteriori del problema está definida por
\begin{equation*}
    \mathbf{P}_{k} = \E[ (\hat{\mathbf{x}}_k - \mathbf{x}_k) (\hat{\mathbf{x}}_k - \mathbf{x}_k)^\top ]
\end{equation*}
se obtiene que
\begin{equation*}
    \begin{aligned}
        \mathbf{P}_{k} & = \E[ (\hat{\mathbf{x}}_k - \mathbf{x}_k) (\hat{\mathbf{x}}_k - \mathbf{x}_k)^\top ] \\
        & = \E[ (\B^* \Phi_\X (\hat{\mathbf{x}}_k) - \B^* \Phi_\X (\mathbf{x}_k)) (\B^* \Phi_\X (\hat{\mathbf{x}}_k) - \B^* \Phi_\X (\mathbf{x}_k))^\top ] \\
        & = \B^* \E[ (\Phi_\X (\hat{\mathbf{x}}_k) - \Phi_\X (\mathbf{x}_k)) (\Phi_\X (\hat{\mathbf{x}}_k) -  \Phi_\X (\mathbf{x}_k))^* ] \B \\
        & = \B^* \E[ (\mu_k - \hat{\mu}_{k}) (\mu_k - \hat{\mu}_{k})^*] \B \\
        & = \B^* \mathcal{P}_k \B.
    \end{aligned}
\end{equation*}
Por lo que la matriz de covarianza de error a posteriori aproximante se define como
\begin{equation*}
    \mathbf{P}_{N, k} = \B^*_{N} \mathcal{P}_{N, k} \B_{N}
\end{equation*}
Con esto se puede deducir de manera sencilla la cota de error deseada para la aproximación del filtro.
\begin{teo}[Error de KKKF]
    Bajo las hipótesis del teorema \ref{teo:error_koop}, sea $\delta \in (0, 1)$, si $\H_\X$ es equivalente en norma $H^{\nu + n/2}$, entonces con probabilidad $1-\delta$ se tiene que existen constantes $\Tilde{C}^1_k$, $\Tilde{C}^2_k$ tales que
    \begin{equation*}
        \| \hat{\mathbf{x}}_k - \hat{\mathbf{x}}_{N, k} \| \leq \Tilde{C}^1_k N^{-1/2}
    \end{equation*}
    \begin{equation*}
        \| \mathbf{P}_k - \mathbf{P}_{N, k} \| \leq \Tilde{C}^2_k N^{-1/2}
    \end{equation*}
    \label{teo:error_kkkf_fin}
\end{teo}
\begin{proof}
    Primero para el término asociado a la estimación del estado
    \begin{equation*}
        \begin{aligned}
            \| \hat{\mathbf{x}}_k - \hat{\mathbf{x}}_{N, k} \| & = \| \B^* \hat{\mu}_{k} - \B^*_{N} \hat{\mu}_{N,k} \| \\
            & = \| \B^* \hat{\mu}_{k} - \B^* \hat{\mu}_{N, k} + \B^* \hat{\mu}_{N, k} - \B^*_{N} \hat{\mu}_{N,k} \| \\
            & \leq \| \B^* \hat{\mu}_{k} - \B^* \hat{\mu}_{N, k}\| + \|\B^* \hat{\mu}_{N, k} - \B^*_{N} \hat{\mu}_{N,k} \| \\
            & \leq \| \B^* \| \| \hat{\mu}_{k} -  \hat{\mu}_{N, k}\| + \|\B^* - \B^*_{N}  \| \|\hat{\mu}_{N,k} \|.
        \end{aligned}
    \end{equation*}
    Entonces, por teoremas \ref{teo:error_koop_sqrt_N} y \ref{teo:teo_kkkf_2} se tiene que con probabilidad $1-\delta$ existe constantes $C$, $C_k^1$ tales que 
    \begin{equation*}
         \| \hat{\mathbf{x}}_k - \hat{\mathbf{x}}_{N, k} \| \leq \| \B \| C_k^1 N^{-1/2} + \| \hat{\mu}_{N, k} \| C N^{-1/2} = \Tilde{C}_{k}^1 N^{-1/2}
    \end{equation*}
    donde se ha utilizado $\| \B^* \| \leq \| \B \|$, $\| \B^* - \B^*_N \| \leq \| \B - \B_N \|$ y denotado $\Tilde{C}_{k}^1 = \| \B \| C_k^1 + \| \hat{\mu}_{N, k} \| C N^{-1/2}$.

    Ahora para la matriz de covarianza de error a posteriori
    \begin{equation*}
        \begin{aligned}
            \| \mathbf{P}_k - \mathbf{P}_{N, k} \| & = \| \B^*\mathcal{P}_k \B - \B^*_N \mathcal{P}_{N, k} \B_N \| \\
            & = \| \B^*\mathcal{P}_k \B - \B^*_N \mathcal{P}_k \B + \B^*_N \mathcal{P}_k \B - \B^*_N \mathcal{P}_{N, k} \B_N \| \\
            & \leq \| \B^*\mathcal{P}_k \B - \B^*_N \mathcal{P}_k \B \| + \| \B^*_N \mathcal{P}_k \B - \B^*_N \mathcal{P}_{N, k} \B_N \| \\
            & \leq \| \B^* - \B^*_N  \| \| \mathcal{P}_k \B \| + \| \B^*_N \| \| \mathcal{P}_k \B - \mathcal{P}_{N, k} \B_N \| \\
            & = \| \B - \B_N  \| \| \mathcal{P}_k \B \| + \| \B_N \| \| \mathcal{P}_k \B -\mathcal{P}_k \B_N + \mathcal{P}_k \B_N - \mathcal{P}_{N, k} \B_N \| \\
            & \leq \| \B - \B_N  \| \| \mathcal{P}_k \B \| + \| \B_N \| \left (\| \mathcal{P}_k \B -\mathcal{P}_k \B_N \| + \| \mathcal{P}_k \B_N - \mathcal{P}_{N, k} \B_N \| \right ) \\
            & \leq \| \B - \B_N  \| \| \mathcal{P}_k \B \| + \| \B_N \| \left ( \| \mathcal{P}_k \B -\mathcal{P}_k \B_N \| + \| \mathcal{P}_k \B_N - \mathcal{P}_{N, k} \B_N \| \right ) \\
            & \leq \| \B - \B_N  \| \| \mathcal{P}_k \B \| + \| \B_N \| \left ( \| \B - \B_N \| \| \mathcal{P}_k \| + \| \mathcal{P}_k - \mathcal{P}_{N, k} \| \| \B_N \| \right ).
        \end{aligned}
    \end{equation*}
    Usando nuevamente los teoremas \ref{teo:error_koop_sqrt_N} y \ref{teo:teo_kkkf_2} se tiene que probabilidad $1-\delta$ existen constantes $C$, $C_k^2$ tales que 
    \begin{equation*}
        \| \mathbf{P}_k - \mathbf{P}_{N, k} \| \leq C_k^2 \| \mathcal{P}_k \B \| N^{-1/2} + \| \B_N \| \left ( C \| \mathcal{P}_k \| + C_k^2 \| \B_N \| \right ) N^{-1/2}
    \end{equation*}
    y además
    \begin{equation*}
        \| \B_N \| = \| \B_N - \B + \B \| \leq \| \B_N - \B \| + \| \B \| \leq C N^{-1/2} + \| \B \| 
    \end{equation*}
    con lo que se obtiene
    \begin{equation*}
        \begin{aligned}
            \| \mathbf{P}_k - \mathbf{P}_{N, k} \| & \leq C_k^2 \| \mathcal{P}_k \B \| N^{-1/2} + (C N^{-1/2} + \| \B \|) \left ( C \| \mathcal{P}_k \| + C_k^2 (C N^{-1/2} + \| \B \|) \right ) N^{-1/2} \\
            & \leq \left ( C_k^2 \| \mathcal{P}_k \B \| + (C N^{-1/2} + \| \B \|) \left ( C \| \mathcal{P}_k \| + C_k^2 (C N^{-1/2} + \| \B \|) \right ) \right ) N^{-1/2} \\
            & \leq \left ( C_k^2 \| \mathcal{P}_k \B \| + (C + \| \B \|) \left ( C \| \mathcal{P}_k \| + C_k^2 (C  + \| \B \|) \right ) \right ) N^{-1/2}
        \end{aligned}
    \end{equation*}
    con lo que denotando $\Tilde{C}_k^2 = C_k^2 \| \mathcal{P}_k \B \| + (C + \| \B \|) \left ( C \| \mathcal{P}_k \| + C_k^2 (C  + \| \B \|) \right )$ se tiene que 
    \begin{equation*}
        \| \mathbf{P}_k - \mathbf{P}_{N, k} \| \leq \Tilde{C}_k^2 N^{-1/2}
    \end{equation*}
    completando el resultado.
\end{proof}
Con esta cota, en conjunto con otros desarrollos realizados en secciones anteriores se puede probar el siguiente resultado sobre el sesgo y el error del estimador.
\begin{teo}
    Bajo la hipótesis del teorema \ref{teo:error_koop}, sea $\delta \in (0, 1)$, luego con probabilidad $1-\delta$ existen constantes $\hat{C}_1$ y $\hat{C}_2$ tales que
    \begin{enumerate}
        \item El sesgo del estimador $\hat{\mathbf{x}}_{N, k}$, para la trayectoria $\mathbf{x}_k$, está acotado por $\hat{C}_1 N^{-1/2}$, esto es
    \begin{equation*}
        \left \| \E \left [ (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k) \right] \right \| \leq \hat{C}_1 N^{-1/2}
    \end{equation*}
        \item El estimador $\hat{\mathbf{x}}_{N, k}$ es un $\hat{C}_2 N^{-1/2}$-mínimo del problema de filtraje, esto es
    \begin{equation*}
        \E \left [ (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k) \right] \leq  \E \left [ (\hat{\mathbf{x}}_{k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{k} - \mathbf{x}_k) \right] + \Tilde{C}_k^2 N^{-1/2}
    \end{equation*}
    \end{enumerate}
\end{teo}
\begin{proof}
    Primero para el punto 1. se tiene que
    \begin{equation*}
        \begin{aligned}
            \| \E [\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k] \| & = \| \E [ \B^*_N \hat{\mu}_{N,k} - \B^*_N \hat{\mu}_{k} + \B^*_N \hat{\mu}_{k} - \B^* \hat{\mu}_k + \B^* \hat{\mu}_k - \B^* \mu_k ] \| \\
            &  = \| \E [ \B^*_N \hat{\mu}_{N,k} - \B^*_N \hat{\mu}_{k} ] + \E [\B^*_N \hat{\mu}_{k} - \B^* \hat{\mu}_k ] + \E [\B^* \hat{\mu}_k - \B^* \mu_k ] \| \\
            & = \| \E [ \B^*_N \hat{\mu}_{N,k} - \B^*_N \hat{\mu}_{k} ] + \E [\B^*_N \hat{\mu}_{k} - \B^* \hat{\mu}_k ] + \B^* \E [ \hat{\mu}_k - \mu_k ] \|.
        \end{aligned}
    \end{equation*}
    Gracias a la proposición \ref{prop:unbias_kalman_operator} se tiene que $\hat{\mu}_k$ es insesgado para $\mu_k$ con lo que
    \begin{equation*}
        \begin{aligned}
            \| \E [\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k] \| 
            & = \| \E [ \B^*_N \hat{\mu}_{N,k} - \B^*_N \hat{\mu}_{k} ] + \E [\B^*_N \hat{\mu}_{k} - \B^* \hat{\mu}_k ] + \B^* \E [ \hat{\mu}_k - \mu_k ] \| \\
            & = \| \E [ \B^*_N \hat{\mu}_{N,k} - \B^*_N \hat{\mu}_{k} ] + \E [\B^*_N \hat{\mu}_{k} - \B^* \hat{\mu}_k ] \| \\
            & \leq  \E [ \| \B^*_N \hat{\mu}_{N,k} - \B^*_N \hat{\mu}_{k} \| ] + \E [\| \B^*_N \hat{\mu}_{k} - \B^* \hat{\mu}_k \|] \\
            & \leq \E [\| \B^*_N \| \| \hat{\mu}_{N,k} - \hat{\mu}_{k}\|] + \E[\| \B^*_N - \B^* \| \| \hat{\mu}_k \|] \\
            & \leq \hat{C}_1 N^{-1/2}
        \end{aligned}
    \end{equation*}
    donde se han utilizado nuevamente los teoremas \ref{teo:error_koop_sqrt_N} y \ref{teo:teo_kkkf_2} para decir que dicha constante $\hat{C}_1$ existe con probabilidad a lo menos $1-\delta$. Para probar el punto 2. notar que
    \begin{equation*}
        \E [ (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k) ] = \E [\hat{\mathbf{x}}_{N, k}^\top \hat{\mathbf{x}}_{N, k}] - 2 \E [\hat{\mathbf{x}}_{N, k}^\top \mathbf{x}_k] + \E[\mathbf{x}_k^\top \mathbf{x}_k]
    \end{equation*}
    \begin{equation*}
        \E [ (\hat{\mathbf{x}}_{k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{k} - \mathbf{x}_k) ] = \E [\hat{\mathbf{x}}_{k}^\top \hat{\mathbf{x}}_{k}] - 2 \E [\hat{\mathbf{x}}_{k}^\top \mathbf{x}_k] + \E[\mathbf{x}_k^\top \mathbf{x}_k].
    \end{equation*}
    Restando ambas se tiene 
    \begin{equation*}
        \begin{aligned}
            \E & [ (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k) ] - \E [ (\hat{\mathbf{x}}_{k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{k} - \mathbf{x}_k) ] \\
            & =  \E [\hat{\mathbf{x}}_{N, k}^\top \hat{\mathbf{x}}_{N, k}] - \E [\hat{\mathbf{x}}_{k}^\top \hat{\mathbf{x}}_{k}] + 2 \E [(\hat{\mathbf{x}}_{k} - \hat{\mathbf{x}}_{N, k})^T \mathbf{x}_k] \\
            & = \E [(\hat{\mathbf{x}}_{N, k} - \hat{\mathbf{x}}_{k})^\top (\hat{\mathbf{x}}_{N, k} + \hat{\mathbf{x}}_{k})] + 2 \E [(\hat{\mathbf{x}}_{k} - \hat{\mathbf{x}}_{N, k})^T \mathbf{x}_k]
        \end{aligned}   
    \end{equation*}
    Notar que dado que $\hat{\mathbf{x}}_k$ es óptimo del problema de filtraje, se tiene que 
    \begin{equation*}
        0 \leq \E [ (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k) ] - \E [ (\hat{\mathbf{x}}_{k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{k} - \mathbf{x}_k) ].
    \end{equation*}
    Por lo que, y utilizando Cauchy-Schwarz se tiene
    \begin{equation*}
        \begin{aligned}
            0 & \leq \E [(\hat{\mathbf{x}}_{N, k} - \hat{\mathbf{x}}_{k})^\top (\hat{\mathbf{x}}_{N, k} + \hat{\mathbf{x}}_{k})] + 2 \E [(\hat{\mathbf{x}}_{k} - \hat{\mathbf{x}}_{N, k})^T \mathbf{x}_k] \\
            & \leq \E [\| \hat{\mathbf{x}}_{N, k} - \hat{\mathbf{x}}_k \| \| \hat{\mathbf{x}}_{N, k} + \hat{\mathbf{x}}_k \|] + 2 \E [\| \hat{\mathbf{x}}_{N, k} - \hat{\mathbf{x}}_k \| \| \mathbf{x}_k \| ] \\
            & \leq \E [\| \hat{\mathbf{x}}_{N, k} - \hat{\mathbf{x}}_k \| ( \| \hat{\mathbf{x}}_{N, k} \| + \| \hat{\mathbf{x}}_k \|)] + 2 \E [\| \hat{\mathbf{x}}_{N, k} - \hat{\mathbf{x}}_k \| \| \mathbf{x}_k \| ] \\
            & \leq \hat{C}_2 N^{-1/2}
        \end{aligned}
    \end{equation*}
    donde se han utilizado nuevamente los teoremas \ref{teo:error_koop_sqrt_N} y \ref{teo:teo_kkkf_2} para decir que dicha constante $\hat{C}_2$ existe con probabilidad a lo menos $1-\delta$. Con ello
    \begin{equation*}
        \E [ (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k) ] - \E [ (\hat{\mathbf{x}}_{k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{k} - \mathbf{x}_k) ] \leq \hat{C}_2 N^{-1/2}
    \end{equation*}
    obteniendo que
    \begin{equation*}
        \E [ (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{N, k} - \mathbf{x}_k) ] \leq \E [ (\hat{\mathbf{x}}_{k} - \mathbf{x}_k)^\top (\hat{\mathbf{x}}_{k} - \mathbf{x}_k) ] + \hat{C}_2 N^{-1/2}
    \end{equation*}
    por lo que $\hat{\mathbf{x}}_{N, k}$ es $\hat{C}_2 N^{-1/2}$-mínimo del problema de filtraje.
\end{proof}

\section{Metodología de Estimación de Parámetros}

Partiendo de la existencia de un algoritmo de filtraje, es posible desarrollar una metodología para la estimación de parámetros constantes y desconocidos en un sistema dinámico en tiempo discreto. 

La idea detrás de este enfoque radica en que, si el sistema o el algoritmo logra integrar de manera efectiva la premisa de que debe existir un parámetro que se ajuste a las observaciones, entonces con la estabilización proporcionada por el operador de ganancia de Kalman, debería permitir que el parámetro constante se aproxime a su valor real conforme las iteraciones temporales del algoritmo de filtraje tiendan a infinito.

De manera formal, si el sistema tiene $n_p$ parámetros sujetos a estimación, es decir, desconocidos en la práctica, denotados por $\mathbf{p} \in \R^{n_p}$, entonces se agregan como estados que tienen derivada discreta nula, con un ruido aditivo centrado en $0$ y segundo momento finito, esto es
\begin{equation*}
    \mathbf{x}_{k+1}^{n+i} = \mathbf{x}_{k}^{n+1} + \Tilde{\mathbf{w}}_k^i, \quad i = 1, \dots, n_p,
\end{equation*}
este ruido $\mathbf{w}_k^i$ ayudará a la convergencia del parámetro a través de las iteraciones. Lo esperado es que
\begin{equation*}
    \mathbf{x}_{k}^{n+1} \to \mathbf{p}_{i}, \quad k \to \infty
\end{equation*}
y esto se observará en experimentos numéricos.

Este enfoque no es novedoso y ya ha sido explorado en la literatura, por ejemplo en \cite{Deng2013AdaptiveObjects, Jiang2007AEstimation, Kandepu2008ApplyingEstimation}. Sin embargo, en esta sección se propone una reinterpretación de dicha metodología para la estimación de parámetros. Además, se busca establecer una equivalencia con los métodos existentes en otras áreas de la literatura, como aquellos empleados en investigaciones aplicadas en las que se enfrenta incertidumbre sobre un parámetro a partir de observaciones. Un ejemplo destacado de estas técnicas son los algoritmos tipo Markov Chain Monte Carlo (MCMC) \cite{2010EncyclopediaLearning}.

Como ocurre en muchas aplicaciones, en particular aquellas que serán de interés en este trabajo, la cantidad de observaciones disponibles es finita, y en muchos casos extremadamente limitada. Por esta razón, considerar la posibilidad de que el algoritmo de filtraje pueda converger al parámetro real en el límite cuando las iteraciones temporales tienden a infinito no resulta una opción válida en este contexto.

Por ello, se propone realizar el filtraje de manera iterativa. En este enfoque, se establece un prior como condición inicial, permitiendo que el algoritmo de filtraje complete todas las iteraciones temporales disponibles. Posteriormente, el último elemento producido por el filtro se utiliza como una nueva condición inicial, aprovechando la matriz de covarianza de error en la última iteración como matriz de covarianza para la condición inicial y la dinámica, lo que haría que el mismo método regule su variabilidad.

Eso se puede repetir todas las veces que sean necesarias y debería, en teoría, converger eventualmente al parámetro original. Este comportamiento será evaluado empíricamente en las gráficas y elementos que se presentarán a continuación.

Es más, y haciendo un análogo con el algoritmo de Markov Chain Monte Carlo (MCMC), se puede aprovechar la paralelización de procesos, utilizando todos los núcleos disponibles para generar múltiples filtros que se ejecuten en paralelo. De este modo, se pueden obtener estimaciones distintas para cada uno de los parámetros, lo que potencialmente mejora la precisión y la robustez del proceso de estimación.

Notar que, para cada hilo o subproceso, se genera una serie de tiempos, en los cuales la unidad temporal corresponde al número de iteraciones que se asignen al algoritmo iterativo. En cada uno de estos tiempos se generará una estimación distinta para el parámetro. Esto da lugar, de manera natural, a una distribución de probabilidad que debería concentrarse en torno al parámetro real. A partir de esta distribución, se eliminarán las primeras iteraciones del algoritmo, las cuales se considerarán como iteraciones de \textit{warm-up} o calentamiento, un procedimiento comúnmente utilizado también en el contexto de Markov Chain Monte Carlo y como lo hacen ciertos \textit{frameworks} que implementan este método \cite{Patil2010PyMC:Python, Carpenter2017Stan:Language, Burkner2017Brms:Stan, Abril-Pla2023PyMC:Python}.

\begin{algorithm}[h]
\caption{ParamEstim($\mathbf{x}_0$, $\mathbf{p}_0$, $\mathbf{y}$, $\text{iters}$)}
\label{alg:ParamEstim}
\begin{algorithmic}[1]
\State \textbf{Entrada:} $\mathbf{\mathbf{x}}_0$ prior sobre los estados, $\mathbf{Q}_0$ covarianza a priori sobre los estados, $\mathbf{\mathbf{p}}_0$ prior sobre el parámetro, $\mathbf{\Tilde{P}}_0$ covarianza a priori sobre los estados, $\text{iters}$ la cantidad de iteraciones.
\State \textbf{Salida:} $\mathbf{\hat{p}} \in \R^{(\text{iters}+1)\times n_p}$ estimación de los parámetros en cada iteración.
\State $\hat{\mathbf{x}}_{0,:} \gets \hat{\mathbf{x}}_0$.
\State $\hat{\mathbf{p}}_{0,:} \gets \hat{\mathbf{p}}_0$.
\State Tomar como prior inicial para KKKF $\Tilde{\mathbf{x}} \gets (\hat{\mathbf{x}}_{0}, \hat{\mathbf{p}}_{0,:})$.
\State Tomar como covarianza inicial para KKKF 
\begin{equation*}
    \mathbf{P} = \begin{pmatrix}
        \mathbf{Q}_0 & 0 \\
        0            & \Tilde{\mathbf{P}}_0
    \end{pmatrix}
\end{equation*}
\For{k=1, \dots, \text{iters}} \\
    \quad $\hat{\mathbf{x}}, \, \hat{\mathbf{P}} \gets $ KKKF($\Tilde{\mathbf{x}}, \mathbf{P}, \mathbf{y})$. \Comment{$\mathbf{y}$ corresponden a las observaciones.} \\
    \quad $\hat{\mathbf{p}}_{k,:} \gets \hat{\mathbf{x}}_{-1,n_p:}$ \Comment{Última iteración del filtro en las últimas $n_p$ entradas.} \\
    \quad $\Tilde{\mathbf{x}} \gets (\hat{\mathbf{x}}_{0}, \hat{\mathbf{p}}_{k,:})$ \\
    \quad Actualizar la matriz de covarianzas como
    \begin{equation*}
    \mathbf{P} = \begin{pmatrix}
        \mathbf{Q}_0 & 0 \\
        0            & \hat{\mathbf{P}}_{n_p:, n_p:}
    \end{pmatrix}
\end{equation*}
    \quad donde $\hat{\mathbf{P}}_{n_p:, n_p:}$ es la submatriz cuadradad en el último bloque de tamaño $n_p \times n_p$.
\EndFor
\end{algorithmic}
\end{algorithm}

En el algoritmo \ref{alg:ParamEstim} se puede observar el pseudo-código para la estimación de parámetros constantes propuestos y que se verá aplicado en la siguiente sección.

\section{Resultados numéricos}

A continuación se presentan los resultados numéricos para los aspectos presentados del filtro creado. Esto se hace en Python, como hay un factor estocástico importante en los experimentos se fija la semilla aleatoria en 42. Se implementan además los filtros de Kalman, Extended Kalman, Unscented Kalman y Filtro de Partículas con Importance Resampling, todas son implementaciones propias y están basadas en \cite{Setoodeh2022NonlinearApplications}.

\subsection{Comparación el filtro de Kalman en el caso lineal}

Se compara el KKKF con el filtro de Kalman, el cual entrega la solución analítica para el problema de filtraje en el caso en que la dinámica y la observación sean de la forma
\begin{equation*}
    \begin{aligned}
        \mathbf{x}_{k+1} &= \mathbf{A} \mathbf{x}_k + \mathbf{w}_k \\
        \mathbf{y}_k &= \mathbf{C} \mathbf{x}_k + \mathbf{v}_k.
    \end{aligned}
\end{equation*}
Para este experimento se utilizaron las matrices de la forma
\begin{equation*}
    \mathbf{A} =
    \begin{pmatrix}
        1.01 & 0.01 & 0.0 \\
        0.01 & 1.02 & \alpha \\
        0.0 & 0.04 & 1.02
    \end{pmatrix},
    \quad  \mathbf{C} = 
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0
    \end{pmatrix},
\end{equation*}
es decir, se supone que se observan el primer y el segundo estado, siendo el tercero el que se encuentra complementamente oculto. Notar que este sistema cumple ser observable en el sentido usual del control \cite{Trelat2013ControleApplications}, que es que la matriz de observabilidad del sistema
\begin{equation*}
    \mathcal{O} = \begin{pmatrix}
        C \\
        CA \\
        \vdots \\
        CA^{n-1}
    \end{pmatrix},
\end{equation*}
donde $n$ es la dimensión del estado, en este caso $n=3$. 

Se corrobora que el cálculo de la matriz de observabilidad en este caso entrega
\[
\mathcal{O} =
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
1.01 & 0.01 & 0 \\
0.01 & 1.02 & \alpha \\
1.0202 & 0.0203 & 0.01\alpha \\
0.0203 & 0.04\alpha + 1.0405 & 2.04\alpha
\end{pmatrix}.
\]
que es una matriz de rango completo al tener columnas linealmente independientes.

En las figuras \ref{fig:kalman_vs_kkkf_01}, \ref{fig:kalman_vs_kkkf_001} y \ref{fig:kalman_vs_kkkf_025} se muestran los resultados del filtro de Kalman y KKKF para tres casos de $\alpha$ para la matriz $\mathbf{A}$, comparando con la trayectoria real sin ruido, ni de dinánica ni de observación.

Se utiliza como condición inicial real del sistema $\mathbf{x}_0 = (1,1,1)$ y un prior sobre la condición inicial $\hat{\mathbf{x}}_0$ con media $(0.8, 1.2, 0.9)$ y matriz de covarianza diagonal con entradas $(0.01, 0.01, 0.01)$. Se utilizan como variables aleatorias para la dinámica y observación un ruido normal aditivo, centrado en el origen y con matrices de covarianza
\begin{equation*}
    \mathbf{Q} = \text{diag}(0.01, 0.01, 0.01), \quad \mathbf{R} = \text{diag}(0.01, 0.01),
\end{equation*}
respectivamente.

Se simula el sistema por $30$ unidades de tiempo y para la aproximación de los operadores se utiliza $N=500$. En las tablas \ref{tab:errores_alpha_01}, \ref{tab:errores_alpha_001} y \ref{tab:errores_alpha_m025} se muestran los errores que cometen tanto el filtro de Kalman como KKKF con respecto a la curva real, en norma y por cada estado.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/kalman_vs_kkkf_01.pdf}
    \caption{Comparación del filtro de Kalman con KKKF para $\alpha = -0.1$. En línea azul se encuentra el resultado del sistema simulado sin ruidos ni de dinámica ni observación, en naranja la solución del filtro de Kalman y en verde la solución de KKKF.}
    \label{fig:kalman_vs_kkkf_01}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/kalman_vs_kkkf_001.pdf}
    \caption{Comparación del filtro de Kalman con KKKF para $\alpha = 0.01$. En línea azul se encuentra el resultado del sistema simulado sin ruidos ni de dinámica ni observación, en naranja la solución del filtro de Kalman y en verde la solución de KKKF.}
    \label{fig:kalman_vs_kkkf_001}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/kalman_vs_kkkf_025.pdf}
    \caption{Comparación del filtro de Kalman con KKKF para $\alpha = 0.25$. En línea azul se encuentra el resultado del sistema simulado sin ruidos ni de dinámica ni observación, en naranja la solución del filtro de Kalman y en verde la solución de KKKF.}
    \label{fig:kalman_vs_kkkf_025}
\end{figure}

\begin{table}[h!]
\centering
\caption{Errores para $\alpha = -0.1$}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Estado} & \textbf{Kalman} & \textbf{KKKF} \\ \hline
Estado 1 & 0.3588 & 0.3909 \\ \hline
Estado 2 & 0.4347 & 0.4624 \\ \hline
Estado 3 & 0.4195 & 0.2238 \\ \hline
\end{tabular}
\label{tab:errores_alpha_01}
\end{table}

\begin{table}[h!]
\centering
\caption{Errores para $\alpha = 0.01$}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Estado} & \textbf{Kalman} & \textbf{KKKF} \\ \hline
Estado 1 & 0.3588 & 0.3912 \\ \hline
Estado 2 & 0.4242 & 0.4264 \\ \hline
Estado 3 & 0.7550 & 0.5928 \\ \hline
\end{tabular}
\label{tab:errores_alpha_001}
\end{table}

\begin{table}[h!]
\centering
\caption{Errores para $\alpha = -0.25$}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Estado} & \textbf{Kalman} & \textbf{KKKF} \\ \hline
Estado 1 & 0.3588 & 0.3906 \\ \hline
Estado 2 & 0.4592 & 0.5241 \\ \hline
Estado 3 & 0.3679 & 0.2485 \\ \hline
\end{tabular}
\label{tab:errores_alpha_m025}
\end{table}

Con el objetivo de visualizar la cota de error probada en el teorema \ref{teo:error_kkkf_fin} se prueba ejecutar el filtro para distintos valores de $N$, manteniendo todo el resto de valores. Se prueba esto para $N \in \{10, 50, 100, 200, 300, 500, 1000, 1500, 2000\}$ y luego se ajusta una curva de la forma $C_1 \cdot N ^{-C_2}$, donde lo esperado es que $C_2$ sea, al menos, $0.5$.

En las figuras \ref{fig:linear_error_01}, \ref{fig:linear_error_001} y \ref{fig:linear_error_025} se muestran los resultados de este experimento, en donde se observa que en efecto el orden de decaimiento del error en función de $N$ es cercano a $1/2$, lo que cumple la cota de error vista en el teorema $\ref{teo:error_kkkf_fin}$.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/linear_error_01.pdf}
    \caption{Error en norma para el estado en función de $N$, para $\alpha = -0.1$}
    \label{fig:linear_error_01}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/linear_error_001.pdf}
    \caption{Error en norma para el estado en función de $N$, para $\alpha = 0.01$}
    \label{fig:linear_error_001}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/linear_error_025.pdf}
    \caption{Error en norma para el estado en función de $N$, para $\alpha = -0.25$}
    \label{fig:linear_error_025}
\end{figure}

\subsection{Comparación para el modelo SIR con otros filtros}

Ahora se compara el filtro con otros filtros no lineales presentados en los preliminares, que vendrían siendo Extended Kalman Filter (EKF), Unscented Kalman Filter (UKF) y Particle Filters (PF), que tal como se mencionó antes, son implementación propia para este trabajo. 

El sistema a considerar es el sistema SIR ya visto anteriormente
\begin{equation*}
    \begin{aligned}
    S_{k+1} &= S_k -\beta S_k I_k, \\
    I_{k+1} &= I_k + \beta S_k I_k - \gamma I_k, \\
    S_{k+1} &= R_k + \gamma I_k,
    \end{aligned}
\end{equation*}
en donde para esta subsección se considerarán $\beta$ y $\gamma$ conocidos y que la función de observación es
\begin{equation*}
    \mathbf{g}(S, I, R) = I,
\end{equation*}
es decir, solo se observan los infectados.

Se estudia primero el caso en que se prueban los filtros para $\beta = 1.0$ y $\gamma = 0.3$ y ruidos normales centrados con matrices de covarianza
\begin{equation*}
    \mathbf{Q} = \text{diag}(\sigma, \sigma, \sigma), \quad \mathbf{R} = \text{diag}(\sigma),
\end{equation*}
en donde $\sigma$ variará en cada experimento para poder visualizar cómo se comporta cada filtro en presencia de mayor ruido.

Se utiliza como condición inicial real $\mathbf{x}_0 = (0.9, 0.1, 0.0)$, prior para la condición inicial una variable aleatoria normal centrada en $\hat{\mathbf{x}}_0 = (0.9, 0.05, 0.05)$ con matriz de covarianza $\mathbf{Q}_0 = (0.01, 0.01, 0.01)$. Para el EKF se utilizan diferencias finitas centradas para la aproximación de Jacobiano de la dinámica con precisión $\varepsilon=10^{-6}$, para PF se utilizaron $N_p = 5000$ partículas y para KKKF se utilizó $N=1000$ como dimensión de aproximación de los operadores. En las figuras \ref{fig:nonlinear_filters_sir_sigma_01}, \ref{fig:nonlinear_filters_sir_sigma_001} y \ref{fig:nonlinear_filters_sir_sigma_0001} se observan los resultados para $\sigma \in \{0.1, 0.01, 0.001\}$, seguido de las tablas \ref{tab:errores_sigma_01}, \ref{tab:errores_sigma_001} y \ref{tabla:errores_sigma_0001} que contienen el detalle de los errores para cada estado y cada filtro.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/nonlinear_filters_sir_sigma_01.pdf}
    \caption{Comparación de resultados de las trayectorias generadas por los filtros EKF (naranja), UKF (verde), PF (rojo) y KKKF (púrpura), junto con la trayectoria real (puntos azules) sin ruidos, ni de dinámica ni de observación, esto para cada uno de los estados del modelo SIR. Para $\beta = 1.0$, $\gamma = 0.3$ y $\sigma = 0.1$.}
    \label{fig:nonlinear_filters_sir_sigma_01}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/nonlinear_filters_sir_sigma_001.pdf}
    \caption{Comparación de resultados de las trayectorias generadas por los filtros EKF (naranja), UKF (verde), PF (rojo) y KKKF (púrpura), junto con la trayectoria real (puntos azules) sin ruidos, ni de dinámica ni de observación, esto para cada uno de los estados del modelo SIR. Para $\beta = 1.0$, $\gamma = 0.3$ y $\sigma = 0.01$.}
    \label{fig:nonlinear_filters_sir_sigma_001}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/nonlinear_filters_sir_sigma_0001.pdf}
    \caption{Comparación de resultados de las trayectorias generadas por los filtros EKF (naranja), UKF (verde), PF (rojo) y KKKF (púrpura), junto con la trayectoria real (puntos azules) sin ruidos, ni de dinámica ni de observación, esto para cada uno de los estados del modelo SIR. Para $\beta = 1.0$, $\gamma = 0.3$ y $\sigma = 0.001$.}
    \label{fig:nonlinear_filters_sir_sigma_0001}
\end{figure}

\begin{table}[h!]
\centering
\caption{Errores para $\sigma = 0.1$}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Estado} & \textbf{EKF} & \textbf{UKF} & \textbf{PF} & \textbf{KKKF} \\ \hline
S & 0.4757 & 0.7738 & 0.9571 & 0.3330 \\ \hline
I & 0.8665 & 0.8300 & 0.8553 & 0.3015 \\ \hline
R & 1.1360 & 1.4221 & 1.2151 & 0.2209 \\ \hline
\end{tabular}
\label{tab:errores_sigma_01}
\end{table}

\begin{table}[h!]
\centering
\caption{Errores para $\sigma = 0.01$}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Estado} & \textbf{EKF} & \textbf{UKF} & \textbf{PF} & \textbf{KKKF} \\ \hline
S & 0.1277 & 0.1328 & 0.1783 & 0.1737 \\ \hline
I & 0.2801 & 0.2565 & 0.2779 & 0.1709 \\ \hline
R & 0.4881 & 0.5135 & 0.4337 & 0.1329 \\ \hline
\end{tabular}
\label{tab:errores_sigma_001}
\end{table}

\begin{table}[h!]
\centering
\caption{Errores para $\sigma = 0.001$}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Estado} & \textbf{EKF} & \textbf{UKF} & \textbf{PF} & \textbf{KKKF} \\ \hline
S & 0.0417 & 0.0528 & 0.0493 & 0.2320 \\ \hline
I & 0.1023 & 0.0985 & 0.1022 & 0.1996 \\ \hline
R & 0.3070 & 0.2489 & 0.2983 & 0.1710 \\ \hline
\end{tabular}
\label{tabla:errores_sigma_0001}
\end{table}

Se observa que KKKF logra superar en error a todos los otros filtros con creces en el caso en que $\sigma \in \{ 0.1, 0.01\}$, aunque para el caso en donde hay menor ruido los resultados son bastante similares. Esto mostraría que el filtro tiene un mejor desempeño en comparación a los otros filtros en escenarios de mayor incertidumbre.

Se muestra ahora el caso en que $\sigma = 0.01$ fijo y varía el parámetro $\beta \in \{0.6, 0.9, 1.5\}$, en donde el objetivo es ver cómo cambian los resultados en función de $\beta$, que es en algún sentido el parámetro que cuantifica la no linealidad del sistema. Dado que $\gamma$ en realidad representa una relación lineal, se deja fijo en $\gamma = 0.3$. Se utilizan los mismos valores para todo el resto de configuraciones. Los resultados se pueden apreciar en las figuras \ref{fig:nonlinear_filters_sir_beta_06}, \ref{fig:nonlinear_filters_sir_beta_09} y \ref{fig:nonlinear_filters_sir_beta_15}, y en las tablas \ref{tab:errores_beta_gamma_06}, \ref{tab:errores_beta_gamma_09} y \ref{tab:errores_beta_gamma_15}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/nonlinear_filters_sir_beta_06.pdf}
    \caption{Comparación de resultados de las trayectorias generadas por los filtros EKF (naranja), UKF (verde), PF (rojo) y KKKF (púrpura), junto con la trayectoria real (puntos azules) sin ruidos, ni de dinámica ni de observación, esto para cada uno de los estados del modelo SIR. Para $\beta = 0.6$, $\gamma = 0.3$ y $\sigma = 0.01$.}
    \label{fig:nonlinear_filters_sir_beta_06}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/nonlinear_filters_sir_beta_09.pdf}
    \caption{Comparación de resultados de las trayectorias generadas por los filtros EKF (naranja), UKF (verde), PF (rojo) y KKKF (púrpura), junto con la trayectoria real (puntos azules) sin ruidos, ni de dinámica ni de observación, esto para cada uno de los estados del modelo SIR. Para $\beta = 0.9$, $\gamma = 0.3$ y $\sigma = 0.01$.}
    \label{fig:nonlinear_filters_sir_beta_09}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/nonlinear_filters_sir_beta_15.pdf}
    \caption{Comparación de resultados de las trayectorias generadas por los filtros EKF (naranja), UKF (verde), PF (rojo) y KKKF (púrpura), junto con la trayectoria real (puntos azules) sin ruidos, ni de dinámica ni de observación, esto para cada uno de los estados del modelo SIR. Para $\beta = 1.5$, $\gamma = 0.3$ y $\sigma = 0.01$.}
    \label{fig:nonlinear_filters_sir_beta_15}
\end{figure}

\begin{table}[h!]
\centering
\caption{Errores para $\beta = 0.6$ y $\gamma = 0.3$}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Estado} & \textbf{EKF} & \textbf{UKF} & \textbf{PF} & \textbf{KKKF} \\ \hline
S & 0.2442 & 0.2379 & 0.3305 & 0.1629 \\ \hline
I & 0.2904 & 0.2815 & 0.2903 & 0.1352 \\ \hline
R & 0.4874 & 0.5503 & 0.4732 & 0.1210 \\ \hline
\end{tabular}
\label{tab:errores_beta_gamma_06}
\end{table}

\begin{table}[h!]
\centering
\caption{Errores para $\beta = 0.9$ y $\gamma = 0.3$}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Estado} & \textbf{EKF} & \textbf{UKF} & \textbf{PF} & \textbf{KKKF} \\ \hline
S & 0.1486 & 0.1533 & 0.2195 & 0.1672 \\ \hline
I & 0.2808 & 0.2590 & 0.2798 & 0.1628 \\ \hline
R & 0.4866 & 0.5246 & 0.4907 & 0.1300 \\ \hline
\end{tabular}
\label{tab:errores_beta_gamma_09}
\end{table}

\begin{table}[h!]
\centering
\caption{Errores para $\beta = 1.5$ y $\gamma = 0.3$}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Estado} & \textbf{EKF} & \textbf{UKF} & \textbf{PF} & \textbf{KKKF} \\ \hline
S & 0.0885 & 0.2296 & 0.2292 & 0.1906 \\ \hline
I & 0.2834 & 0.2788 & 0.2816 & 0.1934 \\ \hline
R & 0.4671 & 0.5295 & 0.4042 & 0.1435 \\ \hline
\end{tabular}
\label{tab:errores_beta_gamma_15}
\end{table}



\subsection{Estimación de parámetros del modelo SIR}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/nonlinear_filters_sir_params.pdf}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/nonlinear_filters_sir_params_evolution.pdf}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/nonlinear_filters_sir_params_density.pdf}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/nonlinear_filters_sir_rec_params.pdf}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/nonlinear_filters_sir_rec_params_evolution.pdf}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{img/content/chapter4/nonlinear_filters_sir_rec_params_density.pdf}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}