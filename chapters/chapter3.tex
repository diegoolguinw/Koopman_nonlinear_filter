En lo que sigue se considerarán sistemas de la forma
\begin{equation}
	\begin{aligned}
		\mathbf{x}_{k+1} &= \mathbf{f}(t_k, \mathbf{x}_k, \mathbf{u}_k) + \mathbf{w}_k \\
		\mathbf{y}_k &= \mathbf{g}(t_k, \mathbf{x}_k, \mathbf{u}_k) + \mathbf{v}_k 
	\end{aligned}
	\label{eq:no_lin_dis_chap3}
\end{equation}
Es decir, en comparación al problema original, no se considerará ni el tiempo ni el \textit{input} en la dinámica. Esto no hace perder gran generalidad al problema, en primer lugar ya que el tiempo se puede agregar como una variable de estado de manera que si $\mathbf{x}_{k}^{n+1 = t_k}$, la $n+1$-ésima coordenada del estado, entonces 
\begin{equation*}
    \mathbf{x}_{k+1}^{n+1} = \mathbf{x}_{k}^{n+1} + \Delta t_k
\end{equation*}
donde $\Delta t_k$ es el paso de tiempo en el instante $k$, que en muchas aplicaciones es fijo. 

Para el \textit{input}, dado que no se está considerando como una variable de decisión, se considerará conocido o al menos que se tiene la capacidad de entregarse al sistema de manera indefinida, esto es, $\{ \mathbf{u}_k \}_{k \geq 0}$, es accesible y por tanto bastaría agregarlo también como una coordenada del estado: $\mathbf{x}_{k}^{n+1} = \mathbf{u}_k$.

El objetivo de este capítulo es presentar un marco general para el \textit{kernel Extended Dynamic Mode Decomposition} (kEDMD) que se ajuste a las necesidades planteadas en el trabajo, las cuales fueron previamente discutidas en la sección anterior. En particular, estas necesidades incluyen la existencia de una función de \textit{lifting forward} para realizar un \textit{embedding} en una dimensión superior, un operador que logre realizar la transición en tiempo discreto y otro que permita recuperar el \textit{embedding}, posibilitando así el retorno a la dimensión original del problema. Cabe señalar que, además de lo anterior, se debe incorporar la necesidad de representar las observaciones en un espacio de dimensión infinita.

La metodología propuesta conecta los operadores de covarianza introducidos en los preliminares con el concepto de operador de Koopman, visualizando este último como un objeto más universal y transversal para el desarrollo del trabajo. 

Finalmente, se tiene como objetivo utilizar un teorema demostrado por Philipp et al. en \cite{Philipp2024ErrorOperator}, el cual proporciona una cota de error para la aproximación de un operador de Koopman. Este teorema será fundamental para el desarrollo de la sección siguiente. El mismo equipo de Philipp ha trabajado en cotas similares, lo que también cobra relevancia a futuro para este trabajo \cite{Philipp2023ErrorFramework, Nuske2023Finite-DataControl, Kohne2024L-errorDecomposition, Harder2024Group-ConvolutionalDecomposition, Peitz2025EquivarianceEquations}.

Es importante destacar que todas las construcciones presentadas en este capítulo, junto con el enfoque de convertir al operador de Koopman en un objeto transversal para el contexto de filtraje y el uso posterior de las cotas de error mencionadas en la literatura, constituyen una contribución original de este trabajo.

\section{Operadores de Koopman en RKHS}

Sea $\B_\X$ la $\sigma$-álgebra de Borel definida sobre $\X$ y $\B_{\R^p}$ la correspondiente a $\R^p$. Se introducen las siguientes medidas de probabilidad:  
\[
\begin{aligned}
    \rho_f: \X \times \B_\X \to [0, 1], & \quad \rho_f (\mathbf{x}, A) = \P (\mathbf{f}(\mathbf{x}, \cdot ) \in A ), \\
    \rho_g: \X \times \B_\Y \to [0, 1], & \quad \rho_g(\mathbf{x}, A) = \P (\mathbf{g}(\mathbf{x}, \cdot ) \in A).
\end{aligned}
\]

En este contexto, $\rho_f$ representa la medida inducida por la dinámica del sistema, mientras que $\rho_g$ corresponde a la medida inducida por las observaciones.  
Se asume que el espacio de estados $\X$ es compacto y que existe un conjunto compacto $\Y \subseteq \R^p$ tal que:  
\[
\rho_f (\mathbf{x}, \X) = 1, \quad \rho_g (\mathbf{x}, \Y) = 1, \quad \forall \mathbf{x} \in \X.
\]

Sea entonces $\B_\Y$ la $\sigma$-álgebra $\B_{\R^p}$ trazada en $\Y$. Se emplea la siguiente notación diferencial:
\[
\rho_f (\mathbf{x}, dx) = d \rho_f (\mathbf{x}, \cdot)(x), \quad \rho_g (\mathbf{x}, dy) = d \rho_g (\mathbf{x}, \cdot)(y).
\]

Adicionalmente, se considera que existen funciones  
\[
p_f : \X \times \X \to \R_+, \quad p_g : \X \times \Y \to \R_+,
\]
tales que, para $\mu_\X$ medida sobre $\X$ y $\mu_\Y$ medida sobre $\Y$, se cumple:
\[
\rho_f (\mathbf{x}, A) = \int_A p_f (\mathbf{x}, y) \, d \mu_\X (y), \quad \rho_g (\mathbf{x}, A) = \int_A p_g (\mathbf{x}, y) \, d \mu_\Y (y).
\]

En este marco, se adoptan las siguientes denominaciones:
\begin{itemize}
    \item La variable aleatoria asociada a $\mu_\X$ se denota por $X$.  
    \item La variable aleatoria asociada a $\rho_f$, es decir, aquella que describe la evolución de un estado $\mathbf{x}$ tras un paso de la dinámica, se denota por $X^+ \mid \mathbf{x}$.  
    \item La variable aleatoria asociada a $\rho_g$, que describe las observaciones condicionadas a un estado $\mathbf{x}$, se denota por $Y \mid \mathbf{x}$.  
\end{itemize}
\begin{obs}

Un ejemplo de la existencia de todo lo anterior es el siguiente, considérese el caso en que la función de dinámica se describe mediante:  
    \[
    \mathbf{f}(\mathbf{x}_k, \mathbf{w}_k) = \Tilde{\mathbf{f}}(\mathbf{x}_k) + \mathbf{w}_k,
    \]
donde $\mathbf{w}_k \sim \mathcal{N}(0, \mathbf{Q}_k)$, con $\mathbf{Q}_k$ definida positiva, y $\mu_\X$ representa la medida de Lebesgue en $\X$. Bajo estas condiciones, la medida $\rho_f (\mathbf{x}_k, \cdot)$ sigue una distribución normal:  
    \[
    \rho_f (\mathbf{x}_k, \cdot) \sim \mathcal{N}(\Tilde{\mathbf{f}}(\mathbf{x}_k), \mathbf{Q}_k).
    \]
    
En consecuencia, la función $p_f$ asociada toma la forma:  
    \[
    p_f(x, y) = (2 \pi \det \mathbf{Q}_k )^{-1/2} \exp \left( -\frac{1}{2} (\Tilde{\mathbf{f}}(x) - y)^\top \mathbf{Q}_k^{-1} (\Tilde{\mathbf{f}}(x) - y) \right),
    \]
la cual es acotada, incluso en el caso en que $\Tilde{\mathbf{f}}$ no lo sea.  
\end{obs}
Durante este trabajo se supondrán las siguientes hipótesis de compatibilidad, las que son necesarias para que los resultados en \cite{Philipp2024ErrorOperator} tengan validez.
\begin{enumerate}
    \item[a)] Sean $k_\X:\X \times \X \to \R$ y $k_\Y: \Y \times \Y \to \R$ dos kernels simétricos, continuos, acotados y semi-definidos positivos. Se denotará por $\H_\X$ y $\H_\Y$ a los espacios de Hilbert con \textit{kernel} reproducible (RKHS) asociados a $k_\X$ y $k_\Y$, respectivamente.
    \item[b)] Si $\psi_\X \in L^2(\X)$ y $\psi_\Y \in L^2(\Y)$ son tales que: 
    \[
        \int_{\X \times \X} k_\X(x,y) \psi_\X (x) \psi_\X (y) d \mu_\X (x) d \mu_\X (y) = 0, 
    \]
    \[
        \int_{\Y \times \Y} k_\Y(x,y) \psi_\Y (x) \psi_\Y (y) d \mu_\Y (x) d \mu_\Y (y) = 0, 
    \]
    entonces $\psi_\X = 0$ y $\psi_\Y = 0$ casi seguro.
    \item[c)] Si $\psi_\X \in \H_\X$ y $\psi_\Y \in \H_\Y$ son tales que $\psi_\X(x) = 0$ para todo $x \in \X$ $\mu_\X$-casi seguro, y $\psi_\Y(y) = 0$ para todo $y \in \Y$ $\mu_\Y$-casi seguro, entonces $\psi_\X \equiv 0$ y $\psi_\Y \equiv 0$.
    \item[d)] Se asumen las siguientes relaciones entre $\rho_f$ y $\rho_g$ con respecto a $\mu_\X$ y $\mu_\Y$:
    \[
        \int_\X \rho_f (x, A_\X) d\mu_\X (x) \leq L_\X \mu_\X (A_\X), \quad \forall A_\X \in \B_\X,
    \]
    \[
        \int_\X \rho_g (x, A_\Y) d\mu_\X (x) \leq L_\Y \mu_\Y (A_\Y), \quad \forall A_\Y \in \B_\Y.
    \]
\end{enumerate}

Un ejemplo de un kernel que satisface las hipótesis a) y b) es el kernel de Matérn. La propiedad a) es garantizada por los resultados presentados en la sección anterior, mientras que la propiedad b) se deriva de la universalidad del kernel en $L^2$. 

La hipótesis c) se satisface si $\mu_\X$ y $\mu_\Y$ tienen densidad respecto a la medida de Lebesgue, y la hipótesis d) se cumple en el caso estocástico cuando las funciones $p_f$ y $p_g$ son acotadas. En el caso determinista, se requiere que las funciones asociadas sean difeomorfismos.

\begin{prop}
    Si $p_f \in L^{\infty} (\X \times \X)$ y $p_g \in L^{\infty} (\X \times \Y)$, entonces:
    \[
        \int_\X \rho_f (x, A) d\mu_\X (x) \leq L_\X \mu_\X (A), \quad \int_\X \rho_g (x, A) d\mu_\X (x) \leq L_\Y \mu_\Y (A),
    \]
    donde:
    \[
        L_\X = \mu_\X (\X) \| p_f \|_\infty, \quad L_\Y = \mu_\X (\X) \| p_g \|_\infty.
    \]
\end{prop}

\begin{proof}
    Para $\rho_f$, se tiene que:
    \[
        \begin{aligned}
            \int_\X \rho_f (x, A) d\mu_\X (x) &= \int_\X \int_A d \rho_f (x, \cdot) (y) d \mu_\X (x) \\
            &= \int_\X \int_A p_f (x, y) d \mu_\X (y) d \mu_\X (x) \\
            &\leq \| p_f \|_\infty \int_\X \int_A d \mu_\X (y) d \mu_\X (x) \\ 
            &= \| p_f \|_\infty \mu_\X (\X) \mu_\X (A) \\
            &= L_\X \mu_\X (A).
        \end{aligned}
    \]

    Para $\rho_g$, de manera similar, se tiene que:
    \[
        \begin{aligned}
            \int_\X \rho_g (x, A) d\mu_\X (x) &= \int_\X \int_A d \rho_g (x, \cdot) (y) d \mu_\X (x) \\
            &= \int_\X \int_A p_g (x, y) d \mu_\Y (y) d \mu_\X (x) \\
            &\leq \| p_g \|_\infty \int_\X \int_A d \mu_\Y (y) d \mu_\X (x) \\ 
            &= \| p_g \|_\infty \mu_\X (\X) \mu_\Y (A) \\
            &= L_\Y \mu_\Y (A).
        \end{aligned}
    \]
\end{proof}
En el caso en que la dinámica o la observación sean deterministas, esto es equivalente a que $\mathbf{f}(\mathbf{x}, \mathbf{w}) = \Tilde{\mathbf{f}}(\mathbf{x})$ o $\mathbf{g}(\mathbf{x}, \mathbf{v}) = \Tilde{\mathbf{g}}(\mathbf{x})$, con lo que $\rho (x, \cdot) = \delta_{\Tilde{\mathbf{f}}(x)}(\cdot)$ o $\xi (x, \cdot) = \delta_{\Tilde{\mathbf{g}}(x)}(\cdot)$. En este caso, es necesario asumir mayor regularidad sobre las funciones involucradas.

\begin{prop}
    Si $\rho (x, \cdot) = \delta_{\Tilde{\mathbf{f}}(x)}(\cdot)$ o $\xi (x, \cdot) = \delta_{\Tilde{\mathbf{g}}(x)}(\cdot)$, y $\tilde{\mathbf{f}}$ o $\tilde{\mathbf{g}}$, según corresponda, son difeomorfismos de clase $C^1$ tales que:
    \begin{equation*}
        \inf_{x \in \X} \left | \det ( D \tilde{\mathbf{f}} (x) ) \right | > 0, \quad \inf_{x \in \X} \left | \det ( D \tilde{\mathbf{g}} (x) ) \right | > 0,
    \end{equation*}
    entonces:
    \begin{equation*}
        \int_\X \rho_f (x, A) d\mu (x) \leq L_f \mu (A), \quad \int_\X \rho_g (x, A) d\mu_\X (x) \leq L_g \mu_\Y (A),
    \end{equation*}
    con:
    \begin{equation*}
        L_f = \left \| \det ( D \tilde{\mathbf{f}}^{-1} ) \right \|_\infty, \quad L_g = \left \| \det ( D \tilde{\mathbf{g}}^{-1} ) \right \|_\infty.
    \end{equation*}
\end{prop}
\begin{proof}

La demostración sigue un razonamiento similar al expuesto en \cite{Kohne2024L-errorDecomposition}. Para un conjunto $A \in \B$, se observa que $\rho_f (x, A) = \delta_{\Tilde{\mathbf{f}}(x)}(A) = \mathds{1}_A (\Tilde{\mathbf{f}}(x))$. Por lo tanto:
    \begin{equation*}
        \begin{aligned}
            \int_\X \rho_f (x, A) d\mu (x) &= \int_\X \mathds{1}_A (\Tilde{\mathbf{f}}(x)) d \mu_\X (x) \\
            &= \int_\X \mathds{1}_A (x) \left | \det ( D \Tilde{\mathbf{f}}^{-1}(x) ) \right | d \mu_\X (x) \\
            &\leq \left \| \det ( D \Tilde{\mathbf{f}}^{-1} ) \right \|_\infty \int_\X \mathds{1}_A (x) d\mu_\X (x) \\
            &= \left \| \det ( D \Tilde{\mathbf{f}}^{-1} ) \right \|_\infty \mu_\X (A) \\
            &= L_f \mu_\X (A).
        \end{aligned}
    \end{equation*}
    El caso de $\rho_g$ se demuestra de manera análoga.
\end{proof}

A continuación, se presentan las definiciones de los operadores de Koopman estocásticos para la dinámica y la observación, adaptados al caso en que se disponen de las funciones $p_f$ y $p_g$ como densidades.

\begin{defn}[Operador de Koopman estocástico para la dinámica, tiempo discreto]
	Se define el operador asociado a $\mathbf{f}$ como $\U : L^2(\X) \to L^2(\X)$, dado por:
	\begin{equation*}
		[\U h](x) = \E [h (\mathbf{f} (x, \cdot) )]  = \int_\X h(x') d \rho_f (x, \cdot) (x') = \int_\X h(x') p_f (x, x') d \mu_\X (x').
	\end{equation*}
\end{defn}

\begin{defn}[Operador de Koopman estocástico para la observación]
    Se define el operador asociado a $\mathbf{g}$ como $\G : L^2(\Y) \to L^2(\X)$, dado por:
	\begin{equation*}
		[\G h](x) = \E [h (\mathbf{g} (x, \cdot) )]  = \int_\Y h(y) d \rho_g (x, \cdot) (y) = \int_\Y h(y) p_g (x, y) d \mu_\Y (y).
	\end{equation*}
\end{defn}

Un objeto de interés que será estudiado próximamente es el operador de Perron-Frobenius. A continuación, se presentan sus definiciones en el contexto estocástico para la dinámica y la observación.

\begin{defn}[Operador de Perron-Frobenius estocástico para la dinámica]
	Se define el operador asociado a $\mathbf{f}$ como $\mathcal{P}_\mathbf{f} : L^2(\X) \to L^2(\X)$, dado por:
	\begin{equation*}
		[\mathcal{P}_\mathbf{f} h](x) = \int_\X h(x') p_f (x', x) d \mu_\X (x').
	\end{equation*}
\end{defn}

\begin{defn}[Operador de Perron-Frobenius estocástico para la observación]
	Se define el operador asociado a $\mathbf{g}$ como $\mathcal{P}_\mathbf{g} : L^2(\X) \to L^2(\Y)$, dado por:
	\begin{equation*}
		[\mathcal{P}_\mathbf{g} h](x) = \int_\X h(y) p_g (y, x) d \mu_\X (y).
	\end{equation*}
\end{defn}

Dado que los operadores $\mathcal{P}_\mathbf{f}$ y $\mathcal{P}_\mathbf{g}$ están definidos en términos de las funciones de densidad $p_f$ y $p_g$, se cumple que:
\begin{equation*}
	\U^* = \mathcal{P}_\mathbf{f}, \quad \G^* = \mathcal{P}_\mathbf{g},
\end{equation*}
donde $\U^*$ y $\G^*$ denotan los operadores adjuntos de $\U$ y $\G$, respectivamente.

En lo que sigue, se retomarán las definiciones de la sección anterior, pero específicas en este contexto.

\begin{defn}[Feature map canónico]
	Se definen los \textit{feature maps} canónicos de los \textit{kernels} $\Phi_\X : \X \to \H_\X$ y $\Phi_\Y : \Y \to \H_\Y$ como:
	\begin{equation*}
		\Phi_\X (x) = k_\X (x, \cdot), \quad \Phi_\Y (y) = k_\Y (y, \cdot).
	\end{equation*}
\end{defn}

\begin{defn}
	Para $x_1, x_2 \in \X$ y $y_1, y_2 \in \Y$, se definen los operadores de rango 1 $C_{x_1,x_2} : \H_\X \to \H_\X$, $C_{y_1,y_2} : \H_\Y \to \H_\Y$ y $C_{y_1,x_1} : \H_\X \to \H_\Y$ como:
	\begin{equation*}
		C_{x_1, x_2} \psi = \langle \psi, \Phi_\X (x_2) \rangle \Phi_\X (x_1) = \psi (x_2) \Phi_\X (x_1),
	\end{equation*}
	\begin{equation*}
		C_{y_1, y_2} \psi = \langle \psi, \Phi_\Y (y_2) \rangle \Phi_\Y (y_1) = \psi (y_2) \Phi_\Y (y_1),
	\end{equation*}
	\begin{equation*}
		C_{y_1, x_1} \psi = \langle \psi, \Phi_\X (x_1) \rangle \Phi_\Y (y_1) = \psi (x_1) \Phi_\Y (y_1).
	\end{equation*}
\end{defn}

\begin{defn}[Operadores de covarianza]
	Se definen los operadores de covarianza 
    \begin{equation*}
        C_X : \H_\X \to \H_\X, \quad C_Y : \H_\Y \to \H_\Y
    \end{equation*} 
    como:
	\begin{equation*}
		C_X \psi = \int_\X \langle \psi, \Phi_\X (x) \rangle \Phi_\X (x) d \mu_\X (x) = \int_\X [\Phi_\X (x) \otimes \Phi_\X (x)] \psi d \mu_\X (x),
	\end{equation*}
	\begin{equation*}
		C_Y \psi = \int_\Y \langle \psi, \Phi_\Y (y) \rangle \Phi_\Y (y) d \mu_\Y (y) = \int_\Y [\Phi_\Y (y) \otimes \Phi_\Y (y)] \psi d \mu_\Y (y).
	\end{equation*}
\end{defn}

\begin{defn}[Operadores de covarianza cruzada]
    El operador de covarianza cruzada asociado a la dinámica es $C_{X X^+} : \H_\X \to \H_\X$, dado por:
	\begin{equation*}
		C_{X X^+} \psi = \int_\X \int_\X \langle \psi, \Phi_\X (y) \rangle \Phi_\X (x) d\rho_f (x, \cdot)(y) d \mu_\X (x).
	\end{equation*}
    
    El operador de covarianza cruzada asociado a la observación es $C_{\Y \X} : \H_\X \to \H_\Y$, dado por:
	\begin{equation*}
		C_{Y X} \psi = \int_\X \int_\Y \langle \psi, \Phi_\X (x) \rangle \Phi_\Y (y) d\rho_g (x, \cdot)(y) d \mu_\X (x).
	\end{equation*}
\end{defn}

\begin{defn}[Operadores de \textit{embedding} condicional]   
    Se definen los operadores de \textit{embedding} condicional $C_{X^+ | X} : \H_\X \to \H_\X$ y $C_{Y | X} : \H_\X \to \H_\Y$ como:
	\begin{equation*}
		C_{X^+ | X} = C_{X^+ X} C_X^{-1}, \quad C_{Y | X} = C_{Y X} C_X^{-1}.
	\end{equation*}
\end{defn}

Finalmente, será necesario asumir que $\U \H_\X \subset \H_\X$ y $\G \H_\Y \subset \H_\X$. Esto se cumple, por ejemplo, si $\H_\X$ es un espacio de Sobolev, como en el caso de que $k_\X$ sea un kernel de Matérn. En este contexto, se tiene la siguiente proposición:

\begin{prop}[Invarianza Sobolev para Koopman]
	Si $p_f \in C^{k,k} (\X \times \X)$ y $p_g \in C^{k,k} (\X \times \Y)$, entonces:
	\begin{equation*}
		\U \H^k (\X) \subset \H^k (\X), \quad \G \H^k (\Y) \subset \H^k (\X).
	\end{equation*}
    \label{prop:inv_koop}
\end{prop}

\begin{proof}
	Sea $m \in \N$, $m \leq k$ y $|\alpha| = m$ un multiíndice. Para $h \in \H^k (\X)$, por el teorema de la convergencia dominada:
	\begin{equation*}
		\partial^\alpha_x (\U h)(x) = \int_\X h(y) \partial^\alpha_x p_{\mathbf{f}} (x, y) d \mu_\X (y).
	\end{equation*}
	Por lo tanto:
	\begin{equation*}
		\begin{aligned}
			\| \partial^\alpha_x (\U h) \|_{L^2}^2 & \leq \int_\X \int_\X h(y)^2 \partial^\alpha_x p_{\mathbf{f}} (x, y)^2 d \mu_\X (y) d \mu_\X (x) \\
			& \leq \| \partial^\alpha_x p_{\mathbf{f}} \|_{C^{k,k}} \mu_\X (\X) \int_\X h(y)^2 d \mu_\X (y) \\
			& \leq \| \partial^\alpha_x p_{\mathbf{f}} \|_{C^{k,k}} \mu_\X (\X) \| h \|_{\H^k (\X)}^2.
		\end{aligned}
	\end{equation*}
	Así, se concluye que $\U h \in \H^k (\X)$ y:
	\begin{equation*}
		\| \U \|_{\H^k \to \H^k} \leq \mu_\X (\X) \sum_{|\alpha| \leq k} \| \partial^\alpha_x p_{\mathbf{f}} \|_{C^{k,k}}.
	\end{equation*}
	El caso de $\G$ se demuestra de manera análoga.
\end{proof}

\begin{teo}
    Si 
    \begin{equation*}
        \U \H^k (\X) \subset \H^k (\X), \quad \G \H^k (\Y) \subset \H^k (\X),
    \end{equation*}
    entonces se cumple que
    \begin{equation*}
        C_{X^+ X} C_X^{-1} = C_{X^+ | X} = \U^* = \mathcal{P}_\mathbf{f}, \quad C_{Y | X} = \G^* = \mathcal{P}_\mathbf{g}.
    \end{equation*}
    \label{teo:cov_koop_equiv}
\end{teo}

\begin{proof}
Primero, un manejo algebraico permite establecer la siguiente relación:
\begin{equation*}
    \begin{aligned}
        C_{X X^+} \psi &= \int_\X \int_\X [\Phi_\X (x) \otimes \Phi_\X (y)] \psi \, d\rho_f (x, \cdot) (y) \, d\mu_\X (x) \\
        &= \int_\X \int_\Y \psi(y) \Phi_\X (x) \, d\rho_f (x, \cdot) (y) \, d\mu_\X (x) \\
        &= \int_\X (\U \psi)(x) \Phi_\X (x) \, d\mu_\X (x) \\
    \end{aligned}
\end{equation*}

En la última relación obtenida se obtiene el operador de rango 1 que representa al producto de Kronecker, visto en la definición \ref{def:kronecker}
\begin{equation*}
    \begin{aligned}
        C_{X X^+} \psi &= \int_\X [\Phi_\X (x) \otimes \Phi_\X (x)] (\U \psi) \, d\mu_\X (x) \\
        &= C_X \U \psi.
    \end{aligned}
\end{equation*}

Por tanto, se tiene que 
\begin{equation*}
    C_{X X^+} = C_X \U.
\end{equation*}

Notando que
\begin{equation*}
    C_X = \mathbb{E}[\Phi_\X (X) \otimes \Phi_\X (X)], \quad
    C_{X X^+} = \mathbb{E}[\Phi_\X (X) \otimes \Phi_\X (X^+)]
\end{equation*}
y que 
\begin{equation*}
    (\Phi_\X (X) \otimes \Phi_\X (X^+))^* = \Phi_\X (X^+) \otimes \Phi_\X (X),
\end{equation*}
se deduce que 
\begin{equation*}
    C_X^* = C_X \quad \text{y} \quad C_{X X^+}^* = C_{X^+X}.
\end{equation*}

Así, resulta que 
\begin{equation*}
    C_{X^+X} C_X^{-1} = \U^*,
\end{equation*}
con lo cual se concluye 
\begin{equation*}
    C_{X^+X} C_X^{-1} = C_{X^+ | X} = \U^* = \mathcal{P}_\mathbf{f}.
\end{equation*}

De forma análoga, se demuestra que
\begin{equation*}
    C_{Y | X} = \G^* = \mathcal{P}_\mathbf{g}.
\end{equation*}

Estas igualdades deben entenderse cuidadosamente y están bien definidas bajo la condición de que $C_X$ sea inyectivo \cite{Fukumizu2013KernelKernels}, lo que ya se discutió en la sección anterior.
\end{proof}

\begin{obs}
    El hecho de que el operador de el operador de \textit{embedding} de la distribución condicional sea el adjunto del operador de Koopman, y no el operador de Koopman en sí mismo, está relacionado con algo ya visto por Gerlach et al., en donde indican que el operador de Koopman genera una dinámica de funciones y el operador de Perron-Frobenius una de distribuciones. Esto se explica mejor en la figura \ref{fig:koopman_exp}.
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.5\linewidth]{img/content/chapter3/koopman_exp.pdf}
        \caption{Diagrama explicativo de la relación entre el operador de Koopman y Perron-Frobenius, obtenido de \cite{Gerlach2020TheSystems}, a su vez adaptado de \cite{Leonard2019ProbabilisticAirdrop}.}
        \label{fig:koopman_exp}
    \end{figure}
\end{obs}

Notar que, una vez realizado el \textit{embedding} a dimensión infinita de las distribuciones de probabilidad del problema, es necesario regresar al espacio de dimensión original. Por ello, se introduce un nuevo operador de Koopman que actuará como un operador de \textit{lifting back}.

Sea $\Tilde{k}_\X: \X \times \X \to \R$ el \textit{kernel} definido a partir del producto interno, con lo cual su \textit{feature map} canónico $\phi : \X \to \R^n$ está dado por la función identidad. Esto es:
\begin{equation*}
    \Tilde{k}_\X (x, y) = x^\top y, \quad \phi (x) = x.
\end{equation*}

Entonces, el operador de covarianza cruzada asociado a $X$ y a los \textit{embeddings} entre $\H_\X$ y $\R^n$, este último considerado como el RKHS asociado a $\Tilde{k}_\X$, $C_{X|X} : \H_\X \to \R^n$, debe satisfacer
\begin{equation*}
    C_{X|X} \Phi_\X (\mathbf{x}) = \mathbb{E} [\phi (X) | X = \mathbf{x}] = \mathbb{E}[ X | X = \mathbf{x}] = \mathbf{x}.
\end{equation*}

De este modo, en virtud del teorema \ref{teo:cov_koop_equiv}, se concluye que existe un operador de Koopman $\B : \R^n \to \H_\X$ tal que $C_{X|X} = \B^*$, al cual denominaremos operador de \textit{lifting back}. Este operador permitirá regresar al espacio original desde un espacio de dimensión mayor.

\section{Aproximación de operadores de Koopman}

En esta sección se describe el proceso de aproximación de los operadores de Koopman relacionados con la dinámica, observación y reconstrucción. Para ello, se consideran \( N \) puntos, representados como \( \{ x_i \}_{i=1}^N \sim \mu_\X^N \), y conjuntos de puntos adicionales \( \{ x^+_i \}_{i=1}^N \) y \( \{ y_i \}_{i=1}^N \), generados bajo las siguientes distribuciones:
\begin{equation*}
    x^+_i \sim \rho_f (x_i, \cdot), \quad y_i \sim \rho_g (x_i, \cdot), \quad i = 1, \dots, N.
\end{equation*}

Se define el espacio:
\begin{equation*}
    \H_{\X, N} = \text{span} \{\Phi_\X (x_i) : i = 1, \dots, N \},
\end{equation*}
cuya base canónica está dada por \( \{\Phi_\X (x_i) : i = 1, \dots, N \} \). En este punto se tomará la elección de tomar $k_\Y$ como el \textit{kernel} dado por el producto interno en $\R^p$, con lo que su \textit{feature map} canónico es la función identidad en dicho espacio.

Se introducen las siguientes matrices:
\begin{equation*}
    \mathbf{X} = (x_{1} | \dots | x_N), \quad \mathbf{X}^+ = (x_{1}^+ | \dots | x_N^+), \quad \mathbf{Y} = (y_1 | \dots | y_N),
\end{equation*}
\begin{equation*}
    \Phi_N (\mathbf{X}) = (k_\X(x_i, x_j))_{i,j = 1}^N, \quad \Phi_N (\mathbf{X}^+) = (k_\X(x_i, x^+_j))_{i,j = 1}^N.
\end{equation*}

Con base en estas definiciones, se introducen los operadores:
\begin{equation*}
    C_{X}^N : \H_{\X, N} \to \H_{\X, N}, \quad C_{XX^+}^N : \H_{\X, N} \to \H_{\X, N}, \quad 
\end{equation*}
\begin{equation*}
    C_{XY}^N : \R^p \to \H_{\X, N}, \quad C_{XX}^N : \R^n \to \H_{\X, N}.
\end{equation*}
definidos como
\[
C_X^N = \frac{1}{N} \sum_{j=1}^N \Phi_\X (x_j) \otimes \Phi_\X (x_j), \quad C_{XX^+}^N = \frac{1}{N} \sum_{j=1}^N \Phi_\X (x_j) \otimes \Phi_\X (x_j^+)
\]
\[
C_{XY}^N = \frac{1}{N} \sum_{j=1}^N \Phi_\X (x_j) \otimes \Phi_\Y (y_j), \quad C_{XX}^N = \frac{1}{N} \sum_{j=1}^N \Phi_\X (x_j) \otimes \phi (x_j)
\]
Entonces las acciones de estos operadores viene dada por
\begin{equation*}
    C_{X}^N \Phi_\X (x_i) = \frac{1}{N} \sum_{j = 1}^N k_\X(x_i, x_j) \Phi_\X (x_j), \quad C_{XX^+}^N \Phi_\X (x_i) = \frac{1}{N} \sum_{j = 1}^N k_\X(x_i, x_j^+) \Phi_\X (x_j),
\end{equation*}
\begin{equation*}
        C_{XY}^N \Phi_\Y (y_i) = \frac{1}{N} \sum_{j = 1}^N k_\Y (y_i, y_j) \Phi_\X (x_j), \quad C_{XX}^N \phi (x_i) = \frac{1}{N} \sum_{j = 1}^N \langle x_i, x_j \rangle \Phi_\X (x_j).
\end{equation*}

Estos operadores están representados mediante las matrices \( \Phi_N (\mathbf{X}) \), \( \Phi_N (\mathbf{X}^+) \), \( \mathbf{Y} \) y \( \mathbf{X} \), respectivamente. A continuación, se definen los operadores:
\begin{equation*}
    \U_N : \H_{\X, N} \to \H_{\X, N}, \quad \G_N : \R^p \to \H_{\X, N}, \quad \B_N : \R^n \to \H_{\X, N},
\end{equation*}
los cuales están representados por las siguientes matrices:
\begin{equation*}
    \mathbf{U}_N = (\Phi_N (\mathbf{X}))^{-1} \Phi_N (\mathbf{X}^+)^\top,
\end{equation*}
\begin{equation*}
    \mathbf{G}_N = (\Phi_N (\mathbf{X}))^{-1} \mathbf{Y}^\top,
\end{equation*}
\begin{equation*}
    \mathbf{B}_N = (\Phi_N (\mathbf{X}))^{-1} \mathbf{X}^\top.
\end{equation*}
es decir,
\begin{equation*}
    \U_N = (C_X^N)^{-1} C_{XX^+}^N,
\end{equation*}
\begin{equation*}
    \G_N = (C_X^N)^{-1} C_{XY}^N,
\end{equation*}
\begin{equation*}
    \B_N = (C_X^N)^{-1} C_{XX}^N.
\end{equation*}
El algoritmo \ref{alg:kEDMD} se deja el detalle para poder aplicar kernel Extended Dynamic Mode Decomposition en el ámbito computacional.
\begin{algorithm}
\caption{kEDMD($\mu_\X$, $\rho_f$, $\rho_g$, $k$, $N$)}\label{alg:kEDMD}
\begin{algorithmic}[1]
\Require $\mu_\X$ medida de probabilidad asociada al estado, $\rho_f$ medida de probabilidad asociada a la dinámica, $\rho_g$ medida de probabilidad asociada a la observación, $\mathbf{k}: \X \times \X \to \R$ un \textit{kernel} semidefinido positivo, $N$ dimensión de aproximación de Koopman.
\Ensure $\mathbf{U}_N \in \R^{N \times N}$ aproximación del operador de Koopman, $\mathbf{\Phi}_N: \X \to \R^{N}$ función de \textit{lifting forward}, $\mathbf{G}_N \in \R^{p \times N}$ matriz de linealización de la observación y $\mathbf{B}_N \in \R^{n \times N}$ matriz de \textit{lifting back}.
\State $x \sim \mu_\X, \, i = 1, \dots, N$ \Comment{$N$ muestras independientes \textit{sampleadas} desde $\mu_\X$}
\State $x_i^+ \gets \rho_f(x_i, \cdot), \, i = 1, \dots, N$ \Comment{$N$ muestras de la dinámica}
\State $y_i \gets \rho_g(x_i, \cdot), \, i = 1, \dots, N$ \Comment{$N$ muestras de la observación}
\State $\Phi_N (\cdot) \gets k(\mathbf{X}, \cdot)$
\Comment{Función de \textit{lift forward}}
\State $\Phi_N (\mathbf{X}) \gets (\mathbf{k}(x_i, x_j))_{i,j=1}^{N}$
\State $\Phi_N (\mathbf{X}^+) \gets (\mathbf{k}(x_i, x_j^+))_{i,j=1}^{N}$
\State $\mathbf{U}_N \gets \Phi_\X (\mathbf{X})^{-1}\Phi_\X (\mathbf{X}^+)^T$
\Comment{Aproximación del operador de Koopman}
\State $\mathbf{G}_N \gets \Phi_\X (\mathbf{X})^{-1} \mathbf{Y}^T$
\Comment{Aproximación del operador de observación}
\State $\mathbf{B}_N \gets \Phi_\X (\mathbf{X})^{-1} \mathbf{X}^T$
\Comment{Matriz de \textit{lift back}}
\end{algorithmic}
\end{algorithm}

Por otra parte, se definen las siguientes normas asociadas al \textit{kernel} \( k_\X \):
\begin{equation*}
    \| k_\X \|_1 = \int_\X k_\X (x,x) d \mu_\X (x), \quad \| k_\X \|_\infty = \sup_{x \in \X} k_\X (x,x),
\end{equation*}
las cuales son finitas en el caso del \textit{kernel} de Matérn. En lo que sigue se probará una cota de error para kEDMD. Para ello primero es necesario ver que algunos de los operadores se pueden extender a $\H_\X$.

\begin{prop}
    Los operadores $C_X^N$ y $C_{XX^+}^N$ se pueden extender como operadores de $\H_\X \to \H_{\X, N}$ de manera continua.
\end{prop}

\begin{proof}
    Sea $\psi \in \H_\X$ y $\varepsilon > 0$ luego por Moore-Aronszajn existen $\{ \Tilde{x}_j \}_{j=1}^m$ tales que
    \[
    \psi_m = \sum_{j=1}^m \alpha_j \Phi_\X (\Tilde{x}_j)
    \]
    cumple $\psi_m \in \H_{\X, m} := \text{span}(\{ \Phi_\X (\Tilde{x}_j), \, j=1,\dots,m \})$ y $\| \psi - \psi_m \| \leq \varepsilon$. Primero se extienden los operadores a espacios finitamente generados como $\H_{\X, m}$, para ello se hace uso de la acción de estos sobre $\Phi_\X (x_i)$, quedando
    \[
    C_X \psi_m =
    \frac{1}{N} \sum_{i=1}^m \alpha_i \sum_{j = 1}^N k_\X(\Tilde{x}_i, x_j) \Phi_\X (x_j), \quad C_{XX^+} \psi_m =
    \frac{1}{N} \sum_{i=1}^m \alpha_i \sum_{j = 1}^N k_\X(\Tilde{x}_i, x_j^+) \Phi_\X (x_j).
    \]
    Entonces se extienden a $\H_\X$ como
    \[
    C_X \psi = \lim_{m \to \infty} C_X \psi_m, \quad C_{XX^+} \psi = \lim_{m \to \infty} C_{XX^+} \psi_m,
    \]
    de forma que
    \[
    C_X \psi =
    \frac{1}{N} \sum_{i \geq 1} \alpha_i \sum_{j = 1}^N k_\X(\Tilde{x}_i, x_j) \Phi_\X (x_j), \quad C_{XX^+} \psi =
    \frac{1}{N} \sum_{i \geq 1} \alpha_i \sum_{j = 1}^N k_\X(\Tilde{x}_i, x_j^+) \Phi_\X (x_j).
    \]
    Con ello basta ver que está definido de manera continua
    \[
    \begin{aligned}
        \| C_X \psi \|^2 & \leq \frac{1}{N^2} \sum_{i \geq 1} \sum_{j=1}^N \alpha_i^2 k(\Tilde{x}_i, x_j)^2 \| \Phi_\X (x_j) \|^2 \\
        & \leq \frac{\| k \|_\infty^2}{N^2} \| \alpha \|_{\ell^2} \sum_{j=1}^N k(x_j, x_j) \\
        & \leq \frac{\| k \|_\infty^2}{N^2} \| \psi \|_{\H_\X} \sum_{j=1}^N k(x_j, x_j). \\
    \end{aligned}
    \]
    Análogamente
    \[
    \| C_{XX^+} \psi \| \leq \frac{\| k \|_\infty^2}{N^2} \| \psi \|_{\H_\X} \sum_{j=1}^N k(x_j, x_j),
    \]
    con lo que los operadores se extienden de manera continua.
\end{proof}

Una desigualdad que será clave es la desigualdad de Hoeffding, que es corolario de un teorema asociado a martingalas, probado por Pinelis en 1994.

\begin{teo}[\cite{Pinelis1994OptimumSpaces} Teorema 3.5]
Sea $(S_k)_{k \in \mathbb{N}}$ una sucesión de martingalas con valores en un espacio de Hilbert real separable $H$ tal que 
\[
\sum_{k=1}^\infty \operatorname{ess\,sup} \|S_k - S_{k-1}\|_H^2 \leq C^2
\]
para alguna constante $C^2 > 0$. Entonces, para todo $\varepsilon > 0$, se tiene
\[
\mathbb{P} \left( \sup_{k \in \mathbb{N}} \|S_k\|_H \geq \varepsilon \right) \leq 2 \exp \left( -\frac{\varepsilon^2}{2C^2} \right).
\]
\end{teo}

\begin{cor}[Desigualdad de Hoeffding en espacios de Hilbert]
Sean $\xi_1, \dots, \xi_n$ variables aleatorias independientes en un espacio de Hilbert separable $H$ tales que $\mathbb{P}$-c.t.p. $\|\xi_i\|_H \leq M$ y $\mathbb{E}[\xi_i] = 0$ para todo $1 \leq i \leq n$. Entonces, para todo $\varepsilon > 0$, se tiene
\[
\mathbb{P} \left( \left\| \frac{1}{n} \sum_{i=1}^n \xi_i \right\|_H \geq \varepsilon \right) \leq 2 \exp \left( -\frac{n\varepsilon^2}{2M^2} \right).
\]
\end{cor}

Algo que se usará de manera recurrente es la inyectividad de $C_X$, específicamente la cota inferior que se tiene cuando operadores lineales son inyectivos.

\begin{lema}[\cite{Christmann2008SupportMachines}]
    $C_X$ es inyectivo; de forma equivalente, existe una constante $c_1 > 0$ tal que
    \[
    \| C_X \psi \| \geq c_1 \| \psi \|, \quad \forall \psi \in \H_\X.
    \]
    A la constante $c_1$ se le denotará la constante de inyectividad de $C_X$.
\end{lema}

Primero se procede de la misma forma en que lo hicieron Phillip et al. \cite{Philipp2024ErrorOperator}, utilizando la desigualdad de Hoeffding para obtener una cota de probabilidad para la aproximación del operador.

\begin{prop}
    Sean $\varepsilon > 0$ y $N \in \N$, luego con probabilidad $(1-\delta)^4$ se tiene que
    \[
    \| C_{X} - C_{X}^N \|_{HS}, \, \| C_{XX^+} - C_{XX^+}^N \|_{HS}, \, \| C_{XY} - C_{XY}^N \|_{HS}, \, \| C_{XX} - C_{XX}^N \|_{HS} \leq \varepsilon.
    \]
\end{prop}

\begin{proof}
        Primero notar que para $\psi \in \H_\X$ se tiene
    \[
    \| (\Phi_\X (x_j) \otimes \Phi_\X (x_j))\psi \|^2 = \| \psi(x_j) \Phi_\X (x_j) \|^2 \leq \| \psi \|_\infty k(x_j, x_j) \leq \| k \|_\infty \| \psi \|_{\H_\X}.
    \]
    Por tanto, como este operador tiene rango $1$ se concluye que
    \[ \| \Phi_\X (x_j) \otimes \Phi_\X (x_j) \|_{HS} \leq  \sqrt{\| k \|_\infty.} \] 

    Análogamente,
    \[
    \| \Phi_\X (x_j) \otimes \Phi_\X (x_j^+) \|_{HS} \leq \sqrt{\| k \|_\infty.}
    \]
     \[
    \| \Phi_\X (x_j) \otimes \Phi_\Y (y_j) \|_{HS} \leq \sqrt{\| k \|_\infty.}
    \]
     \[
    \| \Phi_\X (x_j) \otimes \phi (x_j) \|_{HS} \leq \sqrt{\| k \|_\infty.}
    \]

    Por lo que

    \[ \| \E[ \Phi_\X (x_j) \otimes \Phi_\X (x_j)] \|_{HS} \leq  \E[ \| \Phi_\X (x_j) \otimes \Phi_\X (x_j) \|_{HS} ] \leq \sqrt{\| k \|_\infty.}, \] 
    \[
    \| E[ \Phi_\X (x_j) \otimes \Phi_\X (x_j^+)] \|_{HS} \leq E[ \| \Phi_\X (x_j) \otimes \Phi_\X (x_j^+)\|_{HS}]  \leq \sqrt{\| k \|_\infty.},
    \]
     \[
    \| \E [\Phi_\X (x_j) \otimes \Phi_\Y (y_j)] \|_{HS} \leq \E [\| \Phi_\X (x_j) \otimes \Phi_\Y (y_j) \|_{HS}]  \leq \sqrt{\| k \|_\infty.},
    \]
     \[ \| \E[ \Phi_\X (x_j) \otimes \phi (x_j)] \|_{HS} \leq  \E[ \| \Phi_\X (x_j) \otimes \phi (x_j) \|_{HS} ] \leq \sqrt{\| k \|_\infty.}. \] 

     Obteniendo que
     \[
     \| \Phi_\X (x_j) \otimes \Phi_\X (x_j) - C_X \|_{HS} \leq 2 \sqrt{\| k \|_\infty.},
     \]
     \[
     \| \Phi_\X (x_j) \otimes \Phi_\X (x_j^+) - C_{XX^+} \|_{HS} \leq 2 \sqrt{\| k \|_\infty.},
     \]
     \[
     \| \Phi_\X (x_j) \otimes \Phi_\Y (y_j) - C_{XY} \|_{HS} \leq 2 \sqrt{\| k \|_\infty.},
     \]
    \[
     \| \Phi_\X (x_j) \otimes \phi (x_j) - C_{XX} \|_{HS} \leq 2 \sqrt{\| k \|_\infty.},
     \]

     Sea $\varepsilon > 0$ y $N \in \N$, se denota
     \[
     \delta = 2 \text{exp} \left ( - \frac{N \varepsilon^2}{8 \| k \|_\infty} \right )
     \]
     luego por desigualdad de Hoeffding se tiene que
    \[
    \P ( \| C_{X} - C_{X}^N \|_{HS} > \varepsilon ) \leq \delta,
    \]
    \[
    \P ( \| C_{XX^+} - C_{XX^+}^N \|_{HS} > \varepsilon ) \leq \delta,
    \]
    \[
    \P ( \| C_{XY} - C_{XY}^N \|_{HS} > \varepsilon ) \leq \delta,
    \]
    \[
    \P ( \| C_{XX} - C_{XX}^N \|_{HS} > \varepsilon ) \leq \delta.
    \]

    Entonces
    \[
    \P ( \| C_{X} - C_{X}^N \|_{HS} \leq \varepsilon ) \geq 1-\delta,
    \]
    \[
    \P ( \| C_{XX^+} - C_{XX^+}^N \|_{HS} \leq \varepsilon ) \geq 1-\delta,
    \]
    \[
    \P ( \| C_{XY} - C_{XY}^N \|_{HS} \leq \varepsilon ) \geq 1-\delta,
    \]
    \[
    \P ( \| C_{XX} - C_{XX}^N \|_{HS} \leq \varepsilon ) \geq 1-\delta.
    \]
    Con lo que con probabilidad al menos $(1-\delta)^4$ se tiene que
    \[
    \| C_{X} - C_{X}^N \|_{HS}, \, \| C_{XX^+} - C_{XX^+}^N \|_{HS}, \, \| C_{XY} - C_{XY}^N \|_{HS}, \, \| C_{XX} - C_{XX}^N \|_{HS} \leq \varepsilon.
    \]
\end{proof}

\begin{teo}[Cota para kerEDMD]
Sean $\delta \in (0, 1)$ y $N \in \N$ tales que
\[
\delta > 2 \text{exp} \left ( -\frac{Nc_1^2}{8\|k\|_\infty}\right )
\]
con $c_1$ la constante de inyectividad de $C_X$. Luego si $\U \H_\X \subset \H_\X, \, \G \R^p \subset \H_\X, \, \B \R^n \subset \H_\X$, se tiene con probabilidad $(1-\delta)^4$ que 
 \begin{equation*}
    \| \U - \U_N \|_{\H_{\X} \to \H_{\X}} \leq C_\delta N^{-1/2}
\end{equation*}
\begin{equation*}
\| \G - \G_N \|_{\R^p \to \H_{\X}} \leq C_\delta N^{-1/2}
\end{equation*}
\begin{equation*}
\| \B - \B_N \|_{\R^n \to \H_{\X}} \leq C_\delta N^{-1/2}
\end{equation*}
donde 
\[
C_\delta = \left ( \frac{2}{c_1} + \frac{\sqrt{\| k \|_{\infty}}}{c_1^2}
  \right )\sqrt{8 \| k \|_\infty \ln \left ( \frac{2}{\delta}\right ) }
\]
\end{teo}

\begin{proof}
    Sea $\psi \in \H_\X$, luego
    \[
    \begin{aligned}
        \| \U \psi - \U_N \psi \| &= \| C_X^{-1} C_{XX^+} \psi - \left (C_X^N \right )^{-1} C_{XX^+}^N \psi \| \\
        & \leq \| C_X^{-1} C_{XX^+} \psi - C_X^{-1} C_{XX^+}^N \psi \| + \| C_X^{-1} C_{XX^+}^N \psi - \left (C_X^N \right )^{-1} C_{XX^+}^N \psi \|.
    \end{aligned}
    \]

    Viendo el primer término:
    \[ \| C_X^{-1} C_{XX^+} \psi - C_X^{-1} C_{XX^+}^N \psi \|, \]
    sean
    \[ \Tilde{\psi} = C_{XX^+} \psi, \quad  \Tilde{\psi}_N = C_{XX^+}^N \psi, \]
    con lo que 
    \[ \| C_X^{-1} C_{XX^+} \psi - C_X^{-1} C_{XX^+}^N \psi \| = \| C_X^{-1} \Tilde{\psi} - C_X^{-1} \Tilde{\psi}_N \|, \]
    luego sean $\hat{\psi}$ y $\hat{\psi}_N$ tales que 
    \[ C_X \hat{\psi} = \Tilde{\psi}, \quad C_X \hat{\psi}_N = \Tilde{\psi}_N \]
    que existen dado que $\text{Ran}(C_{XX^+}) \subseteq \text{Ran}(C_X)$.

    Luego, 
    \[
    \| C_X^{-1} C_{XX^+} \psi - C_X^{-1} C_{XX^+}^N \psi \| = \|\hat{\psi} - \hat{\psi}_N \|.
    \]

    Por inyectividad de $C_X$, existe $c_1$ tal que
    \[
    c_1 \| \hat{\psi} - \hat{\psi}_N  \| \leq \| C_X \hat{\psi} - C_X \hat{\psi}_N \| = \| \Tilde{\psi} - \Tilde{\psi}_N \| \leq \| C_{XX^+} - C_{XX^+}^N \| \| \psi \| \leq \varepsilon \| \psi \|,
    \]
    de lo que se obtiene que 
    \[
    \| C_X^{-1} C_{XX^+} \psi - C_X^{-1} C_{XX^+}^N \psi \| \leq \frac{\varepsilon}{c_1} \| \psi \|.
    \]

    Ahora para el término
    \[
    \| C_X^{-1} C_{XX^+}^N \psi - \left (C_X^N \right )^{-1} C_{XX^+}^N \psi \|,
    \]
    se denota
    \[
    \Tilde{\psi}_N = C_{XX^+}^N \psi,
    \]
    con lo que
    \[
    \| C_X^{-1} C_{XX^+}^N \psi - \left (C_X^N \right )^{-1} C_{XX^+}^N \psi \| = \| C_X^{-1} \Tilde{\psi}_N - \left (C_X^N \right )^{-1} \Tilde{\psi}_N \|.
    \]

    Notar que,
    \[
    \Tilde{\psi}_N \in 
   \H_{\X, N} = \text{Ran}(C_{X}^N) = \text{Ran}(C_{XX^+}^N) \subseteq \text{Ran}(C_{XX^+}) \subseteq \text{Ran}(C_X)
    \]
    por lo que $\Tilde{\psi}_N \in \text{Ran}(C_{X}^N)$ y $\Tilde{\psi}_N \in \text{Ran}(C_{X})$. Así, existen $\hat{\psi}_N$, $\hat{\psi}$ tal que
    \[
    C_X^N \hat{\psi}_N = \Tilde{\psi}_N, \quad C_X \hat{\psi} = \Tilde{\psi}_N,
    \]
    con lo que,
    \[
    \| C_X^{-1} C_{XX^+}^N \psi - \left (C_X^N \right )^{-1} C_{XX^+}^N \psi \| = \| \hat{\psi}_N - \hat{\psi}\|
    \]
    luego, $C_X^N \hat{\psi}_N = C_X \hat{\psi}$ y así
    \[
    C_X \hat{\psi} - C_X \hat{\psi}_N = C_X^N \hat{\psi}_N - C_X \hat{\psi}.
    \]
    Por inyectividad de $C_X$, se tiene que
    \[
    c_1 \| \hat{\psi} - \hat{\psi}_N \| \leq  \| C_X \hat{\psi} - C_X \hat{\psi}_N \| \leq \| C_X^N \hat{\psi}_N - C_X \hat{\psi}_N \| \leq \| C_X^N - C_X\| \| \hat{\psi}_N \| \leq \varepsilon \| \hat{\psi}_N \|,
    \]
    es decir,
    \[
    \| \hat{\psi} - \hat{\psi}_N \| \leq \frac{\varepsilon}{c_1} \| \hat{\psi}_N \|.
    \]
    Por otro lado, 
    \[
    C_X^N \hat{\psi}_N = C_X \hat{\psi}_N + (C_X^N - C_X) \hat{\psi}_N,
    \]
    y con ello
    \[
    -C_X \hat{\psi}_N = -C_X^N \hat{\psi}_N + (C_X^N - C_X) \hat{\psi}_N,
    \]
    con lo que
    \[
    \| C_X \hat{\psi}_N \| \leq \| C_X^N \hat{\psi}_N \| + \| (C_X^N - C_X) \hat{\psi}_N \| \leq \| C_X^N - C_X \| \| \hat{\psi}_N \| \leq \varepsilon \| \hat{\psi}_N \|,
    \]
    de lo que se sigue que
    \[
    \| C_X \hat{\psi}_N \| - \varepsilon \| \hat{\psi}_N \| \leq \| C_X^N \hat{\psi}_N \|.
    \]
    Por inyectividad de $C_X$ se tiene que
    \[
    c_1 \| \hat{\psi}_N \| - \varepsilon \| \hat{\psi}_N \| \leq \| C_X \hat{\psi}_N \| - \varepsilon \| \hat{\psi}_N \| \leq \| C_X^N \hat{\psi}_N \|.
    \]
    Recordando que $C_X^N \hat{\psi}_N =  \Tilde{\psi}_N$ y $\Tilde{\psi}_N = C_{XX^+}^N \psi$, se obtiene
    \[
    (c_1 - \varepsilon) \| \hat{\psi}_N \| \leq \| C_X^N \hat{\psi}_N \| = \| \Tilde{\psi}_N \| = \| C_{XX^+}^N \psi \| \leq \| C_{XX^+}^N \| \| \psi \|.
    \]
    Si $\varepsilon < c_1$, se obtiene que
    \[
    \| \hat{\psi}_N \| \leq \frac{1}{c_1 - \varepsilon}
    (\| C_{XX^+} \| + \| C_{XX^+} - C_{XX^+} \| ) \| \psi \|.
    \]

    Con todo esto
    \[
    \begin{aligned}
        \| \hat{\psi} - \hat{\psi}_N \| & \leq \frac{\varepsilon}{c_1(c_1 - \varepsilon)}
    (\| C_{XX^+} \| + \| C_{XX^+} - C_{XX^+} \| ) \| \psi \| \\
    & \leq \frac{\varepsilon}{c_1^2}
    (\| C_{XX^+} \| + \varepsilon ) \| \psi \| \\
    & \leq \frac{\varepsilon}{c_1^2}
    (\| C_{XX^+} \| + c_1 ) \| \psi \|.
    \end{aligned}
    \]
    Con lo que
    \[
    \| C_X^{-1} C_{XX^+}^N \psi - \left (C_X^N \right )^{-1} C_{XX^+}^N \psi \| \leq \frac{\varepsilon}{c_1^2}
    (\| C_{XX^+} \| + c_1 ) \| \psi \|,
    \]
    y así
    \[
    \| \U \psi - \U_N \psi \| \leq \frac{\varepsilon}{c_1} \| \psi \| + \frac{\varepsilon}{c_1^2}
    (\| C_{XX^+} \| + c_1 ) \| \psi \|,
    \]
    de lo que se concluye que
    \[
    \| \U - \U_N \| \leq \left ( \frac{1}{c_1} + \frac{1}{c_1^2}
    (\| C_{XX^+} \| + c_1 ) \right ) \varepsilon \leq \left ( \frac{1}{c_1} + \frac{1}{c_1^2}
    (\sqrt{\| k \|_{\infty}} + c_1 ) \right ) \varepsilon.
    \]

    Ahora por casos, si
    \[
    \delta > 2\text{exp} \left ( - \frac{N c_1^2}{8 \| k \|_\infty} \right ),
    \]
    entonces
    \[
    8 \| k \|_{\infty} \ln \left ( \frac{2}{\delta} \right ) < N c_1^2,
    \]
    con lo que
    \[
    \sqrt{8 \| k \|_{\infty} \ln \left ( \frac{2}{\delta} \right )} < N^{1/2} c_1,
    \]
    así $C_\delta N^{-1/2} < c_1$. Entonces, llamando
    \[
    \varepsilon = C_\delta N^{-1/2} < c_1
    \]
    se tiene
     \[
    \| \U - \U_N \| \leq \left ( \frac{2}{c_1} + \frac{\sqrt{\| k \|_{\infty}}}{c_1^2}
  \right )\sqrt{8 \| k \|_\infty \ln \left ( \frac{2}{\delta}\right ) } N^{-1/2}.
    \]
\end{proof}

Este resultado es una alternativa al resultado de Philipp et al. que, como se verá posteriormente, tiene un peor \textit{trade-off} entre la probabilidad $\delta$ y la cantidad de puntos \textit{sampleados} $N$.

\begin{teo}[Philipp et al. \cite{Philipp2024ErrorOperator}]
    Sea un \( N \in \mathbb{N} \) arbitrario. Se supone que los primeros \( N + 1 \) valores propios \( \lambda_j \) de \( C_X \) son simples, es decir, \( \lambda_{j+1} < \lambda_j \) para todo \( j = 1, \ldots, N \). Se definen:
    \[
    \delta_N = \min_{j=1, \ldots, N} \frac{\lambda_j - \lambda_{j+1}}{2}, \quad c_N = \frac{1}{\sqrt{\lambda_N}} + \frac{N + 1}{\delta_N \lambda_N} (1 + \|k_\X\|_{1}) \|k_\X \|^{1/2}_{1}.
    \]
    Sea además \( \varepsilon \in (0, \delta_N) \) y \( \delta \in (0, 1) \) arbitrarios, y \( N \geq  \frac{8\|k\|^2_\infty \ln(2/\delta)}{\varepsilon^2} \). Si 
    \begin{equation*}
        \U \H_\X \subset \H_\X, \, \G \H_\Y \subset \H_\X, \, \B \R^n \subset \H_\X
    \end{equation*}
    entonces, con probabilidad al menos \( (1 - \delta)^4 \), se cumple que:
    \[
    \|\U - \U_N \|_{\H_{\X, N} \to L^2(\X; \mu_\X)} \leq \sqrt{\lambda_{N+1}} \|\U \|_{\H_{X} \to \mathcal{H_\X}} + c_N \varepsilon
    \]
    \[
    \|\G - \G_N \|_{\R^p \to L^2(\X; \mu_\X)} \leq \sqrt{\lambda_{N+1}} \|\G \|_{\R^p \to \mathcal{H_\X}} + c_N \varepsilon
    \]
    \[
    \|\B - \B_N \|_{\R^n \to L^2(\X; \mu_\X)} \leq \sqrt{\lambda_{N+1}} \|\B \|_{\R^n \to \mathcal{H_\X}} + c_N \varepsilon.
    \]
    \label{teo:error_koop}
\end{teo}

Por último, se presenta un resultado complementario que proporciona una cota necesaria para las secciones posteriores.

\begin{teo}
    Bajo la misma hipótesis del teorema \ref{teo:error_koop}, sea $\delta \in (0, 1)$,  si $\H_\X$ es equivalente en norma a $H^{\nu + n/2}$ y
    \[
    8\|k\|^2_\infty \ln(2/\delta) \leq \frac{1}{2} (N+1)^{-5/2},
    \]
    con probabilidad al menos $(1 - \delta)^4$ se tiene que:
    \begin{equation}
        \| \U - \U_N \|_{\H_{\X, N} \to L^2(\X; \mu_\X)} \leq C N^{-1/2}.
        \label{eq:kEDMD_bound}
    \end{equation}
    \begin{equation*}
    \| \G - \G_N \|_{\R^p \to L^2(\X; \mu_\X)} \leq C N^{-1/2}
    \end{equation*}
    \begin{equation*}
    \| \B - \B_N \|_{\R^n \to L^2(\X; \mu_\X)} \leq C N^{-1/2}
    \end{equation*}
    con $C$ alguna constante que no depende ni de $N$ ni de $\delta$.
    \label{teo:error_koop_sqrt_N_hip}
\end{teo}

Para la demostración es necesario un teorema de Santin et al. que cuantifica el orden de decaimiento de los valores propios del operador $C_X$, que será clave para el teorema final de esta sección.

\begin{teo}[Santin et al. \cite{Santin2016ApproximationSpaces}]
    Si $\H_\X$ es equivalente en norma a un espacio de Sobolev $H^{\nu + n/2}$ y $\lambda_j$ son los valores propios de $C_X$ en orden descendente, luego
    \begin{equation*}
        \sqrt{\lambda_{N+1}} \leq c_1 N^{-(\nu + n)/(2n)}
    \end{equation*}
    con $c_1$ alguna constante que no depende de $N$.
    \label{teo:eig_val_decay}
\end{teo}

\begin{proof}
    Se verá la demostración solo para $\U$, ya que para $\G$ y $\B$ es análogo. Del teorema \ref{teo:error_koop}, bajo las hipótesis mencionadas, para $\delta \in (0, 1)$ se tiene que con probabilidad al menos $1-\delta$
    \[
    \|\U - \U_N \|_{\H_{\X, N} \to L^2(\X; \mu_\X)} \leq \sqrt{\lambda_{N+1}} \|\U \|_{\H_{X} \to \mathcal{H_\X}} + c_N \varepsilon
    \]
    con $c_N$ definido en el enunciado del teorema $\ref{teo:error_koop}$ y con 
    \begin{equation}
        N \geq \frac{8\|k\|^2_\infty \ln(2/\delta)}{\varepsilon^2}.
        \label{eq:N_bound}
    \end{equation}

    Tomando, en virtud del teorema \ref{teo:eig_val_decay}, 
    \[
    \varepsilon = \frac{\delta_N \lambda_N}{c_1(N+1)} N^{-1/2} \leq \frac{\delta_N}{(N+1)N^{(\nu+n)/n}} N^{-1/2} <\delta_N,
    \]
    se tiene
    \[
    c_N \varepsilon = \frac{\delta_N \sqrt{\lambda_N}}{c_1 (N+1) N^{1/2}} + \frac{N^{-1/2}}{c_1} (1 + \|k_\X\|_{1}) \|k_\X \|^{1/2}_{1}.
    \]
    Y notando que 
    \[
    \delta_N = \min_{j=1, \dots, N} \frac{\lambda_j - \lambda_{j+1}}{2} \leq \min_{j=1, \dots, N} \frac{\lambda_j}{2} = \frac{\lambda_N}{2},
    \]
    y llamando
    \[
    \Tilde{c}_1 = \frac{(1 + \|k_\X\|_{1}) \|k_\X \|^{1/2}_{1}}{c_1}
    \]
    se tiene
    \[
    \begin{aligned}
        c_N \varepsilon & \leq  \frac{\lambda_N \sqrt{\lambda_N}}{2 c_1 (N+1) N^{1/2}} + \frac{N^{-1/2}}{c_1} (1 + \|k_\X\|_{1}) \|k_\X \|^{1/2}_{1} \\ 
        & \leq \frac{(N-1)^{-3(n+\nu)/(2n)}}{2 c_1 (N+1) N^{1/2}} + \Tilde{c}_1 N^{-1/2} \\
        & \leq \frac{(N+1)^{-3(n+\nu)/(2n)}}{2 c_1 (N+1) N^{1/2}} + \Tilde{c}_1 N^{-1/2} \\
        & \leq \frac{1}{2c_1} N^{-1/2} + \Tilde{c}_1 N^{-1/2}.
    \end{aligned}
    \]
    
    Llamando $c_2 = 1/(2c_1) + \Tilde{c}_1$ se obtiene que 
    \[
    \|\U - \U_N \|_{\H_{\X, N} \to L^2(\X; \mu_\X)} \leq \sqrt{\lambda_{N+1}} \|\U \|_{\H_{X} \to \mathcal{H_\X}} + c_2 N^{-1/2},
    \]
    con lo que ocupando nuevamente el teorema \ref{teo:eig_val_decay}, se obtiene
    \[
    \|\U - \U_N \|_{\H_{\X, N} \to L^2(\X; \mu_\X)} \leq c_1 \|\U \|_{\H_{X} \to \mathcal{H_\X}} N^{-(n+\nu)/(2n)} + c_2 N^{-1/2} \leq c_1 \|\U \|_{\H_{X} \to \mathcal{H_\X}} N^{-1/2} + c_2 N^{-1/2}.
    \]
    Y denotando $C = c_1 \|\U \|_{\H_{X} \to \mathcal{H_\X}} + c_2$ se obtiene 
    \[
    \|\U - \U_N \|_{\H_{\X, N} \to L^2(\X; \mu_\X)} \leq C N^{-1/2},
    \]
    en donde se ha ocupado que dado que $\nu > 0$, $N^{-(n+\nu)/(2n)} \leq N^{-n/(2n)} = N^{-1/2}$.

    Ahora, notar que de \ref{eq:N_bound} se tiene
    \[
    \begin{aligned}
        8\|k\|^2_\infty \ln(2/\delta) & \leq N \varepsilon^2 \\
        & = N \frac{\delta_N \lambda_N N^{-1/2}}{c_1 (N+1)} \\
        & \leq \frac{\lambda_N^2 N^{1/2}}{2c_1 (N+1)} \\
        & \leq \frac{c_1 N^{-2(\nu+n)/n} N^{1/2}}{2c_1 (N+1)} \\
        & \leq \frac{ (N+1)^{-2(\nu+n)/n} (N+1)^{1/2}}{2 (N+1)} \\
        & \leq \frac{1}{2} (N+1)^{-(2(\nu+n)/n + 1/2)}. \\
        & \leq \frac{1}{2} (N+1)^{-(2n/n + 1/2)} \\
        & = \frac{1}{2} (N+1)^{-(2 + 1/2)} \\
        & = \frac{1}{2} (N+1)^{-5/2}
    \end{aligned}
    \]

    Con lo que se obtiene la cota que relaciona $N$ con $\delta$.
\end{proof}

% \begin{obs}
%     Notar que tomando $\delta$ tal que
%     \[
%     8\|k\|^2_\infty \ln(4/\delta) = \frac{1}{2} (N+1)^{-7/2},
%     \]
%     entonces
%     \[
%      \ln(4/\delta) = \frac{1}{16\|k\|^2_\infty} (N+1)^{-7/2},
%     \]
%     con lo que
%     \[
%     \frac{4}{\delta} = \text{exp} \left ( \frac{1}{16\|k\|^2_\infty} (N+1)^{-7/2} \right ),
%     \]
%     y así
%     \[
%     4  \text{exp} \left ( -\frac{1}{16\|k\|^2_\infty} (N+1)^{-7/2} \right ) = \delta.
%     \]

%     Con lo que en efecto $\delta$, visto como función de $N$, es una 
% \end{obs}

Es importante destacar que \( N^{-1/2} \) representa una cota menos ajustada en comparación con la presentada por \cite{Philipp2024ErrorOperator}, aunque corresponde al orden de error característico de otras aproximaciones analizadas en la sección siguiente.

El teorema anterior se fundamenta en dos hipótesis: la invarianza de Koopman y la simplicidad de los valores propios de \( C_X \). La primera hipótesis no plantea mayores inconvenientes, dado que existen numerosos sistemas donde se cumple. Por ejemplo, en el caso de ruido aditivo normal y funciones dependientes del estado que sean suficientemente suaves, dicha invarianza puede verificarse de acuerdo con la proposición \ref{prop:inv_koop}.

Sin embargo, la hipótesis sobre la simplicidad de los valores propios de \( C_X \) resulta más restrictiva. Philipp et al. no proporcionan ejemplos específicos donde esta condición se cumpla. Además, se sabe que, en ciertos casos, como en el caso de \textit{kernel} de Matérn, los valores propios de \( C_X \) están relacionados con los del operador de Laplace-Beltrami asociado al espacio de estados \( \X \) \cite{Whittle1963StochasticDimensions, Borovitskiy2020MaternManifolds}. Este hecho sugiere que existen numerosos escenarios en los cuales la simplicidad de los valores propios podría no cumplirse.

Los siguientes teoremas apuntan a eliminar ambas hipótesis, lo que permite situar la aproximación de Koopman en un contexto más general. Este nuevo enfoque no depende del dominio, salvo por la condición de compacidad, ni de las propiedades específicas de las funciones involucradas. En cambio, se basa en aspectos controlables, como el \textit{kernel} seleccionado y el número de puntos \textit{sampleados}, \( N \).

Para ello serán necesarios algunos lemas y proposiciones previas.

% \begin{lema}[Desigualdad de Hoeffding en RKHS]
%     Sean $X$, $Y$ variables aleatorias a valores en $\X$, luego para $N$ y $\varepsilon > 0$, se tiene que si $\{ x_i \}$ son muestras i.i.d. de $X$ e $\{ y_i \}$ de $Y$, entonces
%     \[
%     \P \left ( \left \| \frac{1}{N} \sum_{j=1}^N \Phi_\X (x_i) \otimes \Phi_\X (x_i) - C_{X} \right \|  > \varepsilon\right ) \leq 2 \text{exp} \left ( \frac{N \varepsilon^2}{8 \| k \|_\infty}  \right )
%     \]
%     \[
%     \P \left ( \left \| \frac{1}{N} \sum_{j=1}^N \Phi_\X (x_i) \otimes \Phi_\X (y_i) - C_{XY} \right \|  > \varepsilon\right ) \leq 2 \text{exp} \left ( \frac{N \varepsilon^2}{8 \| k \|_\infty}  \right )
%     \]
%     \label{lema:hoeffding}
% \end{lema}

\begin{prop}
    Sea $A: \H_\X \to \H_\X$ un operador compacto y autoadjunto, y sea $N \in \N$ tal que existe un valor propio $\lambda_j$, con $j < N$, cuya multiplicidad es $m > 1$. Entonces, para todo $\varepsilon \in (0, \lambda_{m+j} - \lambda_{m+j+1})$, existe un operador $B^\varepsilon: \H_\X \to \H_\X$ de rango $m$ tal que $A + B^\varepsilon$ posee los primeros $N$ valores propios simples, con las mismas funciones propias, y además $\| B^\varepsilon \| \leq C \cdot \varepsilon$, para alguna constante $C > 0$.
    \label{prop:val_prop_sim}
\end{prop}

Previo a la demostración, la intuición detrás de esta proposición es que el hecho de que haya un discreto de valores propios para un operador compacto y autoadjunto, permite acomodar valores propios con multiplicidad mayor a $1$ entre $2$ valores propios, de manera que la distancia vaya haciéndose nula, como lo explica la figura \ref{fig:simple_eig_vals}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{img/content/chapter3/simple_eig_vals.pdf}
    \caption{Intuición de la proposición \ref{prop:val_prop_sim}. A la izquierda, en naranjo, los valores propios del operador que tienen multiplicidad mayor a $1$ y a la izquierda, en naranjo también, cómo se acomodan entre dos valores propios.}
    \label{fig:simple_eig_vals}
\end{figure}

\begin{proof}
    Dado que $\H_\X$ es un espacio de Hilbert separable y $A$ es compacto y autoadjunto, el teorema espectral \cite{Brezis2011FunctionalEquations} garantiza que $A$ posee un espectro discreto de valores propios $\{ \lambda_j \}_j$, todos positivos y decrecientes hacia $0$, con funciones propias $\{ v_j \}_j$ que constituyen una base ortonormal de $\H_\X$.

    Se definen los operadores $T_k : \H_\X \to \H_\X$ de rango $1$ como
    \[
    T_k \psi = \langle v_k, \psi \rangle v_k, \quad \psi \in \H_\X,
    \]
    los cuales son lineales y continuos. Además, satisfacen
    \[
    T_k v_\ell = \langle v_k, v_\ell \rangle v_k = \delta_{k \ell} v_k,
    \]
    donde $\delta_{k \ell}$ es la delta de Kronecker, ya que $\{ v_j \}_j$ constituye una base ortonormal.

    Sea $\varepsilon \in (0, \lambda_{m+j} - \lambda_{m+j+1})$, se define
    \[
    \varepsilon_k = \frac{\varepsilon (k-j)}{m^2 \cdot j^2}, \quad k \in \{ j, \dots , m+j \},
    \]
    que cumple $\varepsilon_k < \varepsilon < \lambda_{m+j} - \lambda_{m+j+1}$. A partir de esto, se define 
    \[
    B^\varepsilon = \sum_{k=j}^{m+j} \varepsilon_k T_k,
    \]
    el cual, al ser una suma de $m$ operadores de rango $1$ con rangos ortogonales, tiene rango $m$.

    Para \( v_\ell \) con $\ell \notin \{ j, \dots, m+j \}$, se verifica
    \[
    (C_X + B^\varepsilon) v_\ell = C_X v_\ell + B^\varepsilon v_\ell = C_X v_\ell = \lambda_\ell v_\ell.
    \]
    Es decir, $C_X + B^\varepsilon$ conserva los mismos valores propios y funciones propias de $C_X$ para $\ell \notin \{ j, \dots, m+j \}$. En cambio, para $\ell \in \{ j, \dots, m+j \}$ se tiene
    \[
    (C_X + B^\varepsilon) v_\ell = C_X v_\ell + B^\varepsilon v_\ell = \lambda_\ell v_\ell + \sum_{k=j}^{m+j} \varepsilon_k T_k v_\ell = \lambda_\ell v_\ell + \sum_{k=j}^{m+j} \varepsilon_k \delta_{k \ell} v_k = \lambda_\ell v_\ell + \varepsilon_\ell v_\ell.
    \]
    Por lo tanto,
    \[
    (C_X + B^\varepsilon)v_\ell = (\lambda_\ell + \varepsilon_\ell) v_\ell,
    \]
    lo que implica que $\Tilde{\lambda}_\ell = \lambda_\ell + \varepsilon_\ell$ es un valor propio de $C_X + B^\varepsilon$ con función propia $v_\ell$. Estos valores propios son simples ya que
    \[
    \lambda_{j-1} < \lambda_j = \lambda_j + \frac{\varepsilon (j-j)}{m^2 j^2} = \Tilde{\lambda}_j < \Tilde{\lambda}_{m+j} = \lambda_{m+j} + \frac{\varepsilon m}{m^2 j^2} < \lambda_{m+j+1}.
    \]

    En consecuencia, $C_X + B^\varepsilon$ tiene los primeros $N$ valores propios simples con las mismas funciones propias. Finalmente, para $\psi \in \H_\X$, se cumple
    \[
    \| B^\varepsilon \psi \| \leq \sum_{k=j}^{m+j} \varepsilon_k \| T_k \psi \| = \sum_{k=j}^{m+j} \varepsilon_k | \langle v_k, \psi \rangle | \| v_k \| \leq \sum_{k=j}^{m+j} \varepsilon_k \| v_k \| \| \psi \| = \sum_{k=j}^{m+j} \varepsilon_k \| \psi \|,
    \]
    con lo que
    \[
    \| B^\varepsilon \| \leq \sum_{k=j}^{m+j} \varepsilon_k  = \sum_{k=j}^{m+j} \frac{\varepsilon (k-j)}{m^2 j^2} \leq \varepsilon \sum_{k=j}^{m+j} \frac{1}{m j^2} = \frac{\varepsilon}{j^2} \leq \varepsilon \sum_{j \geq 1} \frac{1}{j^2} = C \cdot \varepsilon.
    \]

    Cabe señalar que este factor $1/j^2$ no es estrictamente necesario para una única ventana de valores propios no simples, aunque facilita la extensión de la demostración a una cantidad infinita de ventanas con dicha característica.
\end{proof}

\begin{lema}
    $C_X$ es un operador compacto y autoadjunto.
\end{lema}

\begin{proof}
    Ya se estableció previamente, en el corolario \ref{cor:CX_autoad}, que $C_X$ es autoadjunto, por lo que únicamente resta demostrar su compacidad. Sea $\{ \psi_n \}_n \subset \H_\X$ una sucesión débilmente convergente a $\psi$, es decir, para toda $\phi \in \H_\X$ y $\varepsilon > 0$, existe un $n_0 \in \N$ tal que, para $n \geq n_0$, se cumple
    \[
    \langle \psi_n - \psi, \phi \rangle \to 0.
    \]
    Se debe probar que $C_X \psi_n$ converge en norma a $C_X \psi$. Para ello, se tiene:
    \[
    \begin{aligned}
        \| C_X (\psi_n - \psi) \| & \leq \int_\X | \psi_n (x) - \psi (x) | \| \Phi_\X (x) \| d \mu_\X (x) \\
        & = \int_\X | \langle \psi_n - \psi, \Phi_\X (x) \rangle | \| \Phi_\X (x) \| d \mu_\X (x) \\
        & \leq \left ( \int_\X | \langle \psi_n - \psi, \Phi_\X (x) \rangle |^2 d \mu_\X (x) \right )^{1/2} \left ( \int_\X \| \Phi_\X (x) \|^2 d \mu_\X (x) \right )^{1/2} \\
        & = \left ( \int_\X | \langle \psi_n - \psi, \Phi_\X (x) \rangle |^2 d \mu_\X (x) \right )^{1/2} \left ( \int_\X k(x,x)^2 d \mu_\X (x) \right )^{1/2} \\
        & \to 0,
    \end{aligned}
    \]
    donde se ha aplicado la propiedad reproductora para establecer que
    \[
    \psi_n (x) - \psi (x) = \langle \psi_n - \psi, \Phi_\X (x) \rangle,
    \]
    y la desigualdad de Hölder para separar en dos integrales.
\end{proof}

% \begin{teo}
%     Se supone que los valores propios de $C_X$ son simples. Sea $\delta \in (0, 1)$,  si $\H_\X$ es equivalente en norma a $H^{\nu + n/2}$ y
%     \[
%     8\|k\|^2_\infty \ln(4/\delta) \leq \frac{1}{2} (N+1)^{-7/2},
%     \]
%     entonces con probabilidad al menos $1-\delta$ se tiene que
%     \begin{equation*}
%         \| \U - \U_N \|_{\H_{\X, N} \to L^2(\X; \mu_\X)} \leq C N^{-1/2}.
%     \end{equation*}
%     \begin{equation*}
%         \| \G - \G_N \|_{\R^p \to L^2(\X; \mu_\X)} \leq C N^{-1/2}
%     \end{equation*}
%     \begin{equation*}
%         \| \B - \B_N \|_{\R^n \to L^2(\X; \mu_\X)} \leq C N^{-1/2}
%     \end{equation*}
%     con $C$ alguna constante que no depende ni de $N$ ni de $\delta$.
%     \label{teo:error_koop_sqrt_N_1}
% \end{teo}

% \begin{proof}
%     Se debe eliminar la hipótesis de invarianza de Koopman, es decir, que $\U \H_\X \subset \H_\X$. Al igual que antes se demuestra para $\U$ y para $\G$ y $\B$ es análogo.  
%     Sea $\varepsilon > 0$, luego existe una función $p^\varepsilon_f : \X \times \X \to \R$ de clase $C^\infty$ y con soporte compacto tal que 
%     \[
%     \| p^\varepsilon_f - p_f\|_{L^1} \leq \varepsilon
%     \]
%     Se define el operador aproximante $\U^\varepsilon : \H_\X \to \H_\X$ mediante
%     \[
%     (\U^\varepsilon \psi) (x) = \int_\X \psi (y) p^\varepsilon_f (x, y) d \mu_\X (y).
%     \]
%     Por tanto, sean $\psi \in \H_\X$ y $x \in \X$, con eso
%     \[
%     \begin{aligned}
%         |(\U^\varepsilon \psi) (x) - (\U \psi) (x)| & \leq \int_\X |\psi (y)| |p^\varepsilon_f (x, y) - p_f (x, y)| d \mu (y) \\
%         & \leq \| \psi \|_\infty \int_\X |p^\varepsilon_f (x, y) - p_f (x, y)| d \mu (y) \\
%         & = \sup_{y \in \X} |\psi (y)| \| p^\varepsilon_f - p_f\|_{L^1} \\
%         & \leq \varepsilon \sup_{y \in \X} |\psi (y)| \\
%         & = \varepsilon \sup_{y \in \X} |\langle \Phi_\X (y), \psi \rangle| \\
%         & \leq \varepsilon \sup_{y \in \X} \| \Phi_\X (y) \|_{\H_\X} \| \psi \|_{\H_\X} \\
%         & = \varepsilon \sup_{y \in \X} \sqrt{k(y,y)} \| \psi \|_{\H_\X}.
%     \end{aligned}
%     \]
%     Es así que
%     \[
%     \begin{aligned}
%         \|\U^\varepsilon \psi - \U \psi \|_{L^2}^2 &= \int_\X |(\U^\varepsilon \psi) (x) - (\U \psi) (x)|^2 d \mu_X (x) \\
%         &\leq \int_\X \varepsilon^2 \sup_{y \in \X} k(y,y) \| \psi \|_{\H_\X}^2 d \mu_\X (x) \\
%         & = \varepsilon^2 \sup_{y \in \X} k(y,y) \| \psi \|_{\H_\X}^2.
%     \end{aligned}
%     \]
%     Obteniendo que
%     \[
%     \|\U^\varepsilon - \U \|_{L^2} \leq \varepsilon \sup_{y \in \X} \sqrt{k(y,y)}
%     \]

%     Sea $\U^\varepsilon_N$ la aproximación de $\U^\varepsilon$, que se obtiene de manera análoga a la vista antes, solo que \textit{sampleando} puntos desde $p^\varepsilon_f$ en vez de $p_f$. Con esto
% \[
% \begin{aligned}
%     \| \U - \U_N \| & \leq \| \U - \U^\varepsilon \| + \| \U^\varepsilon - \U_N^\varepsilon \| + \| \U_N^\varepsilon - \U_N \| \\
%     & \leq \varepsilon \sup_{y \in \X} \sqrt{k(y,y)} + C N^{-1/2} + \| \U_N^\varepsilon - \U_N \| 
% \end{aligned}
% \]
% Para el término $\| \U_N^\varepsilon - \U_N \|$ recordar que
% \[
% \U_N = (C_X^N)^{-1} C_{XX^+}, \quad \U_N^\varepsilon = (C_X^N)^{-1} C_{XX^+_\varepsilon}
% \]
% donde $C_{XX^+_\varepsilon}$ es el operador que cumple
% \[
%  C_{XX^+_\varepsilon}^N \Phi_\X (x_i) = \frac{1}{N} \sum_{j = 1}^N k_\X(x_i, x_{j, \varepsilon}^+) \Phi_\X (x_j),
% \]
% siendo $ x_{j, \varepsilon}^+$ puntos \textit{sampleados} desde la variable aleatoria con densidad $p_f^\varepsilon$. 

% Con ello, y en virtud de la expresión para los adjuntos de los operadores de covarianza, para $\psi \in \H_\X$ se tiene
% \[
% \begin{aligned}
%     \| (\U_N^\varepsilon)^* \psi - (\U_N)^* \psi \| & = \| C_{XX_\varepsilon^+}^N (C_X^N)^{-1} \psi - C_{XX^+}^N (C_X^N)^{-1} \psi \| \\
%     & = \| C_{XX_\varepsilon^+}^N \Tilde{\psi}  - C_{XX^+}^N \Tilde{\psi} \|
% \end{aligned}
% \]
% con $\Tilde{\psi}$ satisfaciendo $C_X^N \Tilde{\psi} = \psi$, de lo cual se tiene $(C_X^N - C_X) \Tilde{\psi} + C_X \Tilde{\psi} = \psi$. En virtud de lo que probó Philipp et al. se tiene que con probabilidad $1-\delta$
% \[
% \| C^N_X - C_X \| \leq c \cdot N^{-1/2}.
% \]
% Con lo que, y en virtud de la inyectividad de $C_X$
% \[
% \begin{aligned}
%     \| (C_X^N - C_X) \Tilde{\psi} + C_X \Tilde{\psi}\| & \geq \| C_X \Tilde{\psi} \| - \| (C_X^N - C_X) \Tilde{\psi} \| \\
%     & \geq c_1 \| \Tilde{\psi} \| - \|C_X^N - C_X \| \| \Tilde{\psi}\| \\
%     & \geq (c_1 - c \cdot N^{-1/2}) \| \Tilde{\psi}\|.
% \end{aligned}
% \]
% Si la constante $c_1 - c \cdot N^{-1/2}$ es positiva, entonces
% \[
% \| \Tilde{\psi} \| \leq c_2 \|(C_X^N - C_X) \Tilde{\psi} + C_X \Tilde{\psi} \| = c_2 \| \psi \|
% \]
% Con ello
% \[
% \begin{aligned}
%     \| \U_N^\varepsilon \psi - \U_N \psi \| & \leq \| C_{XX_\varepsilon^+}^N - C_{XX^+}^N  \| \| \Tilde{\psi} \| \\
%     & \leq c_2 \| C_{XX_\varepsilon^+}^N - C_{XX^+}^N  \| \| \psi \|.
% \end{aligned}
% \]
% Basta entonces ver el término $\| C_{XX_\varepsilon^+}^N - C_{XX^+}^N  \|$:
% \[
% \begin{aligned}
%     \| C_{XX_\varepsilon^+}^N - C_{XX^+}^N  \| \leq \| C_{XX_\varepsilon^+}^N - C_{XX^+_\varepsilon} \| +  \| C_{XX^+} - C_{XX^+}^N \| + \| C_{XX_\varepsilon^+} - C_{XX^+} \|.
% \end{aligned}
% \]

%     Los dos primeros términos se acotan en virtud de la desigualdad de Hoeffding por $N^{-1/2}$, mientras que el tercero, sea $\phi \in \H_\X$, en virtud de la Kernel Bayes Rule
% \[
% \begin{aligned}
%     & \| C_{XX_\varepsilon^+} \phi - C_{XX^+} \phi \|\\
%     & \leq \left \| 
%     \int_\X \int_\X \phi(y) \Phi_\X (x) p^\varepsilon_f (x, y) d \mu_\X (y) d \mu_\X (x) -  \int_\X \int_\X \phi(y) \Phi_\X (x) p_f (x, y) d \mu_\X (y) d \mu_\X (x) \right \| \\
%     & \leq  
%     \int_\X \int_\X | \phi(y)| \| \Phi_\X (x) \| |p^\varepsilon_f (x, y) - p_f (x, y)|  d \mu_\X (y) d \mu_\X (x) \\
%     & \leq \int_\X \int_\X \| \phi \|_\infty \sqrt{k(x,x)} |p^\varepsilon_f (x, y) - p_f (x, y)|  d \mu_\X (y) d \mu_\X (x) \\
%     & \leq \| \phi \|_\infty \sup_{x \in \X}  \sqrt{k(x,x)}  \int_\X \int_\X |p^\varepsilon_f (x, y) - p_f (x, y)| d \mu_\X (y) d \mu_\X (x),
% \end{aligned}
% \]
% así,
% \[
% \begin{aligned}
%     & \| C_{XX_\varepsilon^+} \phi - C_{XX^+} \phi \|\\
%     & =  \| \phi \|_\infty \sup_{x \in \X}  \sqrt{k(x,x)} \| p_f^\varepsilon - p_f \|_{L^1} \\
%     & \leq \varepsilon \| \phi \|_\infty \sup_{x \in \X} \sqrt{k(x,x)}.
% \end{aligned}
% \]
% Con esto
% \[
% \| C_{XX_\varepsilon^+} - C_{XX^+}  \| \leq \varepsilon \sup_{x \in \X} \sqrt{k(x,x)}.
% \]
% Así se concluye, tomando $\varepsilon = N^{-1/2}$, que
% \[
% \| \U - \U_N \| \leq \Tilde{c}_1 N^{-1/2},
% \]
% aún cuando no se tenga la invarianza del espacio a través de Koopman.
% \end{proof}

\begin{teo}
    Sea $\delta \in (0, 1)$, $\U \H_X \subset \H_X$, $\G \R^p \subset \H_X$, $\B \R^n \subset \H_X$, $\H_\X$ es equivalente en norma a $H^{\nu + n/2}$ y 
    \[
    8\|k\|^2_\infty \ln(2/\delta) \leq \frac{1}{2} (N+1)^{-7/2},
    \]
    entonces con probabilidad al menos $(1-\delta)^4$ se tiene que
     \begin{equation*}
        \| \U - \U_N \|_{\H_{\X, N} \to L^2(\X; \mu_\X)} \leq C N^{-1/2}.
    \end{equation*}
    \begin{equation*}
    \| \G - \G_N \|_{\R^p \to L^2(\X; \mu_\X)} \leq C N^{-1/2}
    \end{equation*}
    \begin{equation*}
    \| \B - \B_N \|_{\R^n \to L^2(\X; \mu_\X)} \leq C N^{-1/2}
    \end{equation*}
    con $C$ alguna constante que no depende ni de $N$ ni de $\delta$.
    \label{teo:error_koop_sqrt_N_def}
\end{teo}

\begin{proof}
    Ahora se elimina la hipótesis de la simplicidad de los valores propios de $C_X$, que es compacto y autoadjunto. Si se supone que existe un valor propio con multiplicidad mayor que $1$, entonces por la proposición \ref{prop:val_prop_sim}, se tiene que existe $B^\varepsilon$ de rango finito que cumple que $C_X + B^\varepsilon$ tiene los primeros $N+1$ valores propios simples, con las mismas funciones propias y $\| B^\varepsilon \| \leq C \varepsilon$. 

    Se define un nuevo aproximante de Koopman como
    \[
    \U^\varepsilon = (C_X + B^\varepsilon)^{-1} C_{XX^+}
    \]
    y su aproximante en dimensión finita respectivo es
    \[
    \U^\varepsilon_N = (C_X^N + B^\varepsilon)^{-1} C_{XX^+}^N
    \]
    donde $B^\varepsilon$ se deja sin cambios ya que es de rango finito.

    Que cumplen, para $\psi \in \H_\X$
    \[
    \begin{aligned}
        \| \U\psi - \U^\varepsilon\psi \| & = \| C_X^{-1} C_{XX^+} \psi - (C_X + B^\varepsilon)^{-1} C_{XX^+} \psi \| \\
        & = \| C_X^{-1} \Tilde{\psi} - (C_X + B^\varepsilon)^{-1} \Tilde{\psi} \| \\
        & = \| \hat{\psi} - \hat{\psi}_\varepsilon \|
    \end{aligned} 
    \]
    en donde
    \[
    C_{XX^+}\psi = \Tilde{\psi}, \quad C_X \hat{\psi} =  \Tilde{\psi}, \quad (C_X + B^\varepsilon)\hat{\psi}_\varepsilon = \Tilde{\psi}.
    \]
    Así
    \[
    C_X \hat{\psi} = (C_X + B^\varepsilon)\hat{\psi}_\varepsilon,
    \]
    con lo que
    \[
    C_X (\hat{\psi} - \hat{\psi}_\varepsilon) = B^\varepsilon \hat{\psi}_\varepsilon
    \]
    y así
    \[
    \|C_X (\hat{\psi} - \hat{\psi}_\varepsilon)\| = \|B^\varepsilon \hat{\psi}_\varepsilon\|.
    \]

    Gracias a la inyectividad de $C_X$, existe una constante $c_1$ tal que 
    \[
    \| \hat{\psi} - \hat{\psi}_\varepsilon \| \leq \frac{1}{c_1} \| C_X (\hat{\psi} - \hat{\psi}_\varepsilon) \| \leq \| B^\varepsilon \| \| \hat{\psi}_\varepsilon \| \leq C \cdot \varepsilon \| \hat{\psi}_\varepsilon \|. 
    \]
    Con ello notar que
    \[
    C_X \hat{\psi}_\varepsilon  = -B^\varepsilon \hat{\psi}_\varepsilon  + (B^\varepsilon + C_X)\hat{\psi}_\varepsilon
    \]
    así
    \[
    \| C_X \hat{\psi}_\varepsilon \| \leq \| B^\varepsilon \hat{\psi}_\varepsilon \| + \| (B^\varepsilon + C_X)\hat{\psi}_\varepsilon\| \leq C \cdot \varepsilon  \| \hat{\psi}_\varepsilon \| + \| (B^\varepsilon + C_X)\hat{\psi}_\varepsilon\|. 
    \]
    
    Utilizando la inyectividad de $C_X$
    \[
    c_1 \|\hat{\psi}_\varepsilon\| \leq \| C_X \hat{\psi}_\varepsilon \| \leq  C \cdot \varepsilon  \| \hat{\psi}_\varepsilon \| + \| (B^\varepsilon + C_X)\hat{\psi}_\varepsilon\|
    \]
    con ello
    \[
    (c_1 - C\varepsilon)  \|\hat{\psi}_\varepsilon\| \leq \| (B^\varepsilon + C_X)\hat{\psi}_\varepsilon\| = \| \Tilde{\psi} \|
    \]
    
    Tomando $\varepsilon$ tal que $c_1 - C \varepsilon > 0$ se tiene que
    \[
    \|\hat{\psi}_\varepsilon\| \leq \frac{1}{c_1 - C\varepsilon} \| \Tilde{\psi} \| = \frac{1}{c_1 - C\varepsilon} \| C_{XX^+} \psi \| \leq \frac{1}{c_1 - C\varepsilon} \| C_{XX^+} \| \| \psi \|
    \]

     Y así
    \[
     \| \U\psi - \U^\varepsilon\psi \| = \| \hat{\psi} - \hat{\psi}_\varepsilon \| \leq \frac{C \cdot \varepsilon}{c_1 - C \cdot \varepsilon} \| C_{XX^+} \| \| \psi \| \leq \frac{C \cdot \varepsilon}{c_1} \| C_{XX^+} \| \| \psi \| = c_2 \| \psi \|.
    \]

    De manera análoga, existe una constante $c_3$ tal que 

    \[
    \| \U^\varepsilon_N - \U_N \| \leq c_3 \cdot \varepsilon.
    \]
    
    Luego, al igual que antes se tiene
    \[
    \begin{aligned}
        \| \U - \U_N \| & \leq \| \U - \U^\varepsilon \| + \| \U^\varepsilon - \U_N^\varepsilon \| + \| \U_N^\varepsilon - \U_N \| \\
        & \leq c_2 \cdot \varepsilon + C N^{-1/2} + c_3 \cdot \varepsilon.
    \end{aligned}
    \]

   Tomando $\varepsilon = c_4 N^{-1/2}$, con $c_4 = c_1/(2C)$ que cumple
   \[
   C  \varepsilon = C c_4 N^{-1/2} \leq C c_4 = c_1/2 < c_1
   \]
   con lo que se cumple la condición $c_1 - C  \varepsilon > 0$, se tiene
   \[
   \| \U - \U_N \| \leq c_5 N^{-1/2}.
   \]
\end{proof}

\section{Resultados numéricos}

A pesar de que las cotas de error del kernel Extended Dynamic Mode Decomposition serán útiles para la construcción de la cota de error para el filtro, también son interesantes por sí solas y se puede visualizar al aproximar distintos sistemas dinámicos y operadores de manera empírica.\\
Será de interés en esta sección no solo explorar cómo un sistema lineal puede aproximar otro sistema de interés mediante el \textit{sampleo} de datos.

\subsection{kEDMD para el caso lineal}

Un primer caso de interés es estudiar qué ocurre cuando se tiene un sistema lineal, esto es considerar un sistema de la forma
\begin{equation*}
    \mathbf{x}_{k+1} = \mathbf{A} \mathbf{x}_{k}.
\end{equation*}
Para este ejemplo se considera
\begin{equation*}
    \mathbf{A} = 
    \begin{pmatrix}
        1.01 & 0.04 & 0 \\
        0.01 & 1.02 & \alpha \\
        0 & 0.04 & 1.02
    \end{pmatrix}
\end{equation*}
en donde el parámetro $\alpha$ determinará el comportamiento del sistema dinámico en el tiempo. Se considera como distribución de probabilidad en $\X = \R^n$ una normal $N(0_{3}, 3 \cdot I_{3\times3})$, para la dinámica se considera un ruido normal $N(0_{3}, 10^{-7} \cdot I_{3\times3})$ y $\mathbf{x}_0 = (0.1, 0.1, 0.1)^T$ como condición inicial.\\
Notar que en este ejemplo no se satisface la hipótesis de compacidad, pero el sistema con alta probabilidad se mantiene en un compacto en horizonte de tiempo finito.\\
Se compara la dinámica original con aquella linealizada vía Operador de Koopman \textit{sampleando} 2500 puntos del espacio de estados, utilizando \textit{kernel} de Matérn de parámetro $\nu = 1/2$ y un ancho de banda $\gamma = 10^{-3}$. Esto es, se compara con el sistema
\begin{align*}
    \mathbf{z}_{k+1} = & \mathbf{U}_N \mathbf{z}_k \\
    \hat{\mathbf{x}}_k = & \mathbf{B}_N \mathbf{z}_k \\
    \mathbf{z}_0 = & \Phi_N(\mathbf{x}_0)
\end{align*}

Se compara lo obtenido para $\alpha \in \{ -0.3, -0.1, 0.05\}$.
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/content/chapter3/Linear1.pdf}
        \caption{$\alpha=-0.3$}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/content/chapter3/Linear2.pdf}
        \caption{$\alpha=-0.1$}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/content/chapter3/Linear3.pdf}
        \caption{$\alpha=0.05$}
    \end{subfigure}
    \caption{Ilustración de los tres casos de $\alpha$ elegidos para la comparación entre el sistema lineal original y el sistema linealizado por Koopman a 2500 puntos \textit{sampleados} de una variable aleatoria normal. En forma de puntos se dejan los valores reales que toma el sistema y en línea continua los valores entregados por el sistema linealizado, que se consideran como predicción.}
    \label{fig:Comp_traj_lin}
\end{figure}
Aunque la cota en \eqref{eq:kEDMD_bound} no hace referencia directa a la diferencia en norma de las trayectorias, es de interés analizar el orden en que decae el error en función de $N$, para ello se calcula la diferencia en norma de las trayectorias generadas para $N \in \{ 100k : k \in \{1, \dots, 30\} \}$. \\
En la figura \ref{fig:Comp_traj_lin} se observa que las trayectorias obtenidas con el sistema linealizado vía Koopman son muy cercanas a las obtenidas con el sistema lineal original, solo difiriendo en zonas donde el sistema original toma valores muy altos, lo que se puede asociar a la poca probabilidad de \textit{sampleo} que le da la distribución normal elegida a puntos muy alejados del origen. \\
Mientras que en la figura \ref{fig:ErrorLin} se observa que en realidad el orden de decaimiento del error en función de $N$ es mayor que $N^{-1/2}$, mejorando la cota mostrada en \eqref{eq:kEDMD_bound}.
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/content/chapter3/Linear1Errors.pdf}
        \caption{$\alpha=-0.3$}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/content/chapter3/Linear2Errors.pdf}
        \caption{$\alpha=-0.1$}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/content/chapter3/Linear3Errors.pdf}
        \caption{$\alpha=0.05$}
        \label{fig:image3}
    \end{subfigure}
    \caption{Ilustración de los tres casos de $\alpha$ elegidos para la evolución en función de $N$ de la diferencia en norma entre el sistema lineal original y el sistema linealizado por Koopman a $N$ puntos \textit{sampleados} de una variable aleatoria normal. En forma de puntos se deja la evolución observada del error y en línea continua la mejor curva de la forma $C \cdot N^{a}$, donde $a$ es el exponente que se deja en la leyenda.}
    \label{fig:ErrorLin}
\end{figure}
\subsection{kEDMD para modelos utilizados en epidemiología}
Para esta sección se ilustrarán los resultados de kEDMD para modelos epidemiológicos que no consideran nacimientos ni muertes, es decir, su población total se mantiene constante en el tiempo. Estos modelos reciben el nombre de compartimentales, ya que cada parte de la población se encuentra en un único compartimento y toda la población está en alguno de ellos. Para estos casos se supondrá que la población está normalizada, esto es, que cada compartimento está en $[0, 1]$.\\
Esto último permite decir que el espacio de estados de este modelo es, sin considerar los eventuales factores estocásticos, un $(n-1)$-simplex, siendo $n$ la cantidad de compartimentos considerados, que se define como el conjunto
\begin{equation*}
    \Delta_{n-1} = \left \{ x \in \R^n : \sum_{i=1}^n x_i = 1, \, x_i \geq 0, \, \forall i \in \{1, \dots, n\} \right \}.
\end{equation*}
De este conjunto se puede \textit{samplear} de manera eficiente desde una distribución Dirichlet \cite{Frigyik2010IntroductionProcesses}, y se encuentra implementado en las principales librerías con funcionalidades estadísticas como SciPy \cite{Virtanen2020SciPyPython}, que será la utilizada en este trabajo.\\
La densidad de una variable aleatoria Dirichlet con parámetros $\alpha_1, \alpha_2, \ldots, \alpha_K$ se define como:
\[
f(x_1, x_2, \ldots, x_K; \alpha_1, \alpha_2, \ldots, \alpha_K) = \frac{1}{B(\alpha)} \prod_{i=1}^{K} x_i^{\alpha_i - 1}
\]
donde $x_i \geq 0$ para todo $i$, $\sum_{i=1}^{K} x_i = 1$, y
\[
B(\alpha) = \frac{\prod_{i=1}^{K} \Gamma(\alpha_i)}{\Gamma\left(\sum_{i=1}^{K} \alpha_i\right)}
\]
es la función beta multivariable, y $\Gamma(\cdot)$ es la función gamma. En la figura \ref{fig:Dirichlet_samples} se puede apreciar la diferencia de \textit{samplear} para diferentes valores de $\alpha$. Un valor de $\alpha$ con entradas iguales genera la misma dispersión en todas las direcciones, siendo $(1, 1, 1)$ la variable aleatoria uniforme en $\Delta_{n-1}$, mientras que valores altos de $\alpha$ generan una alta concentración de muestras en el centro del conjunto. Por otro lado valores desiguales generar mayor cantidad de muestras en alguna de las caras del conjunto.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{img/content/chapter3/Dirichlet.pdf}
    \caption{1000 muestras de una variable aleatoria Dirichlet para diferentes valores de $\alpha$, observando dos casos de $\alpha$ con valores homogéneos y uno con valores heterogéneos, lo que provoca un desbalance de muestras.}
    \label{fig:Dirichlet_samples}
\end{figure}

El primer ejemplo será el modelo SIR, visualizando lo obtenido por kEDMD, utilizando \textit{kernel} de Matérn de parámetro $\nu=1/2$ y ancho de banda $\gamma=10^{-3}$. Se utiliza una distribución Dirichlet de parámetro $(1,1,1)$ como variable aleatoria asociada al espacio de estados y ruido aditivo centrado y Gaussiano de matriz de covarianza $10^{-7} I_{3 \times 3}$, de estas distribuciones se \textit{samplearán} $N=1000$ puntos. Se toma como condición inicial $\mathbf{x}_0 = (0.9, 0.1, 0.0)$ y los resultados obtenidos se pueden ver en la imagen \ref{fig:Comp_traj_SIR}. 

Similar a lo hecho en el caso lineal, en la figura \ref{fig:ErrorSIR} se compara la diferencia en norma entre la trayectoria original y la trayectoria linealizada, con la mejor curva de la forma $C \cdot N^{a}$, para observar si los errores andan en el orden de decaimiento en \eqref{eq:kEDMD_bound}.

Se observa nuevamente que la discrepancia entre las trayectorias en efecto anda del orden de la cota expuesta en este capítulo, incluso con decaimientos aún más rápidos.

Se comparan las trayectorias generadas por el sistema original con las generadas por kEDMD, utilizando exactamente la misma configuración del sistema anterior, solo cambiando la dinámica.

Nuevamente se observa que el orden de discrepancia de las trayectorias es de un orden similar, o incluso menor, al visto en la cota \ref{eq:kEDMD_bound}.